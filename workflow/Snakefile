'''
* Authors * 
S. Sevilla
P. Homan
* Overview * 
- Multiplexed samples are split based on provided barcodes and named using provide manifests, maximum 10 samples
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged
* Requirements *
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml
import csv

###############################################################
# config handling, set parameters
###############################################################
#snakemake config
source_dir = config['sourceDir']
cont_dir = config['containerDir']
out_dir = config['outputDir'].rstrip('/') + '/'
fastq_dir = config['fastqDir'].rstrip('/') + '/'

sample_manifest = config['sampleManifest']
multiplex_manifest = config['multiplexManifest']
contrast_manifest = config['contrastManifest']

nova_ref = config['reference']
filter_length = config['filterlength']
splice_aware = config['spliceaware'].capitalize()
include_rRNA = config['includerRNA'].capitalize()
splice_bp = config['spliceBPlength']
multiplex_flag = config['multiplexflag'].capitalize()
mismatch = config['mismatch']
min_count = config['mincount']
nt_merge = str(config['ntmerge']) + 'nt'
peak_id = config['peakid'].upper()
DE_method = config['DEmethod'].upper()
sp_junc = config['splicejunction'].upper()
condense_exon = config['condenseexon'].upper()

#define threads
with open(join(source_dir,'config','cluster_config.yaml')) as file:
    CLUSTER = yaml.load(file, Loader=yaml.FullLoader)
getthreads=lambda rname:int(CLUSTER[rname]["threads"]) if rname in CLUSTER and "threads" in CLUSTER[rname] else int(CLUSTER["__default__"]["threads"])

#set split files value
split_value = 500000

#expand reference selection
if (nova_ref == "mm10"):
    genome_ref = "GENCODE mm10 v23"
elif (nova_ref == "hg38"):
    genome_ref = "GENCODE hg38 v32"

#read in index config file and assign paths
index_manifest = join(source_dir, 'config', 'index_config.yaml')

with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)

gen_path = index_list[nova_ref]['gencodepath']
rseq_path = index_list[nova_ref]['refseqpath']
can_path = index_list[nova_ref]['canonicalpath']
intron_path = index_list[nova_ref]['intronpath']
rmsk_path = index_list[nova_ref]['rmskpath']
alias_path = index_list[nova_ref]['aliaspath']
add_anno_path = index_list[nova_ref]['additionalannopath']

#singularity exec command
singularity_exec = "singularity exec -B /data/$USER,/data/CCBR_Pipeliner," + out_dir + "," + fastq_dir + ",/fdb,/scratch"

#annotation config
annotation_config = join(source_dir,'config','annotation_config.txt')

#convert splice junction selection
if (sp_junc=="Y"):
    sp_junc = "TRUE"
else:
    sp_junc = "FALSE"

#convert condense junction selection
if (condense_exon=="Y"):
    cond_exon = "TRUE"
else:
    cond_exon = "FALSE"

#create list of alignment types based on splice_aware flag
if (splice_aware == 'N'):
    align_list = ["unaware"]
else:
    align_list = ["unmasked"]

#convert rRNA selection
if (include_rRNA=="Y"):
    refseq_rrna = "TRUE"
else:
    refseq_rrna = "FALSE"

#set strand ids
strand_list=['P','N']

###############################################################
# create sample lists
###############################################################
#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

###############################################################
# snakemake functions
###############################################################
#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = join(fastq_dir,multiplex_dict[wildcards.mp])
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + wildcards.mp + '/01_preprocess/',

    return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + wildcards.mp + '/01_preprocess/demux_' + bc + '.fastq.gz' + ' ' + out_dir + wildcards.mp + '/01_preprocess/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)
    
#create nondemux command line
def nondemux_cmd(wildcards):
  cmd_line = ''

  for k,v in multiplex_dict.items():
    for k2,v2 in samp_dict[k].items():
      cmd_line = fastq_dir + v + ' ' + out_dir + k + '/01_preprocess/' + k2 + '.fastq.gz; ' + cmd_line
  
  return(cmd_line)

#command needed to cut adaptors from demux samples
#example: 
        #singularity exec -B /scratch /data/CCBR_Pipeliner/iCLIP/container/icount.sif iCount cutadapt -ml 20 --untrimmed_output /path/_untrimmed.fastq.gz 
        # --reads_trimmed /path/sp_trimmed.fastq.gz /path/fastq.gz ABCDEDFGH
def adapt_cmd(wildcards):
    cmd=''

    for sp in sp_list:
        #sub df
        df_sub = df_samples[(df_samples['sample']==sp)]

        #create cmd
        filtered = join(out_dir, '01_preprocess', sp + '_filtered.fastq.gz')
        removed =  join(out_dir, '01_preprocess', sp + '_removed_seq.fastq.gz ')
        fastq = join(out_dir, df_sub.iloc[0]['multiplex'], '01_preprocess',sp + '.fastq.gz')

        cmd = singularity_exec + ' ' + join(cont_dir,'icount.sif') + ' iCount cutadapt -ml ' + str(filter_length) + ' --untrimmed_output ' + filtered + ' --reads_trimmed ' + removed + ' ' + fastq + ' ' + df_sub.iloc[0]['adaptor'] + '; ' + cmd
    
    return(cmd)
    
#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=join(out_dir,'01_preprocess',wildcards.sp + '_filtered.fastq.gz ')

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'qc','split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]

        #split can only create a max of 99 split files; handle large file sets to ensure max reads per 99 files
        if (sub_df.iloc[0]['filenum']>98):
            chunk_size = int(((sub_df.iloc[0]['chunksize'] * sub_df.iloc[0]['filenum']) / 98)+.9)

            #check the size is divisible by 4, if not increase the size until it is
            while (chunk_size % 4 !=0):
                chunk_size = chunk_size + 1
        else:
            chunk_size = sub_df.iloc[0]['chunksize']

    except:
        return(0)
    return(chunk_size)

#determine which umi separator to use
#iCount addes rbc: to all demux files; external demux uses an _
def umi_parameter(wildcards):
    if(multiplex_flag == 'Y'):
        umi_sep="rbc:"
    else:
        umi_sep="_"
    return(umi_sep)

#get index file depending on alignment type
def get_index(wildcards):

    #unaware index
    if (wildcards.al == "unaware"):
        nova_index = index_list[nova_ref]['std']
    elif (wildcards.al == "masked"):
        nova_index = index_list[nova_ref]['spliceawaremasked'][str(splice_bp)+'bp']
    elif (wildcards.al == "unmasked"):
        nova_index = index_list[nova_ref]['spliceawareunmasked'][str(splice_bp)+'bp']
    
    return(nova_index)

#determine alignment input based on splice_aware flag
def get_align_input(wildcards):
    if (splice_aware == "N"):
        f1 = join(out_dir,'01_preprocess','01_alignment', wildcards.sp + '.' + wildcards.al + '.split.' + wildcards.n + '.sam'),
    else:
        f1 = join(out_dir,'01_preprocess','03_genomic', wildcards.sp + '.' + wildcards.al + '.split.' + wildcards.n + '.sam.gz'),
    return(f1)

#read in contrast list for manorm
def get_manorm_list():
    
    #read file
    with open(contrast_manifest) as f:
        reader = csv.reader(f, delimiter="\t")
        manorm_file = list(reader)

        #for each group in comparison list
        manorm_list=[]
        for group in manorm_file:
            
            #for each individual id
            for id in group:
                id = id.replace(",", "_vs_")
            
            #append final list
            manorm_list.append(id)
        
        #remove header
        manorm_list.pop(0)
    return(manorm_list)

#get the input files for analysis
def get_MANORM_analysis_input(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir,'05_manorm', '01_input', gid_1 + '_PeaksforMAnrom.bed'),
                join(out_dir,'05_manorm', '01_input', gid_1 + '_ReadsforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'05_manorm', '01_input', gid_1 + '_PeaksforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'05_manorm', '01_input', gid_2 + '_PeaksforMAnrom.bed'),
                join(out_dir,'05_manorm', '01_input', gid_2 + '_ReadsforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'05_manorm', '01_input', gid_2 + '_PeaksforMAnrom_' + wildcards.strand + '.bed')]
    return(input_list)

#get sample name for manorm comparison - sample
def get_MANORM_gid1(wildcards):
    gid = wildcards.group_id.split("_vs_")[0]
    return(gid)

#get sample name for manorm comparison - background
def get_MANORM_gid2(wildcards):
    gid = wildcards.group_id.split("_vs_")[1]
    return(gid)

#get input files for post-processing
def get_MANORM_post_processing(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir,'05_manorm','02_analysis', wildcards.group_id + "_P", wildcards.group_id + '_all_MAvalues.xls'),
                join(out_dir,'05_manorm','02_analysis', wildcards.group_id + "_N", wildcards.group_id + '_all_MAvalues.xls')]
    return(input_list)

###############################################################
# main code
###############################################################
#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

#determine barcode length
barcode_length = int(len(df_samples.iloc[0,3].replace('N',''))) #length of barcode

#create manorm list
if DE_method == "MANORM": 
    manorm_list = get_manorm_list()

###############################################################
# rule all
###############################################################
#set rule_all inputs depending on flags:
## if samples have been multiplexed
if multiplex_flag == 'Y':
    input_multiplex = [expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list),
                        expand(join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz'), mp=samp_dict.keys())]
else:
    input_multiplex = [expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list)]

## if samples are spliced
if splice_aware == 'Y':
    input_unmapped = expand(join(out_dir,'02_bam','01_unmapped','{sp}.{al}.complete.bam'),  sp=sp_list, al=align_list)

    input_splice = [expand(join(out_dir,'10_mapq_score','{sp}.readids.txt'), sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.unaware.subset.bam'),sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.mapq_recalculated.bam'),sp=sp_list)]
else:
    input_unmapped = expand(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.si.bam'), sp=sp_list, al=align_list),

    input_splice = [expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'),sp=sp_list, al=align_list)]

#if samples are running MANORM
if DE_method == "MANORM":
    input_manorm_reports = [expand(join(out_dir,'05_manorm','03_report','{group_id}_manorm_report.html'),group_id=manorm_list)]

    #testing
    #input_manorm_reports = expand(join(out_dir,'05_manorm', '01_input','{sp}_ReadsforMAnrom_P.bed'), sp=sp_list)
    #input_manorm_reports = expand(join(out_dir,'05_manorm','02_analysis', '{group_id}_{strand}','{group_id}_all_MAvalues.xls'),group_id=manorm_list,strand=strand_list)
    #input_manorm_reports = expand(join(out_dir,'05_manorm','02_analysis', '{group_id}_post_processing.txt'), group_id=manorm_list)
else:
    input_manorm_reports = [expand(join(out_dir,'04_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),sp=sp_list)]

#local rules
localrules: check_manifest, copy_nondemux, rename_demux, determine_splits, multiqc, feature_counts, MANORM_beds

rule all:
    input:
        ################################################################################################
        # required, non temp files
        ################################################################################################
        #non-rule: check sample fastq files exist
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list),
        
        #check manifest
        join(out_dir,'qc','manifest_clean.txt'),
        
        #different inputs for multiplexed or nonmultiplexed pipeline
        input_multiplex,
        
        #remove_adaptors
        expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list),
        
        #determine_splits
        join(out_dir,'qc','split_params.tsv'),
        
        #merge_mm_and_unique
        expand(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.si.bam'), sp=sp_list, al=align_list),

        # #multiqc
        # join(out_dir,'qc','multiqc_report.html'),

        # #qc_troubleshoot
        # join(out_dir,'qc','qc_report.html'),

        # #dedup
        expand(join(out_dir,'02_bam','03_dedup','{sp}.dedup.si.bam'),sp=sp_list),

        #create_beds_safs
        expand(join(out_dir,'03_peaks','01_bed','{sp}_all.bed'), sp=sp_list),
        expand(join(out_dir,'03_peaks','01_bed','{sp}_unique.bed'), sp=sp_list),
        expand(join(out_dir,'03_peaks','02_SAF','{sp}_all.SAF'), sp=sp_list),
        expand(join(out_dir,'03_peaks','02_SAF','{sp}_unique.SAF'), sp=sp_list),
        
        #annotation_report
        expand(join(out_dir,'04_annotation', '{sp}_annotation_final_report.html'),sp=sp_list),
        expand(join(out_dir,'04_annotation', '{sp}_annotation_final_table.txt'),sp=sp_list),

        #MANORM
        input_manorm_reports

        ################################################################################################
        #intermediate troubleshooting, temp files
        ################################################################################################
        # #qc_fastq_pre
        # expand(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list,sp=sp_list),

        # #qc_fastq_post
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        
        # #qc_screen
        # expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered_screen.txt'),sp=sp_list),
        # expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered_screen.txt'),sp=sp_list),
        
        # #merge_splits_unique_mm
        # expand(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.unique.bam'), sp=sp_list, al=align_list),
        # expand(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.mm.bam'),sp=sp_list, al=align_list),

        # #merge_mm_and_unique
        # expand(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.bam'), sp=sp_list, al=align_list),
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'), sp=sp_list, al=align_list),

        # #merge_unmapped_splits
        # input_unmapped,

        # #qc_alignment
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),

        # #feature_counts
        # expand(join(out_dir,'03_peaks','03_uniquereadpeaks','{sp}_uniqueCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'03_peaks','03_uniquereadpeaks','{sp}_allFracMMCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'03_peaks','03_allreadpeaks','{sp}_uniqueCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'03_peaks','03_allreadpeaks','{sp}_allFracMMCounts.txt'), sp=sp_list),

        # #project_annotations
        # join(out_dir,'04_annotation', '01_project','annotations.txt'),

        # #peak_annotations
        # expand(join(out_dir,'04_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),sp=sp_list),

        # #MANORM
        # input_manorm_input,
        # input_manorm_analysis,
        # input_manorm_post_proccessing, 

#common and other SMK 
if source_dir == "":
    include: "workflow/rules/common.smk"
    include: "workflow/rules/other.smk"
else:
    include: join(source_dir,"workflow/rules/common.smk")
    include: join(source_dir,"workflow/rules/other.smk")

###############################################################
# snakemake rules
###############################################################
rule check_manifest:
    """
    Read in multiplex manifest, sample manifest, contrast manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    Will only check contrast manifest if "MANORM" is selected
    If files are correct, outputs a temp file. If not, outputs error file.
    """
    input:
        mm = multiplex_manifest,
        sm = sample_manifest,
        cm = contrast_manifest
    params:
        py = join(source_dir,'workflow','scripts','01_check_manifest.py'),
        base = join(out_dir,'qc','manifest_')
    envmodules:
        config['python']  
    output:
        o1 = join(out_dir, 'qc', 'manifest_clean.txt'),
    shell:
        """
        python {params.py} \
            '{params.base}' {input.mm} {input.sm} {input.cm} {DE_method}
        """

#pipeline branches to demultiplex, if necessary
if (multiplex_flag == 'Y'):
    rule qc_barcode:
        """
        generate counts of barcodes and output to text file
        will run python script that determines barcode expected and generates mismatches based on input
        output barplot with top barcode counts
        """
        input:
            manifest = join(out_dir,'qc', "manifest_clean.txt"),
            fq = get_fq_names,
        params:
            rname = "01a_qc_barcode",
            R = join(source_dir,'workflow', 'scripts', '02_barcode_qc.R'),
            base = join(out_dir,'{mp}','00_qc_post'),
            mm = mismatch,
            bc_len = barcode_length,
            start_pos = 6 if barcode_length==6 else 4
        envmodules:
            config['R'],
        output:
            counts = temp(join(out_dir,'{mp}','00_qc_post','{mp}_barcode_counts.txt')),
            png = temp(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.png')),
            txt = temp(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'))
        shell:
            """
            gunzip -c  {input.fq} | \
            sed -n '/@/{{n;p}}' | awk '{{print substr($0, {params.start_pos}, {params.bc_len});}}' | sort -n | uniq -c > {output.counts};
            
            Rscript {params.R} --sample_manifest {sample_manifest} --multiplex_manifest {multiplex_manifest} --barcode_input {output.counts} \
                --mismatch {params.mm} --mpid {wildcards.mp} --output_dir {params.base}
            """

    rule demultiplex:
        """
        https://icount.readthedocs.io/en/latest/ref_CLI.html
        Reads in fastq files from multiplex_manifest.
        Finds multiplex match between manifest files, splits files based on list of
        barcodeIDs found in sample_manifest
        For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN
        file_name                   multiplex
        SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1
        multiplex       sample          group       barcode     adaptor
        SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
        SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
        """
        input:
            f1 = get_fq_names,
            #manifest = join(out_dir,'qc', "manifest_clean.txt"),
        params:
            rname='01b_demux',
            s_exec = singularity_exec,
            doc = join(cont_dir,'icount.sif'),
            cmd = demux_cmd,
            ml = filter_length,
            mm = mismatch
        envmodules:
            config['singularity'],
        output:
            o1 = temp(join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz')),
        shell:
            """
            {params.s_exec} {params.doc} iCount demultiplex --minimum_length {params.ml} --mismatches {params.mm} {params.cmd}
            """

    rule rename_demux:
        """
        renames demultiplexed files based on
        from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
        to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
        """
        input:
            f1 = join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz')
        params:
            rname = '01c_rename_demux',
            cmd = rename_cmd
        output:
            o1 = temp(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'))
        shell:
            """
            #move files
            mv {params.cmd}
            """     
else: 
    rule copy_nondemux:
        """
        copies fastq files from source location and moves into renamed file folder
        """
        input:
            manifest = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname = '01_copy_nondemux',
            cmd = nondemux_cmd
        output:
            o1 = temp(expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list))
        shell:
            """
            cp {params.cmd}
            """

rule remove_adaptors:
    """
    removes adaptors from files. software outputs:
    1) untrimmed.fastq.gz - the remaining sequences without the adaptor --> filtered.fastq.gz
    2) trimmed.fastq.gz - the sequences that were trimmed from the input file --> removed_seq.fastq.gz
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list),
    params:
        rname='02_remove_adaptors',
        cmd = adapt_cmd,
    envmodules:
        config['singularity'],    
    output:
        o1 = temp(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz')),
        o2 = temp(join(out_dir,'01_preprocess','{sp}_removed_seq.fastq.gz'))
    shell:
        """
        if [ -d "/tmp/iCount/" ]; then rm -r "/tmp/iCount/"; fi
        
        {params.cmd}
        """

rule qc_fastq_pre:
    """
    Runs FastQC report on each sample before adaptors have been removed
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list)
    params:
        rname='03a_qc_fastq_pre',
        base = join(out_dir,'{mp}','00_qc_pre/')
    envmodules:
        config['fastqc']
    output:
        o1 = temp(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'))
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_fastq_post:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        f1 = expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list)
    params:
        rname='03b_qc_fastq_post',
        base = join(out_dir, 'qc', '00_qc_post'),
    envmodules:
        config['fastqc']
    output:
        o1 = temp(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'))
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_screen_validator:
    """
    #fastq screen
    - this will align first to human, mouse, bacteria then will align to rRNA
    must run fastq_screen as two separate commands - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html

    - fastq validator
    Quality-control step to ensure the input FastQC files are not corrupted or
    incomplete prior to running the entire workflow.
    @Input:
        Raw FastQ file (scatter)
    @Output:
        Log file containing any warnings or errors on file
    """
    input:
        f1 = join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz')
    params:
        rname='04_qc_screen_validator',
        tmp = join('{sp}_filtered.fastq'),
        base_species = join(out_dir, 'qc', '00_qc_screen_species'),
        conf_species = join(source_dir,'config','fqscreen_species_config.conf'),
        base_rrna = join(out_dir, 'qc', '00_qc_screen_rrna'),
        conf_rrna = join(source_dir,'config','fqscreen_rrna_config.conf'),
        base_val = join(out_dir,'qc'),
        threads = getthreads("qc_screen_validator")
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = temp(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered_screen.txt')),
        o2 = temp(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered_screen.txt')),
        o3 = join(out_dir,'qc','{sp}.validated.fastq.log'),
    shell:
        """
        set +e

        #set / create tmp dir
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir="/lscratch/${{SLURM_JOB_ID}}"
        else
            tmpdir="{out_dir}01_preprocess/qc_screen"
            if [ ! -d "${{tmpdir}}" ]; then 
                mkdir "$tmpdir"
            fi
        fi
        #gunzip input files
        gunzip -c {input.f1} > ${{tmpdir}}/{params.tmp};
        
        #run screen
        fastq_screen --conf {params.conf_species} \
            --outdir {params.base_species} \
            --threads {params.threads} \
            --subset 1000000 \
            --aligner bowtie2 \
            --force \
            ${{tmpdir}}/{params.tmp};
        fastq_screen --conf {params.conf_rrna} \
            --outdir {params.base_rrna} \
            --threads {params.threads} \
            --subset 1000000 \
            --aligner bowtie2 \
            --force \
            ${{tmpdir}}/{params.tmp};
        
        #remove gunzipped file
        rm ${{tmpdir}}/{params.tmp}

        #make fastq val dir
        mkdir -p {params.base_val}
        
        #run validator
        /data/CCBR_Pipeliner/db/PipeDB/bin/fastQValidator \
            --noeof \
            --printableErrors 100000000 \
            --baseComposition \
            --avgQual \
            --file {input.f1} > {output.o3};
        
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 0
        else
            exit 0
        fi
        """

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to split FASTQ file
    split is necessary to improve speed of alignment
    split is done based off split_value input, maintaining every 4 lines = 1 seq
    """
    input:
        f1 = expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list)
    params:
        sh = join(source_dir, 'workflow', 'scripts', '03_find_split_parameters.sh'),
    output:
        o1 = join(out_dir,'qc','split_params.tsv')
    shell:
        """
        sh {params.sh} {output.o1} {split_value} {input.f1}
        """

rule split_files:
    """
    performs split of fastq file into smaller files using parameters in split_params.tsv
    """
    input:
        f1 = join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),
        sp = join(out_dir,'qc','split_params.tsv')
    params:
        rname='05_split_files',
        base = join(out_dir,'01_preprocess','00_splits','{sp}.split.'),
        cmd = get_filechunk_size
    output:
        o1 = temp(dynamic(join(out_dir,'01_preprocess','00_splits','{sp}.split.{n}.fastq.gz')))
    shell:
        """
        zcat {input.f1} | split --additional-suffix .fastq -l {params.cmd} --numeric-suffixes=1 --filter='gzip > $FILE.gz' - {params.base}
        """

rule novoalign:
    """
    alignment for non-splice aware pipeline and for splice aware pipeline MAPQ recalculation
    http://www.novocraft.com/documentation/novoalign-2/novoalign-user-guide/novoalign-command-options/
    """
    input:
        f1 = join(out_dir,'01_preprocess','00_splits','{sp}.split.{n}.fastq.gz')
    params:
        rname='06_novoalign',
        n_index = get_index,
        l = filter_length
    envmodules:
        config['novocraft']
    output:
        o1 = temp(join(out_dir,'01_preprocess','01_alignment','{sp}.{al}.split.{n}.sam')),
    shell:
        """
        zcat {input.f1} | \
        novoalign -d {params.n_index} -k \
        -f - \
        -F STDFQ \
        -c 32 \
        -t 15,3 \
        -l {params.l} \
        -x 4 \
        -g 20 \
        -s 1 \
        -o SAM \
        -R 0 \
        -r EXHAUSTIVE 999 > {output.o1}
        """

#pipeline branches for splice_aware processing
if (splice_aware == "Y"):
    rule cleanup_conversion:
        """
        performs cleanup for inserts
        
        converts transcriptome coordinates to genomic coordinates
        uses USeq version 8.9.6
        unmapped reads are removed during genomic coordinates conversion, need to include them in final file
        convert sam file to bam file
        
        merge unmapped reads from cleanup to into final sam
        convert bam back to sam file
        """
        input:
            f1 = join(out_dir,'01_preprocess','01_alignment','{sp}.{al}.split.{n}.sam'),
        params:
            rname = '07a_cleanup_conversion',
            base = '{sp}.{al}.split.{n}',
            base_out = join(out_dir,'01_preprocess','03_genomic','{sp}.{al}.split.{n}.sam'),
            doc = join(cont_dir,'USeq_8.9.6','Apps/SamTranscriptomeParser')
        envmodules:
            config['samtools'],
            config['java']
        output:
            sam = temp(join(out_dir,'01_preprocess','03_genomic','{sp}.{al}.split.{n}.sam.gz')),
            bam = temp(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.final.si.bam')),
        shell:
            """
            #set / create tmp dir
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmpdir="/lscratch/${{SLURM_JOB_ID}}"
            else
                tmpdir="{out_dir}01_preprocess/03_tmp_genomic"
                if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            fi
            #run cleanup
            set +e
            samtools view {input.f1} | awk '{{ if (($4 == 1 && $6!~/^[0-9]I/ && $1~/:/ )||($4 > 1 && $1~/:/ )) {{ print }} }}' > ${{tmpdir}}/{params.base}.tmp.sam;
            samtools view -H {input.f1} | cat - ${{tmpdir}}/{params.base}.tmp.sam > ${{tmpdir}}/{params.base}.tmp.mapped.sam; 
            samtools view -f 4 {input.f1} > ${{tmpdir}}/{params.base}.tmp.unmapped.sam;
            
            #run genomic conversion
            java -Djava.io.tmpdir=${{tmpdir}} -jar -Xmx100G {params.doc} \
                -f ${{tmpdir}}/{params.base}.tmp.mapped.sam \
                -a 50000 -n 25 -u -s {params.base_out};

            #convert input to bam
            gunzip -c {output.sam} | samtools view -S -b  - > ${{tmpdir}}/{params.base}.unmasked.bam;
            
            #merge unmapped sam and unmasked bam into final output
            samtools merge -f ${{tmpdir}}/{params.base}.merged.bam ${{tmpdir}}/{params.base}.tmp.unmapped.sam ${{tmpdir}}/{params.base}.unmasked.bam;
            
            #pull header from merged and add to final output
            samtools view -h -o ${{tmpdir}}/{params.base}.final.bam ${{tmpdir}}/{params.base}.merged.bam;
            
            #sort and index final outupt
            samtools sort -T ${{tmpdir}} ${{tmpdir}}/{params.base}.final.bam -o {output.bam};
            samtools index {output.bam}; 
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi                    
            """
    
    rule merge_unmapped_splits:
        """
        merge all sample unmapped reads into one file
        """
        input:
            sorted_list = dynamic(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.final.si.bam')),
        params:
            rname='07d_merge_unmapped_splits',
        envmodules:
            config['samtools']  
        output:
            final = join(out_dir,'02_bam','01_unmapped','{sp}.{al}.complete.bam'),
        shell:
            """
            samtools merge -f {output.final} {input.sorted_list} 
            """

rule create_bam_mm_unique:
    """
    novoalign creates files where NH = the number of possible alignments and IH = the number of 
    acutal alignments in a file 
    creates two text files from sam file
    1. unique file without IH:i flag
    2. MM file with IH:i flag
    read sam file header, print to header txt
    umitools dedup can only reference NH flags since novoalign creates files where NH = the 
    number of possible alignments and IH = the number of acutal alignments in a file we must 
    replace the NH flags with the IH flags to deduplicate based off the correct value
    add nh:i:1 tag, cat header, create unique bam file
    """
    input:
        f1 = get_align_input,
    params:
        rname='08_create_bam_mm_unique',
        base = '{sp}.{al}.split.{n}.tmp',
    envmodules:
        config['samtools']
    output:
        sort_u = temp(join(out_dir,'01_preprocess','05_unique','{sp}.{al}.split.{n}.unique.si.bam')),
        sort_m = temp(join(out_dir,'01_preprocess','05_mm','{sp}.{al}.split.{n}.mm.si.bam')),
    shell:
        """
        ################################################
        #set / create tmp dir
        ################################################
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir="/lscratch/${{SLURM_JOB_ID}}"
            tmpdir_m="${{tmpdir}}/m"
            tmpdir_u="${{tmpdir}}/u"

            mkdir $tmpdir_m
            mkdir $tmpdir_u
        else
            tmpdir="{out_dir}01_preprocess/05_tmp_bam"
            tmpdir_m="$tmpdir/m"
            tmpdir_u="$tmpdir/u"
            
            if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            if [ ! -d $tmpdir_u ]; then mkdir $tmpdir_u; fi
            if [ ! -d $tmpdir_m ]; then mkdir $tmpdir_m; fi
        fi
        ################################################
        #create mm, unique, header files
        ################################################
        set +e
        #output of genomic conversion, if splice aware, is a zipped file
        if [[ {splice_aware} == "N" ]]; then
            samtools view {input.f1} | grep -v 'IH:i' > ${{tmpdir_u}}/{params.base}.unique.txt;
            samtools view {input.f1} | grep 'IH:i' > ${{tmpdir_m}}/{params.base}.mm.txt;
            samtools view -H {input.f1} > ${{tmpdir}}/{params.base}.header.txt
        else
            gunzip -c {input.f1} | samtools view | grep -v 'IH:i' > ${{tmpdir_u}}/{params.base}.unique.txt;
            gunzip -c {input.f1} | samtools view | grep 'IH:i' > ${{tmpdir_m}}/{params.base}.mm.txt;
            gunzip -c {input.f1} | samtools view -H > ${{tmpdir}}/{params.base}.header.txt
        fi
        ################################################
        #create unique bam, correcting NH header
        ################################################
        #if the file exists, remove it
        if [ -f ${{tmpdir_u}}/{params.base}.unique.sam ]; then rm ${{tmpdir_u}}/{params.base}.unique.sam; fi
        
        #create the new files
        touch ${{tmpdir_u}}/{params.base}.unique.sam

        #read the sam file
        while read rd; do 
            #if the line doesn't have IH
            if [[ ! $rd == "IH:i" ]]; then

                #if the line doesnt have NH then add NH:i:1
                if [[ ! $rd == "NH:i" ]]; then
                    echo "$rd" | awk '{{ $(NF+1) = "NH:i:1"; print }}' | echo "${{rd// /	}}" >> ${{tmpdir_u}}/{params.base}.unique.sam
                
                #if it does have NH then replace it with NH:i:1
                else 
                    replacement="NH:i:1"
                    fixed=${{rd/NH:i:[0-9]*/$replacement}}
                    echo "$fixed" >> ${{tmpdir_u}}/{params.base}.unique.sam
                fi
            #if does have IH (and will always have an NH)
            #replace the NH value with the IH value
            else
                IH=`echo $rd | grep -o "IH:i:[0-9]*"`
                IH_num=`echo ${{IH/IH:i:/}}`
                NH=`echo $rd | grep -o "NH:i:.*\S"`
                NH_num=`echo ${{NH/NH:i:/}}`
                NH_complete=`echo ${{NH/$NH_num/$IH_num}}`
                echo "${{rd/$NH/$NH_complete}}" >> ${{tmpdir_u}}/{params.base}.unique.sam
            fi
        done < ${{tmpdir_u}}/{params.base}.unique.txt
        
        #place header back on file, sort, convert to bam
        cat ${{tmpdir}}/{params.base}.header.txt ${{tmpdir_u}}/{params.base}.unique.sam | samtools sort -T ${{tmpdir_u}} | samtools view -Sb > ${{tmpdir_u}}/{params.base}.unique.bam;
        
        ################################################
        #create mm bam, correcting NH header
        ################################################
        #if the file exists, remove it
        if [ -f ${{tmpdir_m}}/{params.base}.mm.sam ]; then rm ${{tmpdir_m}}/{params.base}.mm.sam; fi
        
        #create the new files
        touch ${{tmpdir_m}}/{params.base}.mm.sam

        #read the sam file
        while read rd; do 
            #if the line doesn't have IH
            if [[ ! $rd == "IH:i" ]]; then 
                #if the line doesnt have NH then add NH:i:1
                if [[ ! $rd == "NH:i" ]]; then 
                    echo "$rd" | awk '{{ $(NF+1) = "NH:i:1"; print }}' | echo "${{rd// /	}}">> ${{tmpdir_m}}/{params.base}.mm.sam
                
                #if it does have NH then replace it with NH:i:1
                else 
                    replacement="NH:i:1"
                    fixed=${{rd/NH:i:[0-9]*/$replacement}}
                    echo "$fixed" >> ${{tmpdir_m}}/{params.base}.mm.sam
                fi
            
            #if does have IH (and will always have an NH)
            #replace the NH value with the IH value
            else
                IH=`echo $rd | grep -o "IH:i:[0-9]*"`
                IH_num=`echo ${{IH/IH:i:/}}`
                NH=`echo $rd | grep -o "NH:i:.*\S"`
                NH_num=`echo ${{NH/NH:i:/}}`
                NH_complete=`echo ${{NH/$NH_num/$IH_num}}`
                echo "${{rd/$NH/$NH_complete}}" >> ${{tmpdir_m}}/{params.base}.mm.sam
            fi
        done < ${{tmpdir_m}}/{params.base}.mm.txt

        #place header back on file, sort, convert to bam
        cat ${{tmpdir}}/{params.base}.header.txt ${{tmpdir_m}}/{params.base}.mm.sam | samtools sort -T ${{tmpdir_m}} | samtools view -Sb > ${{tmpdir_m}}/{params.base}.mm.bam;
        
        ################################################
        #sort unique and mm bam
        ################################################
        samtools sort -T ${{tmpdir_u}} ${{tmpdir_u}}/{params.base}.unique.bam -o {output.sort_u};
        samtools sort -T ${{tmpdir_m}} ${{tmpdir_m}}/{params.base}.mm.bam -o {output.sort_m};
        samtools index {output.sort_u};
        samtools index {output.sort_m};

        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 0
        else
            exit 0
        fi
        """

rule merge_splits_unique_mm:
    """
    merge split unique bam files into one merged.unique bam file
    merge split multimapped bam files into one merged.mm bam file
    """
    input:
        unique = dynamic(join(out_dir,'01_preprocess','05_unique','{sp}.{al}.split.{n}.unique.si.bam')),
        mm = dynamic(join(out_dir,'01_preprocess','05_mm','{sp}.{al}.split.{n}.mm.si.bam'))
    params:
        rname='10a_merge_splits_unique_mm',
    envmodules:
        config['samtools']  
    output:
        unique = temp(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.unique.bam')),
        mm = temp(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.mm.bam')),
    shell:
        """
        samtools merge -f {output.unique} {input.unique};
        samtools merge -f {output.mm} {input.mm}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files by sample
    sort and index output
    run samtools
    """
    input:
        un = join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.unique.bam'),
        mm = join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.mm.bam'),
    params:
        rname='10b_merge_mm_and_unique',
    envmodules:
        config['samtools']  
    output:
        merged = temp(join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.bam')),
        samstat = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt')),
        sort = join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.si.bam'),
    shell:
        """
        #set / create tmp dir
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir="/lscratch/${{SLURM_JOB_ID}}"
        else
            tmpdir="{out_dir}01_preprocess/05_merge_mu"
            if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
        fi
        #merge uniuqe and mm
        samtools merge -f {output.merged} {input.un} {input.mm};

        #sort and index
        samtools sort -T ${{tmpdir}} {output.merged} -o {output.sort};
        samtools index {output.sort};
        
        #run samstats
        samtools view -h {output.merged} | samtools stats - > {output.samstat}
        """

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir,'{mp}', '00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f2 = expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        f3 = expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered_screen.txt'),sp=sp_list),
        f4 = expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered_screen.txt'),sp=sp_list),
        f5 = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'),sp=sp_list, al=align_list)
    params:
        out = join(out_dir,'qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_pre = expand(join(out_dir,'{mp}','00_qc_pre'),mp = samp_dict.keys()),
        dir_post = expand(join(out_dir, 'qc', '00_qc_post')),
        dir_screen_species = expand(join(out_dir, 'qc', '00_qc_screen_species')),
        dir_screen_rrna = expand(join(out_dir, 'qc', '00_qc_screen_rrna')),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'qc/multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
            -d -dd 1 {params.dir_pre} {params.dir_post} \
            {params.dir_screen_rrna} {params.dir_screen_species} \
            -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir,'02_bam','02_merged','{sp}.{al}.merged.si.bam'),
    params:
        rname = "11_qc_alignment",
        R = join(source_dir,'workflow','scripts','04_alignment_stats.R'),
        sampleid = '{sp}.{al}',
        base = join(out_dir, 'qc', '00_qc_post/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_align_len.txt')),
        bam_u = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unalign_len.txt')),
        png_align = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png')),
        png_unalign = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png')),
        txt_align = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt')),
        txt_unalign = temp(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt')),
    shell:
        """
        #gather stats for all reads
        samtools view -F 4 {input.f1} | awk '{{ print length($10) }}' | sort -n | uniq -c > {output.bam_a};
        samtools view -f 4 {input.f1} | awk '{{ print length($10) }}' | sort -n | uniq -c > {output.bam_u};
        #run alignment stats code
        Rscript {params.R} --sampleid {params.sampleid} --bam_aligned {output.bam_a} --bam_unaligned {output.bam_u} --output_dir {params.base}
        """

#pipeline splits to handle multiplexed QC vs non-multiplexed QC
if (multiplex_flag == 'Y'):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
            txt_bc = expand(join(out_dir,'{mp}', '00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
            png_bc = expand(join(out_dir,'{mp}', '00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys())
        params:
            rname = "12_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','05_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}", \
                    b_txt = "{input.txt_bc}"))'
            """
else:
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
        params:
            rname = "12_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','05_qc_report_nondemux.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}"))'
            """

rule dedup:
    """
    deduplicate reads
    sort,index dedup.bam file
    get header of dedup file
    """
    input:
        f1 = join(out_dir,'02_bam','02_merged','{sp}.unmasked.merged.si.bam'),
    params:
        rname='13_dedup',
        umi = umi_parameter,
        base = '{sp}.unmasked'
    envmodules:
        config['umitools'],
        config['samtools']
    output:
        bam = join(out_dir,'02_bam','03_dedup','{sp}.dedup.si.bam'),
    shell:
        """
        #set / create tmp dir
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir="/lscratch/${{SLURM_JOB_ID}}"
        else
            tmpdir="{out_dir}01_preprocess/06_dedup/"
            if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
        fi
        umi_tools dedup \
        -I {input.f1} \
        --method unique \
        --multimapping-detection-method=NH \
        --umi-separator={params.umi} \
        -S ${{tmpdir}}/{params.base}.bam\
        --log2stderr;

        samtools sort -T ${{tmpdir}} ${{tmpdir}}/{params.base}.bam -o {output.bam};
        samtools index {output.bam};
        """

rule create_beds_safs:
    """
    split dedup file into unique files
    create bed file from the deduped unique file and deduped all file
    create SAF files from the deduped unique bed and all bed files
    """
    input:
        bam = join(out_dir,'02_bam','03_dedup','{sp}.dedup.si.bam'),
    params:
        rname='14_create_beds_safs',
        base = '{sp}.dedup.'
    envmodules:
        config['samtools'],
        config['bedtools']  
    output:
        bed_all = join(out_dir,'03_peaks','01_bed','{sp}_all.bed'),
        bed_unique = join(out_dir,'03_peaks','01_bed','{sp}_unique.bed'),
        saf_all = join(out_dir,'03_peaks','02_SAF','{sp}_all.SAF'),
        saf_unique = join(out_dir,'03_peaks','02_SAF','{sp}_unique.SAF')
    shell:
        """
        set +e
        #set / create tmp dir
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir_u="/lscratch/${{SLURM_JOB_ID}}"
        else
            tmpdir="{out_dir}01_preprocess/07_bed"
            tmpdir_u="{out_dir}01_preprocess/07_bed/u"
            if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            if [ ! -d $tmpdir_u ]; then mkdir $tmpdir_u; fi
        fi
        
        #create header for unique
        samtools view -H {input.bam} >  ${{tmpdir_u}}/{params.base}.header.txt;
        
        #create unique bam
        samtools view {input.bam} | grep -w 'NH:i:1' | cat ${{tmpdir_u}}/{params.base}.header.txt - |  samtools sort -T ${{tmpdir_u}} | samtools view -Sb > ${{tmpdir_u}}/{params.base}.unique.bam;
        
        #index
        cp ${{tmpdir_u}}/{params.base}.unique.bam ${{tmpdir_u}}/{params.base}.unique.i.bam; 
        samtools index ${{tmpdir_u}}/{params.base}.unique.i.bam;
        
        #create SAFS
        bedtools bamtobed \
            -split -i {input.bam} | bedtools sort -i - > {output.bed_all}; 
        bedtools bamtobed \
            -split -i ${{tmpdir_u}}/{params.base}.unique.i.bam | bedtools sort -i - > {output.bed_unique};
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {output.bed_all} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3"_"$5,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.saf_all};
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {output.bed_unique} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3"_"$5,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.saf_unique}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule feature_counts:
    """
    Unique reads (fractional counts correctly count splice reads for each peak. 
    When peaks counts are combined for peaks connected by splicing in Rscript)
    Include Multimap reads - MM reads given fractional count based on # of mapping 
    locations. All spliced reads also get fractional count. So Unique reads can get 
    fractional count when spliced peaks combined in R script the summed counts give 
    whole count for the unique alignement in combined peak.

    http://manpages.ubuntu.com/manpages/bionic/man1/featureCounts.1.html
    """
    input:
        bam = join(out_dir,'02_bam','03_dedup','{sp}.dedup.si.bam'),
        saf_all = join(out_dir,'03_peaks','02_SAF','{sp}_all.SAF'),
        saf_unique = join(out_dir,'03_peaks','02_SAF','{sp}_unique.SAF'),
    params:
        rname='15_feature_counts',
        threads = getthreads("feature_counts")
    envmodules:
        config['subread']  
    output:
        all_unique = join(out_dir,'03_peaks','03_allreadpeaks','{sp}_uniqueCounts.txt'),
        all_mm = join(out_dir,'03_peaks','03_allreadpeaks','{sp}_allFracMMCounts.txt'),
        unique_unique = join(out_dir,'03_peaks','03_uniquereadpeaks','{sp}_uniqueCounts.txt'),
        unique_mm = join(out_dir,'03_peaks','03_uniquereadpeaks','{sp}_allFracMMCounts.txt')
    shell:
        """
        #run for allreadpeaks
        featureCounts -F SAF \
            -a {input.saf_all} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T {params.threads} \
            -o {output.all_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf_all} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T {params.threads} \
            -o {output.all_mm} \
            {input.bam}
        #run for uniquereadpeaks
        featureCounts -F SAF \
            -a {input.saf_unique} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T {params.threads} \
            -o {output.unique_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf_unique} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T {params.threads} \
            -o {output.unique_mm} \
            {input.bam}
        """   

rule project_annotations:
    """
    generate annotation table once per project
    """
    input:
        join(out_dir, 'qc', 'split_params.tsv'),
    params:
        rname='16a_project_annotations',
        script = join(source_dir,'workflow','scripts','08_annotation.R'),
        ref_sp = nova_ref,
        rrna_flag = refseq_rrna,
        a_path = alias_path,
        g_path = gen_path,
        rs_path = rseq_path,
        c_path = can_path,
        i_path = intron_path,
        r_path = rmsk_path,
        custom_path = add_anno_path,
        base = join(out_dir,'04_annotation', '01_project/',),
        a_config = annotation_config,
    envmodules:
        config['R']
    output:
        anno = join(out_dir,'04_annotation', '01_project','annotations.txt'),
        nc_anno = join(out_dir,'04_annotation', '01_project','ncRNA_annotations.txt'),
        ref_gencode = join(out_dir,'04_annotation', '01_project','ref_gencode.txt'),
    shell:
        """
        Rscript {params.script} \
            --ref_species {params.ref_sp} \
            --refseq_rRNA {params.rrna_flag} \
            --alias_path {params.a_path} \
            --gencode_path {params.g_path} \
            --refseq_path {params.rs_path} \
            --canonical_path {params.c_path} \
            --intron_path {params.i_path} \
            --rmsk_path {params.r_path} \
            --custom_path {params.custom_path} \
            --out_dir {params.base} \
            --reftable_path {params.a_config}
        """

#pipeline splits to include manorm outputs with annotation outputs, if required
if(DE_method=="MANORM"):
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'03_peaks','03_allreadpeaks', '{sp}_uniqueCounts.txt'),
            all = join(out_dir,'03_peaks','03_allreadpeaks', '{sp}_allFracMMCounts.txt'),
            anno = join(out_dir,'04_annotation', '01_project','annotations.txt'),
        params:
            rname = '16b_peak_annotations',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = nova_ref,
            out = join(out_dir,'04_annotation','02_peaks/',),
            out_m = join(out_dir,'05_manorm','01_input/',),
            anno_dir = join(out_dir,'04_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
            error = join(out_dir,'04_annotation','read_depth_error.txt')
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'04_annotation', '02_peaks','{sp}_annotable.bed'),
            o2 = join(out_dir,'04_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
            m1 = temp(join(out_dir,'05_manorm', '01_input','{sp}_PeaksforMAnrom.bed')),
            m2 = temp(join(out_dir,'05_manorm', '01_input','{sp}_PeaksforMAnrom_P.bed')),
            m3 = temp(join(out_dir,'05_manorm', '01_input','{sp}_PeaksforMAnrom_N.bed')),
        shell:
            '''
            #set / create tmp dir
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmpdir="/lscratch/${{SLURM_JOB_ID}}"
            else
                tmpdir="{out_dir}01_preprocess/07_rscripts/"
                if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            fi
            
            Rscript {params.script} \
                --peak_type {params.p_type} \
                --peak_unique {input.unique} \
                --peak_all {input.all} \
                --join_junction {params.junc} \
                --condense_exon {params.c_exon} \
                --read_depth {params.r_depth} \
                --demethod {params.d_method} \
                --sample_id {params.sp} \
                --ref_species {params.ref_sp} \
                --anno_dir {params.anno_dir} \
                --reftable_path {params.a_config} \
                --gencode_path {params.g_path} \
                --intron_path {params.i_path} \
                --rmsk_path {params.r_path} \
                --tmp_dir ${{tmpdir}} \
                --out_dir {params.out} \
                --out_dir_DEP {params.out_m} \
                --output_file_error {params.error}
            '''
else:
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'03_peaks','03_allreadpeaks', '{sp}_uniqueCounts.txt'),
            all = join(out_dir,'03_peaks','03_allreadpeaks', '{sp}_allFracMMCounts.txt'),
            anno = join(out_dir,'04_annotation', '01_project','annotations.txt'),
        params:
            rname = '16b_peak_annotations',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = nova_ref,
            out = join(out_dir,'04_annotation','02_peaks/',),
            out_m = join(out_dir,'05_manorm','01_input/',),
            anno_dir = join(out_dir,'04_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
            error = join(out_dir,'04_annotation','read_depth_error.txt')
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'04_annotation', '02_peaks','{sp}_annotable.bed'),
            o2 = join(out_dir,'04_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
        shell:
            '''
            #set / create tmp dir
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmpdir_u="/lscratch/${{SLURM_JOB_ID}}"
            else
                tmpdir="{out_dir}01_preprocess/07_rscripts/"
                if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            fi
            
            Rscript {params.script} \
                --peak_type {params.p_type} \
                --peak_unique {input.unique} \
                --peak_all {input.all} \
                --join_junction {params.junc} \
                --condense_exon {params.c_exon} \
                --read_depth {params.r_depth} \
                --demethod {params.d_method} \
                --sample_id {params.sp} \
                --ref_species {params.ref_sp} \
                --anno_dir {params.anno_dir} \
                --reftable_path {params.a_config} \
                --gencode_path {params.g_path} \
                --intron_path {params.i_path} \
                --rmsk_path {params.r_path} \
                --tmp_dir ${{tmpdir}} \
                --out_dir {params.out} \
                --out_dir_manorm {params.out_m} \
                --output_file_error {params.error}
            '''

rule annotation_report:
    """
    generates an HTML report for peak annotations
    """
    input:
        peak_in = join(out_dir,'04_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
    params:
        rname = "16c_annotation_output",
        R = join(source_dir,'workflow','scripts','10_annotation.Rmd'),
        sp = "{sp}",
        ref_sp = nova_ref,
        r_depth= min_count,
        c_exon = cond_exon,
        junc = sp_junc,
        p_type = peak_id,
        rrna = refseq_rrna,
    envmodules:
        config['R']
    output:
        o1 = join(out_dir,'04_annotation', '{sp}_annotation_final_report.html'),
        o2 = join(out_dir,'04_annotation', '{sp}_annotation_final_table.txt'),
    shell:
        """
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(samplename = "{params.sp}", \
                peak_in = "{input.peak_in}", \
                output_table = "{output.o2}", \
                readdepth = "{params.r_depth}", \
                PeakIdnt = "{params.p_type}"))'
        """

#pipeline will continue through manorm processing, if required
if(DE_method=="MANORM"):
    rule MANORM_beds:
        input:
            f1 = join(out_dir,'03_peaks','01_bed','{sp}_all.bed'),
        params:
            rname = "17a_MANORM_beds"
        output:
            o1 = temp(join(out_dir,'05_manorm', '01_input','{sp}_ReadsforMAnrom_P.bed')),
            o2 = temp(join(out_dir,'05_manorm', '01_input','{sp}_ReadsforMAnrom_N.bed')),
        shell:
            """
            awk '$6=="+"' {input.f1} > {output.o1}
            awk '$6=="-"' {input.f1} > {output.o2}
            """

    rule MANORM_analysis:
        '''
        input requirements:
        - '05_manorm', '01_input', sample1 + '_PeaksforMAnrom.bed'),
        - '05_manorm', '01_input', sample1 + '_ReadsforMAnrom_' + P + '.bed'),
        - '05_manorm', '01_input', sample1 + '_ReadsforMAnrom_' + N + '.bed'),
        - '05_manorm', '01_input', sample2 + '_PeaksforMAnrom.bed'),
        - '05_manorm', '01_input', sample2 + '_ReadsforMAnrom_' + P + '.bed'),
        - '05_manorm', '01_input', sample2 + '_ReadsforMAnrom_' + N + '.bed'),

        output:
        - '05_manorm','02_analysis', 'sample1_vs_sample2_P/
        - '05_manorm','02_analysis', 'sample1_vs_sample2_N/
        '''
        input:
            get_MANORM_analysis_input
        params:
            rname = "17b_MANORM_analysis",
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
            base = join(out_dir,'05_manorm','02_analysis', '{group_id}_{strand}/'),
        envmodules:
            config['manorm']
        output:
            o1 = join(out_dir,'05_manorm','02_analysis', '{group_id}_{strand}','{group_id}_all_MAvalues.xls')
        shell:
            """
            manorm \
            --p1 "{out_dir}05_manorm/01_input/{params.gid_1}_PeaksforMAnrom_{wildcards.strand}.bed" \
            --p2 "{out_dir}05_manorm/01_input/{params.gid_2}_PeaksforMAnrom_{wildcards.strand}.bed" \
            --r1 "{out_dir}05_manorm/01_input/{params.gid_1}_ReadsforMAnrom_{wildcards.strand}.bed" \
            --r2 "{out_dir}05_manorm/01_input/{params.gid_2}_ReadsforMAnrom_{wildcards.strand}.bed" \
            --s1 0 \
            --s2 0 \
            -p 1 \
            -d 25 \
            -n 10000 \
            -s \
            -o {params.base} \
            --name1 {params.gid_1} \
            --name2 {params.gid_2}
            """
            
    rule MANORM_post_processing:
        '''
        input requirements:
        - '04_annotation', '02_peaks', sample1 + '_annotation_final_table.txt'
        - '04_annotation', '02_peaks', sample2 + '_annotation_final_table.txt'
        - '05_manorm','02_analysis', sample1_vs_sample2 + "_P", sample1_vs_sample2 + '_all_MAvalues.xls'),
        - '05_manorm','02_analysis', sample1_vs_sample2 + "_N", sample1_vs_sample2 + '_all_MAvalues.xls')

        output:
        '05_manorm','02_analysis', sample1_vs_sample2_post_processing.txt
        '''
        input:
            get_MANORM_post_processing,
            expand(join(out_dir,'04_annotation', '{sp}_annotation_final_table.txt'),sp=sp_list)
        params:
            rname = "17c_MANORM_processing",
            script = join(source_dir,'workflow','scripts','11_MAnormProcess.R'),
            anno_dir = join(out_dir,'04_annotation','02_peaks'),
            ma_dir = join(out_dir,'05_manorm', '02_analysis'),
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
        envmodules:
            config['R']
        output:
            o1 = temp(join(out_dir,'05_manorm','02_analysis', '{group_id}_post_processing.txt'))
        shell:
            """
            Rscript {params.script} \
                --samplename {params.gid_1} \
                --background {params.gid_2} \
                --peak_anno_g1 {params.anno_dir}/{params.gid_1}_annotation_final_table.txt \
                --peak_anno_g2 {params.anno_dir}/{params.gid_2}_annotation_final_table.txt \
                --pos_manorm {params.ma_dir}/{wildcards.group_id}_P/{wildcards.group_id}_MAvalues.xls \
                --neg_manorm {params.ma_dir}/{wildcards.group_id}_N/{wildcards.group_id}_MAvalues.xls \
                --output_file {output.o1}
            """

    rule MANORM_RMD:
        input:
            f1 = join(out_dir,'05_manorm','02_analysis', '{group_id}_post_processing.txt')
        params:
            rname = "17d_MANORM_RMD",
            R = join(source_dir,'workflow','scripts','12_MAnormAnnotation.Rmd'),
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
            p_id = peak_id
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'05_manorm','03_report','{group_id}_manorm_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(peak_in="{input.f1}", \
                    samplename="{params.gid_1}", \
                    background="{params.gid_2}", \
                    PeakIdnt="{params.p_id}"))'
            """