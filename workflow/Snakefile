'''
* Authors *
S. Sevilla
P. Homan
S. Kuhn
V. Koparde
* Overview *
- Multiplexed samples are split based on provided barcodes and named using provide manifests, maximum 10 samples
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged
* Requirements *
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml
import csv

###############################################################
# set snakemake config params
###############################################################
# dirs
source_dir = config['sourceDir']
out_dir = config['outputDir'].rstrip('/') + '/'
fastq_dir = config['fastqDir'].rstrip('/') + '/'

# manifests
sample_manifest = config['sampleManifest']
multiplex_manifest = config['multiplexManifest']
contrast_manifest = config['contrastManifest']

# user parameters
multiplex_flag = config['multiplexflag'].capitalize()
mismatch = config['mismatch']
species_ref = config['reference']
filter_length = config['filterlength']
phredQuality = config['phredQuality']
include_rRNA = config['includerRNA'].capitalize()
sp_junc = config['splicejunction'].upper()
anno_anchor=config['AnnoAnchor']
min_count = config['mincount']
nt_merge = str(config['ntmerge']) + 'nt'
peak_id = config['peakid'].upper()
DE_method = config['DEmethod'].upper()
manorm_w = config['MANormWidth']
manorm_d = config['MNormDistance']
sample_overlap = int(config['sampleoverlap'])
pval = config['pval']
fc = config['fc']

# STAR parameters
star_bam_limit = "50297600554"
star_align_type = config['alignEndsType']
star_align_intron = config['alignIntronMax']
star_align_sjdb = config['alignSJDBoverhangMin']
star_align_sj = config['alignSJoverhangMin']
star_align_transc = config['alignTranscriptsPerReadNmax']
star_align_windows = config['alignWindowsPerReadNmax']
star_filt_match = config['outFilterMatchNmin']
star_filt_readmatch = config['outFilterMatchNminOverLread']
star_filt_mismatch = config['outFilterMismatchNmax']
star_filt_readmm = config['outFilterMismatchNoverReadLmax']
star_filt_mm = config['outFilterMultimapNmax']
star_filt_mmscore = config['outFilterMultimapScoreRange']
star_filt_score = config['outFilterScoreMin']
star_filt_type = config['outFilterType']
star_sam_att = config['outSAMattributes']
star_sam_unmap = config['outSAMunmapped']
star_filt_sjmin = config['outSJfilterCountTotalMin']
star_filt_overhang = config['outSJfilterOverhangMin']
star_filt_sjreads = config['outSJfilterReads']
star_seed_mm = config['seedMultimapNmax']
star_seed_loci = config['seedNoneLociPerWindow']
star_seed_read = config['seedPerReadNmax']
star_seed_wind = config['seedPerWindowNmax']
star_sj = config['sjdbScore']
star_win_anchor = config['winAnchorMultimapNmax']

# modules, container
cont_dir = config['containerDir']
fastq_val = config['fastq_val']

# testing option
testing_option = config['testing_option']
###############################################################
# set parameters
###############################################################
#define threads
with open(join(out_dir,'config','cluster_config.yaml')) as file:
    CLUSTER = yaml.load(file, Loader=yaml.FullLoader)
getthreads=lambda rname:int(CLUSTER[rname]["threads"]) if rname in CLUSTER and "threads" in CLUSTER[rname] else int(CLUSTER["__default__"]["threads"])
getmemg=lambda rname:CLUSTER[rname]["mem"] if rname in CLUSTER and "mem" in CLUSTER[rname] else CLUSTER["__default__"]["mem"]
# some commands like sort require mem with upper G hence using getmemG
getmemG=lambda rname:getmemg(rname).replace("g","G")
# sort command sometimes run out of memory if all 100% of the RAM is given to sort... especially if you have cascading pipes
getmemG_80perc=lambda rname:str(int(int(getmemG(rname).split("G")[0])*0.8))+"G"

#read in index config file and assign paths
index_manifest = join(out_dir, 'config', 'index_config.yaml')

with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)

gen_path = index_list[species_ref]['gencodepath']
rseq_path = index_list[species_ref]['refseqpath']
can_path = index_list[species_ref]['canonicalpath']
intron_path = index_list[species_ref]['intronpath']
rmsk_path = index_list[species_ref]['rmskpath']
alias_path = index_list[species_ref]['aliaspath']
add_anno_path = index_list[species_ref]['additionalannopath']

#annotation config
annotation_config = join(out_dir,'config','annotation_config.txt')

#determine which umi separator to use
if(multiplex_flag == 'Y' or testing_option == "Y"):
    #demultiplexing addes rbc: to all demux files;
    umi_sep="rbc:"
else:
    # external demux uses an _
    umi_sep=config['umiSeparator']

#convert splice junction selection
if (sp_junc=="Y"):
    sp_junc = "TRUE"
else:
    sp_junc = "FALSE"

#convert rRNA selection
if (include_rRNA=="Y"):
    refseq_rrna = "TRUE"
else:
    refseq_rrna = "FALSE"

#set strand ids
strand_list=['P','N']

###############################################################
# create sample lists
###############################################################
#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

###############################################################
# snakemake functions
###############################################################
#get list of fq names based on multiplex name
def get_fq_names(wildcards):
    #example: {fastq_dir}/{filename}.fastq.gz
    fq = join(fastq_dir,multiplex_dict[wildcards.mp])
    return(fq)

# create command to move each fastq file from source location to pre-processing folder
def nondemux_cmd(wildcards):
  cmd_line = ''

  for k,v in multiplex_dict.items():
    for k2,v2 in samp_dict[k].items():
      cmd_line = 'cp ' + fastq_dir + v + ' ' + out_dir + k + '/02_preprocess/ultraplex_demux_' + k2 + '.fastq.gz; ' + cmd_line
  
  return(cmd_line)

# for each sample name, move the sample from the multiplexed folder to the sample folder
def rename_cmd(wildcards):
    cmd=''
    for sp in sp_list:
        #sub df to determine multiplex name
        df_sub = df_samples[(df_samples['sample']==sp)]

        # set source (rename_fq) and destination files
        source = join(out_dir, df_sub.iloc[0]['multiplex'], '02_preprocess/ultraplex_demux_' + sp + '.fastq.gz')
        destination = join(out_dir, '01_preprocess', '01_fastq', sp + '.fastq.gz')

        # create command
        cmd = 'mv ' + source + ' ' + destination + '; ' + cmd
    return(cmd)

#read in contrast list for manorm
def get_de_list():
    
    #read file
    with open(contrast_manifest) as f:
        reader = csv.reader(f, delimiter="\t")
        manorm_file = list(reader)

        #for each group in comparison list
        contrast_list=[]
        for group in manorm_file:
            
            #for each individual id
            for id in group:
                id = id.replace(",", "_vs_")
            
            #append final list
            contrast_list.append(id)
        
        #remove header
        contrast_list.pop(0)
    return(contrast_list)

#get sample name for demethod comparison - sample
def get_DEMETHOD_gid1(wildcards):
    gid = wildcards.group_id.split("_vs_")[0]
    return(gid)

#get sample name for demethod comparison - background
def get_DEMETHOD_gid2(wildcards):
    gid = wildcards.group_id.split("_vs_")[1]
    return(gid)

#get the input files for DE analysisanalysis
def get_MANORM_analysis_input(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir, '05_demethod', '01_input', gid_1 + '_' + peak_id + 'readPeaks_for' + DE_method + '.bed'),
                join(out_dir, '05_demethod', '01_input', gid_1 + '_Readsfor' + DE_method + '_' + wildcards.strand + '.bed'),
                join(out_dir, '05_demethod', '01_input', gid_1 + '_' + peak_id + 'readPeaks_for' + DE_method + '_' + wildcards.strand + '.bed'),
                join(out_dir, '05_demethod', '01_input', gid_2 + '_' + peak_id + 'readPeaks_for' + DE_method + '.bed'),
                join(out_dir, '05_demethod', '01_input', gid_2 + '_Readsfor' + DE_method + '_' + wildcards.strand + '.bed'),
                join(out_dir, '05_demethod', '01_input', gid_2 + '_' + peak_id + 'readPeaks_for' + DE_method + '_' + wildcards.strand + '.bed')]
    return(input_list)

def get_DIFFBIND_analysis_input(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir,'05_demethod', '01_input', gid_1 + '_Readsfor' + DE_method + '_' + wildcards.strand + '.bam'),
                join(out_dir,'05_demethod', '01_input', gid_1 + '_' + peak_id + 'readPeaks_for' + DE_method + '_' + wildcards.strand + '.bed'),
                join(out_dir,'05_demethod', '01_input', gid_2 + '_Readsfor' + DE_method + '_' + wildcards.strand + '.bam'),
                join(out_dir,'05_demethod', '01_input', gid_2 + '_' + peak_id + 'readPeaks_for' + DE_method + '_' + wildcards.strand + '.bed')]
    return(input_list)

#get input files for post-processing MANORM
def get_MANORM_post_processing(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [
                join(out_dir,'05_demethod','02_analysis', wildcards.group_id, wildcards.group_id + '_P', wildcards.group_id + '_' + peak_id + 'readPeaks_MAvalues.xls'),
                join(out_dir,'05_demethod','02_analysis', wildcards.group_id, wildcards.group_id + '_N', wildcards.group_id + '_' + peak_id + 'readPeaks_MAvalues.xls'),
                join(out_dir,'02_bam','02_dedup',gid_1 + '.dedup.si.bam'),
                join(out_dir,'02_bam','02_dedup',gid_2 + '.dedup.si.bam'),
                join(out_dir,'04_annotation', gid_1 + '_annotation_' + peak_id + 'readPeaks_final_table.txt'),
                join(out_dir,'04_annotation', gid_2 + '_annotation_' + peak_id + 'readPeaks_final_table.txt'),
                join(out_dir,'03_peaks','02_SAF',gid_1 + '_' + peak_id + 'readPeaks.SAF'),
                join(out_dir,'03_peaks','02_SAF',gid_2 + '_' + peak_id + 'readPeaks.SAF')]
    return(input_list)

###############################################################
# main code
###############################################################
#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep=",")
df_samples = pd.read_csv(sample_manifest,sep=",")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

#determine barcode length
barcode_length = int(len(df_samples.iloc[0,3].replace('N',''))) #length of barcode

#create manorm list
if DE_method == "MANORM" or DE_method == "DIFFBIND":
    contrast_list = get_de_list()

###############################################################
# rule all
###############################################################
# set rule_all inputs depending on flags:

# if samples are running MANORM
if DE_method == "MANORM":
    input_demethod_reports = [expand(join(out_dir,'05_demethod','02_analysis','{group_id}', '{group_id}_' + peak_id + 'readPeaks_manorm_report.html'),group_id=contrast_list),
    expand(join(out_dir,'05_demethod','02_analysis', '{group_id}', '{group_id}_' + peak_id + 'readPeaks_post_processing.txt'), group_id=contrast_list)]

elif DE_method == "DIFFBIND":
    #input_demethod_reports = [expand(join(out_dir,'05_demethod','02_analysis','{group_id}', '{group_id}_manorm_report.html'),group_id=contrast_list),
    #expand(join(out_dir,'05_demethod','02_analysis', '{group_id}', '{group_id}_post_processing.txt'), group_id=contrast_list)]

    #testing
    input_demethod_reports = [expand(join(out_dir,'05_demethod','02_analysis','{group_id}','{group_id}_diffbind_report.html'),group_id=contrast_list)]

else:
    input_demethod_reports = [expand(join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + 'readPeaks_annotation_complete.txt'),sp=sp_list)]

#local rules
localrules: nondemux, rename_fastqs, multiqc

#structure
#mp
##01_qc_post
##02_preprocess

#01_preprocess
##01_fastq
##02_alignment
####01_unmapped
#02_bam
##01_merged
##02_dedup
#03_peaks
##01_bed
##02_SAF
#04_annotation
##01_project
##02_peaks
#05_demethod
##01_input
##02_analysis

#qc
##01_qc_post
##02_qc_screen_species
##02_qc_screen_rrna
##02_qc_screen_species

rule all:
    input:
        ################################################################################################
        # required, non  files
        # non-rule: check sample fastq files exist
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list),

        # rename_fastqs
        expand(join(out_dir,'01_preprocess','01_fastq','{sp}.fastq.gz'),sp=sp_list),       
         
        # index_stats
        expand(join(out_dir,'02_bam','01_merged','{sp}.si.bam'), sp=sp_list),

        # multiqc
        join(out_dir,'qc','multiqc_report.html'),

        # qc_troubleshoot
        join(out_dir,'qc','qc_report_troubleshooting.html'),

        # dedup
        expand(join(out_dir,'02_bam','02_dedup','{sp}.dedup.si.bam'),sp=sp_list),

        # bgzip_beds
        expand(join(out_dir,'03_peaks','01_bed','{sp}_ALLreadPeaks.bed.gz'),sp=sp_list),
        expand(join(out_dir,'03_peaks','01_bed','{sp}_UNIQUEreadPeaks.bed.gz'),sp=sp_list),
        
        # feature_counts
        expand(join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_uniqueCounts.txt'), sp=sp_list),
        expand(join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_uniqueCounts.txt'), sp=sp_list),
        
        # annotation_report
        expand(join(out_dir,'04_annotation', '{sp}_annotation_' + peak_id + 'readPeaks_final_report.html'),sp=sp_list),
        expand(join(out_dir,'04_annotation', '{sp}_annotation_' + peak_id + 'readPeaks_final_table.txt'),sp=sp_list),

        # MANORM or DIFFBIND
        input_demethod_reports,

        ###############################################################################################
        # intermediate tmp files
        ###############################################################################################        
        # # qc barcode
        # expand(join(out_dir,'{mp}','01_qc_post','{mp}_barcode_counts.txt'),mp=mp_list),
        # expand(join(out_dir,'{mp}','01_qc_post','{mp}_barcode.png'),mp=mp_list),
        # expand(join(out_dir,'{mp}','01_qc_post','{mp}_barcode.txt'),mp=mp_list),

        # # demux OR nondemux
        # expand(join(out_dir,'{mp}', '02_preprocess','ultraplex_demux_5bc_no_match.fastq.gz'), mp=samp_dict.keys()),

        # # qc_fastq
        # expand(join(out_dir, 'qc', '01_qc_post','{sp}_fastqc.html'),sp=sp_list),

        # # qc_screen_validator
        # expand(join(out_dir, 'qc', '02_qc_screen_species','{sp}_screen.txt'),sp=sp_list),
        # expand(join(out_dir, 'qc', '02_qc_screen_rrna','{sp}_screen.txt'),sp=sp_list),

        # # star
        # expand(join(out_dir,'01_preprocess','02_alignment','{sp}_Aligned.sortedByCoord.out.bam'),sp=sp_list),

        # create_beds_safs
        # expand(join(out_dir,'03_peaks','01_bed','{sp}_ALLreadPeaks.bed'),sp=sp_list),
        # expand(join(out_dir,'03_peaks','01_bed','{sp}_UNIQUEreadPeaks.bed'),sp=sp_list),
        # expand(join(out_dir,'03_peaks','02_SAF','{sp}_ALLreadPeaks.SAF'),sp=sp_list),
        # expand(join(out_dir,'03_peaks','02_SAF','{sp}_UNIQUEreadPeaks.SAF'),sp=sp_list),
       
        # # project_annotations
        # join(out_dir,'04_annotation', '01_project','annotations.txt'),

        # # peak_junctions
        # expand(join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),sp=sp_list),
        
        # # peak_Transcripts
        # expand(join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_transcripts_{AnnoStrand}.txt'),AnnoStrand=["SameStrand","OppoStrand"],sp=sp_list),

        # # peak_ExonIntron
        # expand(join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_IntronExon_{AnnoStrand}.txt'),AnnoStrand=["SameStrand","OppoStrand"],sp=sp_list),

        # # peak_RMSK
        # expand(join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_RMSK_{AnnoStrand}.txt'),AnnoStrand=["SameStrand","OppoStrand"],sp=sp_list),

        # # peak_process
        # expand(join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + 'readPeaks_annotation_complete.txt'),sp=sp_list),

# common and other SMK
if source_dir == "":
    include: "workflow/rules/common.smk"
    include: "workflow/rules/other.smk"
else:
    include: join(source_dir,"workflow/rules/common.smk")
    include: join(source_dir,"workflow/rules/other.smk")

###############################################################
# snakemake rules
###############################################################
#pipeline branches to demultiplex, if necessary
if (multiplex_flag == 'Y'):
    rule qc_barcode:
        """
        generate counts of barcodes and output to text file
        will run python script that determines barcode expected and generates mismatches based on input
        output barplot with top barcode counts
        """
        input:
            fq = get_fq_names,
        params:
            rname = "01a_qc_barcode",
            memG = getmemG_80perc("qc_barcode"),
            R = join(source_dir,'workflow', 'scripts', '02_barcode_qc.R'),
            base = join(out_dir,'{mp}','01_qc_post'),
            mm = mismatch,
            bc_len = barcode_length,
            start_pos = 6 if barcode_length==6 else 4
        threads: getthreads("qc_barcode")
        envmodules:
            config['R'],
        output:
            counts = temp(join(out_dir,'{mp}','01_qc_post','{mp}_barcode_counts.txt')),
            png = temp(join(out_dir,'{mp}','01_qc_post','{mp}_barcode.png')),
            txt = temp(join(out_dir,'{mp}','01_qc_post','{mp}_barcode.txt'))
        shell:
            """
            set -exo pipefail
            gunzip -c  {input.fq} \\
                | awk 'NR%4==2 {{print substr($0, {params.start_pos}, {params.bc_len});}}' \\
                | LC_ALL=C sort --buffer-size={params.memG} --parallel={threads} --temporary-directory=/lscratch/${{SLURM_JOB_ID}} -n \\
                | uniq -c > {output.counts};
            
            Rscript {params.R} --sample_manifest {sample_manifest} \\
                --multiplex_manifest {multiplex_manifest} \\
                --barcode_input {output.counts} \\
                --mismatch {params.mm} \\
                --mpid {wildcards.mp} \\
                --output_dir {params.base}
            """

    rule demultiplex:
        """
        https://github.com/ulelab/ultraplex

        NOTE: our SLURM system does not allow the use of --sbatchcompression which is recommended
        for increase in speed with --ultra. When the --sbatchcompression is used on our system, files 
        do not get compressed and will be transferred using a significant amount of disc space. 

        file_name                   multiplex
        SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1
        multiplex       sample          group       barcode     adaptor
        SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
        SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
        """
        input:
            f1 = get_fq_names,
            barcodes = join(out_dir,'manifests','{mp}_barcode_manifest.txt')
        params:
            rname='01b_demultiplex',
            ml = filter_length,
            mm = mismatch,
            pq = phredQuality,
            out_dir=join(out_dir,'{mp}', '02_preprocess')
        threads: getthreads('demultiplex')
        envmodules:
            config['ultraplex'],
        output:
            fastq = temp(join(out_dir,'{mp}', '02_preprocess','ultraplex_demux_5bc_no_match.fastq.gz')),
        shell:
            """
            set -exo pipefail

            # set tmp dir
            tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
            export tmp_dir
            
            # run ultraplex to remove adaptors, separate barcodes
            # output files to tmp scratch dir
            ultraplex \\
                --threads {threads} \\
                --barcodes {input.barcodes} \\
                --directory $tmp_dir \\
                --inputfastq {input.f1} \\
                --final_min_length {params.ml} \\
                --phredquality {params.pq} \\
                --fiveprimemismatches {params.mm} \\
                --ultra 

            # move files to final location after they are zipped
            mv $tmp_dir/* {params.out_dir}
            """

else:
    rule nondemux:
        """
        creates a dummy file for nondemux projects
        moves fastq files to output location
        """
        input:
            f1 = get_fq_names,
        params:
            rname = '01_nondemux',
            cmd = nondemux_cmd
        output:
            fastq = temp(join(out_dir,'{mp}', '02_preprocess','ultraplex_demux_5bc_no_match.fastq.gz')),
        shell:
            """
            set -exo pipefail

            # create empty file
            touch {output.fastq}

            # moves files to project output dir
            {params.cmd}
            """

rule rename_fastqs:
    """
    moves all demultiplexed files from individual to single processing folder 
    /output/individual_multiplex_name/02_preprocess 
    /output/01_preprocess
    """
        input:
            fastqs = expand(join(out_dir,'{mp}', '02_preprocess','ultraplex_demux_5bc_no_match.fastq.gz'),mp=mp_list),
        params:
            rname = '02_rename_fq',
            cmd = rename_cmd
        output:
            fastq = expand(join(out_dir,'01_preprocess','01_fastq','{sp}.fastq.gz'),sp=sp_list),
        shell:
            """
            set -exo pipefail

            # Rename files
            {params.cmd} 
            """

rule qc_fastq:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        fastq = join(out_dir,'01_preprocess','01_fastq','{sp}.fastq.gz')
    params:
        rname='03_qc_fastq_post',
        base = join(out_dir, 'qc', '01_qc_post'),
    envmodules:
        config['fastqc']
    output:
        html = temp(join(out_dir, 'qc', '01_qc_post','{sp}_fastqc.html'))
    shell:
        """
        set -exo pipefail

        # run FASTQC
        fastqc {input.fastq} -o {params.base}
        """

rule qc_screen_validator:
    """
    #fastq screen
    - this will align first to human, mouse, bacteria then will align to rRNA
    must run fastq_screen as two separate commands - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    - fastq validator
    Quality-control step to ensure the input FastQC files are not corrupted or
    incomplete prior to running the entire workflow.
    @Input:
        Raw FastQ file (scatter)
    @Output:
        Log file containing any warnings or errors on file
    """
    input:
        filtered = join(out_dir,'01_preprocess','01_fastq','{sp}.fastq.gz')
    params:
        rname='04_qc_screen_validator',
        fastq_v = fastq_val,
        tmp = join('{sp}.fastq'),
        base_species = join(out_dir, 'qc', '02_qc_screen_species'),
        conf_species = join(source_dir,'config','fqscreen_species_config.conf'),
        base_rrna = join(out_dir, 'qc', '02_qc_screen_rrna'),
        conf_rrna = join(source_dir,'config','fqscreen_rrna_config.conf'),
        base_val = join(out_dir,'qc'),
    threads: getthreads("qc_screen_validator")
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        species = temp(join(out_dir, 'qc', '02_qc_screen_species','{sp}_screen.txt')),
        screen = temp(join(out_dir, 'qc', '02_qc_screen_rrna','{sp}_screen.txt')),
        log = join(out_dir,'qc','{sp}.validated.fastq.log'),
    shell:
        """
        set -exo pipefail
        
        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir

        # Gzip input files
        gunzip -c {input.filtered} > ${{tmp_dir}}/{params.tmp};
        
        # Run FastQ Screen
        fastq_screen --conf {params.conf_species} \\
            --outdir {params.base_species} \\
            --threads {threads} \\
            --subset 1000000 \\
            --aligner bowtie2 \\
            --force \\
            ${{tmp_dir}}/{params.tmp};
        fastq_screen --conf {params.conf_rrna} \\
            --outdir {params.base_rrna} \\
            --threads {threads} \\
            --subset 1000000 \\
            --aligner bowtie2 \\
            --force \\
            ${{tmp_dir}}/{params.tmp};
        
        # Remove tmp gzipped file
        rm ${{tmp_dir}}/{params.tmp}
        
        # Run FastQ Validator
        mkdir -p {params.base_val}
        {params.fastq_v} \\
            --disableSeqIDCheck \\
            --noeof \\
            --printableErrors 100000000 \\
            --baseComposition \\
            --avgQual \\
            --file {input.filtered} > {output.log};
        """

rule star:
    """
    STAR Alignment
    https://github.com/alexdobin/STAR/releases

    """
    input:
        f1 = join(out_dir,'01_preprocess','01_fastq','{sp}.fastq.gz')
    params:
        rname = '05_star',
        s_index=index_list[species_ref]['stardir'],
        s_gtf=index_list[species_ref]['stargtf'],
        s_atype = star_align_type,
        s_intron = star_align_intron, 
        s_sjdb = star_align_sjdb,
        s_asj = star_align_sj,
        s_transc = star_align_transc,
        s_windows = star_align_windows,
        s_bam_limit = star_bam_limit,
        s_match = star_filt_match,
        s_readmatch = star_filt_readmatch,
        s_mismatch = star_filt_mismatch,
        s_readmm = star_filt_readmm,
        s_fmm = star_filt_mm,
        s_mmscore = star_filt_mmscore,
        s_score = star_filt_score,
        s_ftype = star_filt_type,
        s_att = star_sam_att,
        s_unmap = star_sam_unmap,
        s_sjmin = star_filt_sjmin,
        s_overhang = star_filt_overhang,
        s_sjreads = star_filt_sjreads,
        s_smm = star_seed_mm,
        s_loci = star_seed_loci,
        s_read = star_seed_read,
        s_wind = star_seed_wind,
        s_sj = star_sj,
        s_anchor = star_win_anchor,
        out_prefix = '{sp}_'
    envmodules:
        config['star']
    threads: getthreads("star")
    output:
        unmapped = join(out_dir,'01_preprocess','02_alignment','01_unmapped','{sp}.unmapped.out'),
        bam = temp(join(out_dir,'01_preprocess','02_alignment','{sp}_Aligned.sortedByCoord.out.bam')),
        log = join(out_dir,'log','STAR','{sp}.log')
    shell:
        """
        set -exo pipefail

        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir

        # STAR cannot handle sorting large files - allow samtools to sort output files
        STAR \
        --runMode alignReads \
        --genomeDir {params.s_index} \
        --sjdbGTFfile {params.s_gtf} \
        --readFilesCommand zcat \
        --readFilesIn {input.f1} \
        --outFileNamePrefix $tmp_dir/{params.out_prefix} \
        --outReadsUnmapped Fastx \
        --outSAMtype BAM Unsorted \
        --alignEndsType {params.s_atype} \
        --alignIntronMax {params.s_intron} \
        --alignSJDBoverhangMin {params.s_sjdb} \
        --alignSJoverhangMin {params.s_asj} \
        --alignTranscriptsPerReadNmax {params.s_transc} \
        --alignWindowsPerReadNmax {params.s_windows} \
        --limitBAMsortRAM {params.s_bam_limit} \
        --outFilterMatchNmin {params.s_match} \
        --outFilterMatchNminOverLread {params.s_readmatch} \
        --outFilterMismatchNmax {params.s_mismatch} \
        --outFilterMismatchNoverReadLmax {params.s_readmm} \
        --outFilterMultimapNmax {params.s_fmm} \
        --outFilterMultimapScoreRange {params.s_mmscore} \
        --outFilterScoreMin {params.s_score} \
        --outFilterType {params.s_ftype} \
        --outSAMattributes {params.s_att} \
        --outSAMunmapped {params.s_unmap} \
        --outSJfilterCountTotalMin {params.s_sjmin} \
        --outSJfilterOverhangMin {params.s_overhang} \
        --outSJfilterReads {params.s_sjreads} \
        --seedMultimapNmax {params.s_smm} \
        --seedNoneLociPerWindow {params.s_loci} \
        --seedPerReadNmax {params.s_read} \
        --seedPerWindowNmax {params.s_wind} \
        --sjdbScore {params.s_sj} \
        --winAnchorMultimapNmax {params.s_anchor}

        # sort file
        samtools sort -m 80G -T $tmp_dir $tmp_dir/${params.out_prefix}Aligned.out.bam -o $tmp_dir/${params.out_prefix}Aligned.sortedByCoord.out.bam

        # move STAR files and final log file to output
        mv $tmp_dir/{params.out_prefix}Aligned.sortedByCoord.out.bam {output.bam}
        mv $tmp_dir/{params.out_prefix}Log.final.out {output.log}
        
        # move mates to unmapped file
        touch {output.unmapped}
        for f in $tmp_dir/{params.out_prefix}Unmapped.out.mate*; do cat $f >> {output.unmapped}; done
        """

rule index_stats:
    """
    sort, index files
    run samstats on files
    """
    input:
        bam = rules.star.output.bam
    params:
        rname='06_index_stats',
    envmodules:
        config['samtools']
    threads: getthreads("index_stats")
    output:
        bam = join(out_dir,'02_bam','01_merged','{sp}.si.bam'),
        samstat = temp(join(out_dir, 'qc', '01_qc_post','{sp}_samstats.txt'))
    shell:
        """
        set -exo pipefail

        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir
        
        # Index
        cp {input.bam} {output.bam}
        samtools index -@ {threads} {output.bam};
        
        # Run samstats
        samtools stats --threads {threads} {output.bam} > {output.samstat}
        """

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir, 'qc', '01_qc_post','{sp}_samstats.txt'),sp=sp_list),
        f2 = expand(join(out_dir, 'qc', '01_qc_post','{sp}_fastqc.html'),sp=sp_list),
        f3 = expand(join(out_dir, 'qc', '02_qc_screen_species','{sp}_screen.txt'),sp=sp_list),
        f4 = expand(join(out_dir, 'qc', '02_qc_screen_rrna','{sp}_screen.txt'),sp=sp_list),
    params:
        out_dir = join(out_dir,'qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_post = join(out_dir, 'qc', '01_qc_post'),
        dir_screen_species = join(out_dir, 'qc', '02_qc_screen_species'),
        dir_screen_rrna = join(out_dir, 'qc', '02_qc_screen_rrna'),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'qc', 'multiqc_report.html')
    shell:
        """
        set -exo pipefail

        multiqc -f -v \\
            -c {params.qc_config} \\
            -d -dd 1 \\
            {params.dir_post} \\
            {params.dir_screen_rrna} \\
            {params.dir_screen_species} \\
            -o {params.out_dir}
        """

# Pipeline splits to handle multiplexed QC vs non-multiplexed QC
if (multiplex_flag == 'Y'):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            log_list = expand(join(out_dir,'log','STAR','{sp}.log'),sp=sp_list),
            txt_bc = expand(join(out_dir,'{mp}', '01_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
            png_bc = expand(join(out_dir,'{mp}', '01_qc_post','{mp}_barcode.png'),mp=samp_dict.keys())
        params:
            rname = "08_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','03_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report_troubleshooting.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(log_list = "{input.log_list}", \
                    b_txt = "{input.txt_bc}"))'
            """
else:
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            log_list = expand(join(out_dir,'log','STAR','{sp}.log'),sp=sp_list),
        params:
            rname = "08_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','03_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report_troubleshooting.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(log_list = "{input.log_list}"))'
            """

rule dedup:
    """
    deduplicate reads
    sort,index dedup.bam file
    get header of dedup file
    """
    input:
        f1 = rules.index_stats.output.bam
    params:
        rname='09_dedup',
        umi = umi_sep,
        base = '{sp}.unmasked',
    envmodules:
        config['umitools'],
        config['samtools']
    threads: getthreads("dedup")
    output:
        bam = join(out_dir,'02_bam','02_dedup','{sp}.dedup.si.bam'),
    shell:
        """
        set -exo pipefail

        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir
 
        # Run UMI Tools Deduplication
        echo "Using the following UMI seperator: {params.umi}"
        umi_tools dedup \\
            -I {input.f1} \\
            --method unique \\
            --multimapping-detection-method=NH \\
            --umi-separator={params.umi} \\
            -S ${{tmp_dir}}/{params.base}.bam \\
            --log2stderr;
        
        # Sort and Index
        samtools sort --threads {threads} -m 10G -T ${{tmp_dir}} \\
            ${{tmp_dir}}/{params.base}.bam \\
            -o {output.bam};
        samtools index -@ {threads} {output.bam};
        """

rule create_beds_safs:
    """
    split dedup file into unique files
    create bed file from the deduped unique file and deduped all file
    create SAF files from the deduped unique bed and all bed files
    """
    input:
        bam = rules.dedup.output.bam
    params:
        rname='10_create_beds_safs',
        base = '{sp}.dedup',
        pyscript = join(source_dir,'workflow','scripts','09_bam_to_unique_bam.py'),
        memG=getmemG_80perc("create_beds_safs")
    envmodules:
        config['samtools'],
        config['bedtools'],
        config['python']
    threads: getthreads("create_beds_safs")
    output:
        bed_all = temp(join(out_dir,'03_peaks','01_bed','{sp}_ALLreadPeaks.bed')),
        bed_unique = temp(join(out_dir,'03_peaks','01_bed','{sp}_UNIQUEreadPeaks.bed')),
        saf_all = temp(join(out_dir,'03_peaks','02_SAF','{sp}_ALLreadPeaks.SAF')),
        saf_unique = temp(join(out_dir,'03_peaks','02_SAF','{sp}_UNIQUEreadPeaks.SAF')),
    shell:
        """
        set -exo pipefail
        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir
        
        # if read alignment has tag "NH:i:1" then it is an unique alignment
        python {params.pyscript} --inputBAM {input.bam} --outputBAM ${{tmp_dir}}/{params.base}.unique.bam
        samtools index -@ {threads} ${{tmp_dir}}/{params.base}.unique.bam;
        
        # Create SAFs
        bedtools bamtobed -split -i {input.bam} \\
            | LC_ALL=C sort --buffer-size={params.memG} --parallel={threads} --temporary-directory=$tmp_dir -k1,1V -k2,2n  > {output.bed_all};
        bedtools bamtobed -split -i ${{tmp_dir}}/{params.base}.unique.bam \\
            | LC_ALL=C sort --buffer-size={params.memG} --parallel={threads} --temporary-directory=$tmp_dir -k1,1V -k2,2n > {output.bed_unique};
        awk '{{OFS="\\t";print "GeneID","Chr","Start","End","Strand"}}' > {output.saf_all}
        awk '{{OFS="\\t";print "GeneID","Chr","Start","End","Strand"}}' > {output.saf_unique}
        bedtools merge \\
            -c 6 -o count,distinct \\
            -bed -s -d 50 \\
            -i {output.bed_all} \\
            | awk '{{OFS="\\t"; print $1":"$2"-"$3"_"$5,$1,$2,$3,$5}}' >> {output.saf_all};
        bedtools merge \\
            -c 6 -o count,distinct \\
            -bed -s -d 50 \\
            -i {output.bed_unique} \\
            | awk '{{OFS="\\t"; print $1":"$2"-"$3"_"$5,$1,$2,$3,$5}}' >> {output.saf_unique}
        """

rule bgzip_beds:
    """
    To minimize disc space bed files should be gzipped. However, downstream uses of the files
    require that they are not zipped. Files are copied before zipping to ensure the original files
    are accessible until the SM process has been completed
    """
    input:
        bed_all = rules.create_beds_safs.output.bed_all,
        bed_unique = rules.create_beds_safs.output.bed_unique
    output:
        bed_all = join(out_dir,'03_peaks','01_bed','{sp}_ALLreadPeaks.bed.gz'),
        bed_unique = join(out_dir,'03_peaks','01_bed','{sp}_UNIQUEreadPeaks.bed.gz'),
    params:
        rname='11_bgzip_beds',
    envmodules:
        config['samtools'],
    threads: getthreads("bgzip_beds")
    shell:
        """
        set -exo pipefail
        # set tmp dir
        tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        export tmp_dir
        
        cp {input.bed_all} $tmp_dir/all.bed
        bgzip --threads {threads} --force $tmp_dir/all.bed
        mv $tmp_dir/all.bed.gz {output.bed_all}
        tabix -p bed -f {output.bed_all}

        cp {input.bed_unique} $tmp_dir/unique.bed
        bgzip --threads {threads} --force $tmp_dir/unique.bed
        mv $tmp_dir/unique.bed.gz {output.bed_unique}
        tabix -p bed -f {output.bed_unique}
        """

rule feature_counts:
    """
    Unique reads (fractional counts correctly count splice reads for each peak.
    When peaks counts are combined for peaks connected by splicing in Rscript)
    Include Multimap reads - MM reads given fractional count based on # of mapping
    locations. All spliced reads also get fractional count. So Unique reads can get
    fractional count when spliced peaks combined in R script the summed counts give
    whole count for the unique alignement in combined peak.
    http://manpages.ubuntu.com/manpages/bionic/man1/featureCounts.1.html
    Output summary
    - Differences within any folder (allreadpeaks or uniquereadpeaks) should ONLY be the counts column -
    as this represent the number of peaks that were uniquely identified (uniqueCounts) or the number of peaks MM (allFracMMCounts)
    - Differences within folders (03_allreadpeaks, 03_uniquereadpeaks) will be the peaks identified, as the first takes
    all reads as input and the second takes only unique reads as input
    """
    input:
        bam = rules.dedup.output.bam,
        saf_all = rules.create_beds_safs.output.saf_all,
        saf_unique = rules.create_beds_safs.output.saf_unique
    params:
        rname='12_feature_counts',
    threads: getthreads("feature_counts")
    envmodules:
        config['subread']
    output:
        all_unique = join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_uniqueCounts.txt'),
        all_mm = join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_FracMMCounts.txt'),
        all_total = join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_totalCounts.txt'),
        unique_unique = join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_uniqueCounts.txt'),
        unique_mm = join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_FracMMCounts.txt'),
        unique_total = join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_totalCounts.txt'),
        all_unique_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_uniqueCounts.txt.jcounts'),
        all_mm_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_ALLreadpeaks_FracMMCounts.txt.jcounts'),
        unique_unique_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_uniqueCounts.txt.jcounts'),
        unique_mm_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_UNIQUEreadpeaks_FracMMCounts.txt.jcounts'),
    shell:
        """
        set -exo pipefail
        # Run for allreadpeaks
        featureCounts -F SAF \\
            -a {input.saf_all} \\
            -O \\
            -J \\
            --fraction \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.all_unique} \\
            {input.bam};
        featureCounts -F SAF \\
            -a {input.saf_all} \\
            -M \\
            -O \\
            -J \\
            --fraction \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.all_mm} \\
            {input.bam};
        featureCounts -F SAF \\
            -a {input.saf_all} \\
            -M \\
            -O \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.all_total} \\
            {input.bam};
        # Run for uniquereadpeaks
        featureCounts -F SAF \\
            -a {input.saf_unique} \\
            -O \\
            -J \\
            --fraction \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.unique_unique} \\
            {input.bam};
        featureCounts -F SAF \\
            -a {input.saf_unique} \\
            -M \\
            -O \\
            -J \\
            --fraction \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.unique_mm} \\
            {input.bam}
        featureCounts -F SAF \\
            -a {input.saf_unique} \\
            -M \\
            -O \\
            --minOverlap 1 \\
            -s 1 \\
            -T {threads} \\
            -o {output.unique_total} \\
            {input.bam}    
        """

rule project_annotations:
    """
    generate annotation table once per project
    """
    params:
        rname='13_project_annotations',
        script = join(source_dir,'workflow','scripts','04_annotation.R'),
        ref_sp = species_ref,
        rrna_flag = refseq_rrna,
        a_path = alias_path,
        g_path = gen_path,
        rs_path = rseq_path,
        c_path = can_path,
        i_path = intron_path,
        r_path = rmsk_path,
        custom_path = add_anno_path,
        base = join(out_dir,'04_annotation', '01_project/'),
        a_config = annotation_config,
    envmodules:
        config['R']
    output:
        anno = join(out_dir,'04_annotation', '01_project','annotations.txt'),
        nc_anno = join(out_dir,'04_annotation', '01_project','ncRNA_annotations.txt'),
        ref_gencode = join(out_dir,'04_annotation', '01_project','ref_gencode.txt'),
    shell:
        """
        Rscript {params.script} \\
            --ref_species {params.ref_sp} \\
            --refseq_rRNA {params.rrna_flag} \\
            --alias_path {params.a_path} \\
            --gencode_path {params.g_path} \\
            --refseq_path {params.rs_path} \\
            --canonical_path {params.c_path} \\
            --intron_path {params.i_path} \\
            --rmsk_path {params.r_path} \\
            --custom_path {params.custom_path} \\
            --out_dir {params.base} \\
            --reftable_path {params.a_config}
        """

# Pipeline splits depending on DE_method or none
if DE_method=="MANORM" or DE_method=="DIFFBIND":
    rule peak_junctions:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_uniqueCounts.txt'),
            unique_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_uniqueCounts.txt.jcounts'),
            mm = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_FracMMCounts.txt'),
            mm_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_FracMMCounts.txt.jcounts'),
            total = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_totalCounts.txt'),
            anno = rules.project_annotations.output.anno
        params:
            rname = '14_peak_junctions',
            script = join(source_dir,'workflow','scripts','05_Anno_junctions.R'),
            functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
            bashscript = join(source_dir,'workflow','scripts','05_get_site2peak_lookup.sh'),
            pyscript = join(source_dir,'workflow','scripts','05_jcounts2peakconnections.py'),
            p_type = peak_id,
            junc = sp_junc,
            anchor= anno_anchor,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = species_ref,
            out = join(out_dir,'04_annotation','02_peaks/',),
            out_de = join(out_dir,'05_demethod','01_input/',),
            error = join(out_dir,'04_annotation','read_depth_error.txt')
        envmodules:
            config['R'],
            config['bedtools'],
        threads: getthreads("peak_annotations")
        output:
            splice_table = join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + '_connected_peaks.txt'),
            text = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
            bed = (join(out_dir,'05_demethod', '01_input', '{sp}_' + peak_id + 'readPeaks_for' + DE_method + '.bed')),
            p_bed = (join(out_dir,'05_demethod', '01_input', '{sp}_' + peak_id + 'readPeaks_for' + DE_method + '_P.bed')),
            n_bed = (join(out_dir,'05_demethod', '01_input', '{sp}_' + peak_id + 'readPeaks_for' + DE_method + '_N.bed'))
        shell:
            '''
            # Setup tmp directory
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"
            else
                tmp_dir="{out_dir}01_preprocess/07_rscripts/"
                if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
            fi
            
            #bash script to run bedtools and get site2peak lookuptable
            bash {params.bashscript} {input.mm_jcounts} {input.mm} {params.sp}_{params.p_type} {params.out} {params.pyscript} 
            # above bash script will create {output.splice_table}
            Rscript {params.script} \\
                --rscript {params.functions} \\
                --peak_type {params.p_type} \\
                --peak_unique {input.unique} \\
                --peak_all {input.mm} \\
                --peak_total {input.total} \\
                --join_junction {params.junc} \\
                --anno_anchor {params.anchor} \\
                --read_depth {params.r_depth} \\
                --demethod {params.d_method} \\
                --sample_id {params.sp} \\
                --ref_species {params.ref_sp} \\
                --splice_table {output.splice_table} \\
                --tmp_dir ${{tmp_dir}} \\
                --out_dir {params.out} \\
                --out_file {output.text} \\
                --out_dir_DEP {params.out_de} \\
                --output_file_error {params.error}
            '''
else:
    rule peak_junctions:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_uniqueCounts.txt'),
            unique_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_uniqueCounts.txt.jcounts'),
            mm = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_FracMMCounts.txt'),
            mm_jcounts = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_FracMMCounts.txt.jcounts'),
            total = join(out_dir,'03_peaks','03_counts','{sp}_' + peak_id + 'readpeaks_totalCounts.txt'),
            anno = rules.project_annotations.output.anno
        params:
            rname = '14_peak_junctions',
            script = join(source_dir,'workflow','scripts','05_Anno_junctions.R'),
            functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
            bashscript = join(source_dir,'workflow','scripts','05_get_site2peak_lookup.sh'),
            pyscript = join(source_dir,'workflow','scripts','05_jcounts2peakconnections.py'),
            p_type = peak_id,
            junc = sp_junc,
            anchor= anno_anchor,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = species_ref,
            out = join(out_dir,'04_annotation','02_peaks/',),
            out_m = join(out_dir,'05_demethod','01_input/',),
            error = join(out_dir,'04_annotation','read_depth_error.txt')
        threads: getthreads("peak_annotations")
        envmodules:
            config['R'],
            config['bedtools'],
            config['python']
        output:
            text = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
            splice_table = join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + '_connected_peaks.txt'),
        shell:
            '''
            # Setup tmp directory
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"
            else
                tmp_dir="{out_dir}01_preprocess/07_rscripts/"
                if [[ ! -d $tmp_dir ]]; then mkdir $tmp_dir; fi
            fi
            
            #bash script to run bedtools and get site2peak lookuptable
            bash {params.bashscript} {input.mm_jcounts} {input.mm} {params.sp}_{params.p_type} {params.out} {params.pyscript}
            # above bash script will create {output.splice_table}
                
            Rscript {params.script} \\
                --rscript {params.functions} \\
                --peak_type {params.p_type} \\
                --peak_unique {input.unique} \\
                --peak_all {input.mm} \\
                --peak_total {input.total} \\
                --join_junction {params.junc} \\
                --anno_anchor {params.anchor} \\
                --read_depth {params.r_depth} \\
                --demethod {params.d_method} \\
                --sample_id {params.sp} \\
                --ref_species {params.ref_sp} \\
                --splice_table {output.splice_table} \\
                --tmp_dir ${{tmp_dir}} \\
                --out_dir {params.out} \\
                --out_file {output.text} \\
                --out_dir_DEP {params.out_m} \\
                --output_file_error {params.error}
            '''

rule peak_Transcripts:
    '''
    find peak junctions, annotations peaks, merges junction and annotation information
    '''
    input:
        peaks = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
        anno = rules.project_annotations.output.anno,
        refgen = rules.project_annotations.output.ref_gencode
    params:
        rname = '15_peak_Transcripts',
        script = join(source_dir,'workflow','scripts','05_Anno_Transcript.R'),
        functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
        p_type = peak_id,
        anchor= anno_anchor,
        r_depth= min_count,
        sp = "{sp}",
        n_merge = nt_merge,
        ref_sp = species_ref,
        out = join(out_dir,'04_annotation','02_peaks/',),
        anno_dir = join(out_dir,'04_annotation','01_project/'),
        a_config = annotation_config,
        g_path = gen_path,
        i_path = intron_path,
        r_path = rmsk_path,
        anno_strand="{AnnoStrand}",
        error = join(out_dir,'04_annotation','read_depth_error.txt')
    envmodules:
        config['R'],
        config['bedtools'],
    threads: getthreads("peak_annotations")
    output:
        outfile = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_transcripts_{AnnoStrand}.txt'),
    shell:
        '''
        # Setup tmp directory
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"
        else
            tmp_dir="{out_dir}01_preprocess/07_rscripts/"
            if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
        fi
        Rscript {params.script} \\
            --rscript {params.functions} \\
            --peak_type {params.p_type} \\
            --anno_anchor {params.anchor} \\
            --read_depth {params.r_depth} \\
            --sample_id {params.sp} \\
            --ref_species {params.ref_sp} \\
            --anno_dir {params.anno_dir} \\
            --reftable_path {params.a_config} \\
            --gencode_path {params.g_path} \\
            --intron_path {params.i_path} \\
            --rmsk_path {params.r_path} \\
            --tmp_dir ${{tmp_dir}} \\
            --out_dir {params.out} \\
            --out_file {output.outfile} \\
            --anno_strand {params.anno_strand} 
        '''

rule peak_ExonIntron:
    '''
    find peak junctions, annotations peaks, merges junction and annotation information
    '''
    input:
        peaks = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
        anno = rules.project_annotations.output.anno,
        refgen = rules.project_annotations.output.ref_gencode
    params:
        rname = '16_peak_ExonIntron',
        script = join(source_dir,'workflow','scripts','05_Anno_ExonIntron.R'),
        functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
        p_type = peak_id,
        anchor= anno_anchor,
        r_depth= min_count,
        sp = "{sp}",
        n_merge = nt_merge,
        ref_sp = species_ref,
        out = join(out_dir,'04_annotation','02_peaks/',),
        anno_dir = join(out_dir,'04_annotation','01_project/'),
        a_config = annotation_config,
        g_path = gen_path,
        i_path = intron_path,
        r_path = rmsk_path,
        error = join(out_dir,'04_annotation','read_depth_error.txt')
    envmodules:
        config['R'],
        config['bedtools'],
    threads: getthreads("peak_annotations")
    output:
        EIOut_SS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_IntronExon_SameStrand.txt'),
        EIOut_OS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_IntronExon_OppoStrand.txt'),
    shell:
        '''
        # Setup tmp directory
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
        else
            tmp_dir="{out_dir}01_preprocess/07_rscripts/"
            if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
        fi
        tmp_dir_s=$tmp_dir/same
        tmp_dir_o=$tmp_dir/oppo

        mkdir $tmp_dir_s
        mkdir $tmp_dir_o

        Rscript {params.script} \\
            --rscript {params.functions} \\
            --peak_type {params.p_type} \\
            --anno_anchor {params.anchor} \\
            --read_depth {params.r_depth} \\
            --sample_id {params.sp} \\
            --ref_species {params.ref_sp} \\
            --anno_dir {params.anno_dir} \\
            --reftable_path {params.a_config} \\
            --gencode_path {params.g_path} \\
            --intron_path {params.i_path} \\
            --rmsk_path {params.r_path} \\
            --tmp_dir ${{tmp_dir_s}} \\
            --out_dir {params.out} \\
            --out_file {output.EIOut_SS} \\
            --anno_strand "SameStrand" 

        Rscript {params.script} \\
            --rscript {params.functions} \\
            --peak_type {params.p_type} \\
            --anno_anchor {params.anchor} \\
            --read_depth {params.r_depth} \\
            --sample_id {params.sp} \\
            --ref_species {params.ref_sp} \\
            --anno_dir {params.anno_dir} \\
            --reftable_path {params.a_config} \\
            --gencode_path {params.g_path} \\
            --intron_path {params.i_path} \\
            --rmsk_path {params.r_path} \\
            --tmp_dir ${{tmp_dir_o}} \\
            --out_dir {params.out} \\
            --out_file {output.EIOut_OS} \\
            --anno_strand "OppoStrand"
        '''

rule peak_RMSK:
    '''
    find peak junctions, annotations peaks, merges junction and annotation information
    '''
    input:
        peaks = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
        anno = rules.project_annotations.output.anno
    params:
        rname = '17_peak_RMSK',
        script = join(source_dir,'workflow','scripts','05_Anno_RMSK.R'),
        functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
        p_type = peak_id,
        anchor= anno_anchor,
        r_depth= min_count,
        sp = "{sp}",
        n_merge = nt_merge,
        ref_sp = species_ref,
        out = join(out_dir,'04_annotation','02_peaks/',),
        anno_dir = join(out_dir,'04_annotation','01_project/'),
        a_config = annotation_config,
        g_path = gen_path,
        i_path = intron_path,
        r_path = rmsk_path,
        anno_strand="{AnnoStrand}",
        error = join(out_dir,'04_annotation','read_depth_error.txt')
    envmodules:
        config['R'],
        config['bedtools'],
    threads: getthreads("peak_annotations")
    output:
        outfile = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_RMSK_{AnnoStrand}.txt'),
    shell:
        '''
        # Setup tmp directory
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"
        else
            tmp_dir="{out_dir}01_preprocess/07_rscripts/"
            if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
        fi
        Rscript {params.script} \\
            --rscript {params.functions} \\
            --peak_type {params.p_type} \\
            --anno_anchor {params.anchor} \\
            --read_depth {params.r_depth} \\
            --sample_id {params.sp} \\
            --ref_species {params.ref_sp} \\
            --anno_dir {params.anno_dir} \\
            --reftable_path {params.a_config} \\
            --gencode_path {params.g_path} \\
            --intron_path {params.i_path} \\
            --rmsk_path {params.r_path} \\
            --tmp_dir ${{tmp_dir}} \\
            --out_dir {params.out} \\
            --out_file {output.outfile} \\
            --anno_strand {params.anno_strand}  
        '''

rule peak_process:
    '''
    find peak junctions, annotations peaks, merges junction and annotation information
    '''
    input:
        peaks = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions.txt'),
        TransOut_SS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_transcripts_SameStrand.txt'),
        TransOut_OS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_transcripts_OppoStrand.txt'),
        EIOut_SS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_IntronExon_SameStrand.txt'),
        EIOut_OS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_IntronExon_OppoStrand.txt'),
        RMSKOut_SS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_RMSK_SameStrand.txt'),
        RMSKOut_OS = join(out_dir,'04_annotation', '02_peaks', '{sp}_' + peak_id + 'readPeaks_AllRegions_RMSK_OppoStrand.txt'),
    params:
        rname = '18_peak_process',
        script = join(source_dir,'workflow','scripts','05_Anno_Process.R'),
        functions = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
        p_type = peak_id,
        anchor= anno_anchor,
        r_depth= min_count,
        sp = "{sp}",
        n_merge = nt_merge,
        ref_sp = species_ref,
        out = join(out_dir,'04_annotation','02_peaks/',),
    envmodules:
        config['R'],
        config['bedtools'],
    threads: getthreads("peak_annotations")
    output:
        outfile = join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + 'readPeaks_annotation_complete.txt'),
    shell:
        '''
        # Setup tmp directory
        if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"
        else
            tmp_dir="{out_dir}01_preprocess/07_rscripts/"
            if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
        fi
        Rscript {params.script} \\
            --rscript {params.functions} \\
            --peak_type {params.p_type} \\
            --anno_anchor {params.anchor} \\
            --read_depth {params.r_depth} \\
            --sample_id {params.sp} \\
            --ref_species {params.ref_sp} \\
            --tmp_dir ${{tmp_dir}} \\
            --out_dir {params.out} \\
            --out_file {output.outfile}
        '''


rule annotation_report:
    """
    generates an HTML report for peak annotations
    """
    input:
        peak_in = join(out_dir,'04_annotation', '02_peaks','{sp}_' + peak_id + 'readPeaks_annotation_complete.txt'),
    params:
        rname = "19_annotation_output",
        R = join(source_dir,'workflow','scripts','06_annotation.Rmd'),
        sp = "{sp}",
        ref_sp = species_ref,
        r_depth= min_count,
        junc = sp_junc,
        p_type = peak_id,
        rrna = refseq_rrna,
    envmodules:
        config['R']
    output:
        o1 = join(out_dir,'04_annotation', '{sp}_annotation_' + peak_id + 'readPeaks_final_report.html'),
        o2 = join(out_dir,'04_annotation', '{sp}_annotation_' + peak_id + 'readPeaks_final_table.txt'),
    shell:
        """
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(samplename = "{params.sp}", \
                peak_in = "{input.peak_in}", \
                output_table = "{output.o2}", \
                readdepth = "{params.r_depth}", \
                PeakIdnt = "{params.p_type}"))'
        """

#pipeline will continue through manorm processing, if required
if (DE_method=="MANORM"):
    rule MANORM_beds:
        input:
            bed_all = rules.create_beds_safs.output.bed_all
        params:
            rname = "21a_MANORM_beds"
        threads: getthreads("MANORM_beds")
        output:
            p_bed = (join(out_dir,'05_demethod', '01_input','{sp}_ReadsforMANORM_P.bed')),
            n_bed = (join(out_dir,'05_demethod', '01_input','{sp}_ReadsforMANORM_N.bed')),
        shell:
            """
            set -exo pipefail
            awk '$6=="+"' {input.bed_all} > {output.p_bed}
            awk '$6=="-"' {input.bed_all} > {output.n_bed}
            """

    rule MANORM_analysis:
        '''
        input requirements are listed as list as they cannot be resolved from the output
        - group_id is not part of the input parameters

        input requirements:
        - '05_demethod', '01_input', sample1 + '_' + peak_id + 'readPeaks_forMANORM.bed'),
        - '05_demethod', '01_input', sample1 + '_ReadsforMANORM_' + P + '.bed'),
        - '05_demethod', '01_input', sample1 + '_ReadsforMANORM_' + N + '.bed'),
        - '05_demethod', '01_input', sample2 + '_' + peak_id + 'readPeaks_forMANORM.bed'),
        - '05_demethod', '01_input', sample2 + '_ReadsforMANORM_' + P + '.bed'),
        - '05_demethod', '01_input', sample2 + '_ReadsforMANORM_' + N + '.bed'),
        output:
        - '05_demethod','02_analysis', 'sample1_vs_sample2_P/
        - '05_demethod','02_analysis', 'sample1_vs_sample2_N/
        '''
        input:
            get_MANORM_analysis_input    
        params:
            rname = "21b_MANORM_analysis",
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            de_dir = join(out_dir,'05_demethod', '01_input'),
            base = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_{strand}/'),
            peak_id = peak_id,
            manorm_w = manorm_w,
            manorm_d = manorm_d,
        envmodules:
            config['manorm']
        threads: getthreads("MANORM_analysis")
        output:
            mavals = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_{strand}','{group_id}_' + peak_id + 'readPeaks_MAvalues.xls'),
        shell:
            """
            manorm \\
                --p1 "{params.de_dir}/{params.gid_1}_{params.peak_id}readPeaks_forMANORM_{wildcards.strand}.bed" \\
                --p2 "{params.de_dir}/{params.gid_2}_{params.peak_id}readPeaks_forMANORM_{wildcards.strand}.bed" \\
                --r1 "{params.de_dir}/{params.gid_1}_ReadsforMANORM_{wildcards.strand}.bed" \\
                --r2 "{params.de_dir}/{params.gid_2}_ReadsforMANORM_{wildcards.strand}.bed" \\
                --s1 0 \\
                --s2 0 \\
                -p 1 \\
                -m 0 \\
                -w {params.manorm_w} \\
                -d {params.manorm_d} \\
                -n 10000 \\
                -s \\
                -o {params.base} \\
                --name1 {params.gid_1} \\
                --name2 {params.gid_2}
            
            # rename MANORM final output file
            mv {params.base}{wildcards.group_id}_all_MAvalues.xls {output.mavals}

            # rename individual file names for each sample
            mv {params.base}{params.gid_1}_MAvalues.xls {params.base}{params.gid_1}_{params.peak_id}readPeaks_MAvalues.xls
            mv {params.base}{params.gid_2}_MAvalues.xls {params.base}{params.gid_2}_{params.peak_id}readPeaks_MAvalues.xls

            # mv folders of figures, filters, tracks to new location
            # remove folders if they already exist
            if [[ -d {params.base}output_figures_{params.peak_id}readPeaks ]]; then rm -r {params.base}output_figures_{params.peak_id}readPeaks; fi
            if [[ -d {params.base}output_filters_{params.peak_id}readPeaks ]]; then rm -r {params.base}output_filters_{params.peak_id}readPeaks; fi
            if [[ -d {params.base}output_tracks_{params.peak_id}readPeaks ]]; then rm -r {params.base}output_tracks_{params.peak_id}readPeaks; fi
            mv {params.base}output_figures {params.base}output_figures_{params.peak_id}readPeaks
            mv {params.base}output_filters {params.base}output_filters_{params.peak_id}readPeaks
            mv {params.base}output_tracks {params.base}output_tracks_{params.peak_id}readPeaks
            """
            
    rule MANORM_post_processing:
    ### MAnorm returns differential for sample vs Bk and Bk vs Sample so we will create reports for comparison in both directions
        '''
        input requirements:
        - '04_annotation', sample1 + '_annotation_' + peak_id + 'readPeaks_final_table.txt'
        - '04_annotation', sample2 + '_annotation_' + peak_id + 'readPeaks_final_table.txt'
        - '05_demethod','02_analysis', sample1_vs_sample2 + "_P", sample1 + '_' + peak_id + 'readPeaks_MAvalues.xls'),
        - '05_demethod','02_analysis', sample1_vs_sample2 + "_N", sample1 + '_' + peak_id + 'readPeaks_MAvalues.xls')
        output:
        - '05_demethod','02_analysis', sample1_vs_sample2_post_processing.txt
        - '05_demethod','02_analysis', sample2_vs_sample1_post_processing.txt
        '''
        input:
            get_MANORM_post_processing
        params:
            rname = "20c_MANORM_processing",
            script = join(source_dir,'workflow','scripts','07_MAnorm_Process.R'),
            anno_dir = join(out_dir,'04_annotation'),
            de_dir = join(out_dir,'05_demethod', '02_analysis'),
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            smplbam = join(out_dir,'02_bam','02_dedup','{gid_1}.dedup.si.bam'),
            smplSAF= join(out_dir,'03_peaks','02_SAF','{gid_1}_' + peak_id + 'readPeaks.SAF'),
            bkbam = join(out_dir,'02_bam','02_dedup','{gid_2}.dedup.si.bam'),
            bkSAF= join(out_dir,'03_peaks','02_SAF','{gid_2}_' + peak_id + 'readPeaks.SAF'),
            peak_id = peak_id
        envmodules:
            config['R'],
            config['subread']
        threads: getthreads("MANORM_post_processing")
        output:
            bkUniqcountsmplPk= join(out_dir,'05_demethod','02_analysis', '{group_id}','counts','{gid_2}_Unique_{gid_1}_' + peak_id + 'readPeaks.txt'),
            bkMMcountsmplPk= join(out_dir,'05_demethod','02_analysis', '{group_id}','counts','{gid_2}_FracMMCounts_{gid_1}_' + peak_id + 'readPeaks.txt'),
            post_proc = join(out_dir,'05_demethod','02_analysis', '{group_id}','{gid_1}_vs_{gid_2}_' + peak_id + 'readPeaks_post_processing.txt'),
            smplUniqcountbkPk= join(out_dir,'05_demethod','02_analysis', '{group_id}','counts','{gid_1}_Unique_{gid_2}_' + peak_id + 'readPeaks.txt'),
            smplMMcountbkPk= join(out_dir,'05_demethod','02_analysis', '{group_id}','counts','{gid_1}_FracMMCounts_{gid_2}_' + peak_id + 'readPeaks.txt'),
            post_procRev = join(out_dir,'05_demethod','02_analysis', '{group_id}','{gid_2}_vs_{gid_1}_' + peak_id + 'readPeaks_post_processing.txt')

        shell:
            """
            set -exo pipefail
            
            ### for sample vs Bg compairson
            featureCounts -F SAF \\
                -a {params.smplSAF} \\
                -O \\
                --fraction \\
                --minOverlap 1 \\
                -s 1 \\
                -T {threads} \\
                -o {output.bkUniqcountsmplPk} \\
                {params.bkbam}
            featureCounts -F SAF \\
                -a {params.smplSAF} \\
                -M \\
                -O \\
                --fraction \\
                --minOverlap 1 \\
                -s 1 \\
                -T {threads} \\
                -o {output.bkMMcountsmplPk} \\
                {params.bkbam}
            Rscript {params.script} \\
                --samplename {params.gid_1} \\
                --background {params.gid_2} \\
                --peak_anno_g1 {params.anno_dir}/{params.gid_1}_annotation_{params.peak_id}readPeaks_final_table.txt \\
                --peak_anno_g2 {params.anno_dir}/{params.gid_2}_annotation_{params.peak_id}readPeaks_final_table.txt \\
                --Smplpeak_bkgroundCount_MM {output.bkMMcountsmplPk} \\
                --Smplpeak_bkgroundCount_unique {output.bkUniqcountsmplPk} \\
                --pos_manorm {params.de_dir}/{wildcards.group_id}/{wildcards.group_id}_P/{params.gid_1}_{params.peak_id}readPeaks_MAvalues.xls \\
                --neg_manorm {params.de_dir}/{wildcards.group_id}/{wildcards.group_id}_N/{params.gid_1}_{params.peak_id}readPeaks_MAvalues.xls \\
                --output_file {output.post_proc}
            
            ### for Bg vs sample compairson
            featureCounts -F SAF \\
                -a {params.bkSAF} \\
                -O \\
                --fraction \\
                --minOverlap 1 \\
                -s 1 \\
                -T {threads} \\
                -o {output.smplUniqcountbkPk} \\
                {params.smplbam}
            featureCounts -F SAF \\
                -a {params.bkSAF} \\
                -M \\
                -O \\
                --fraction \\
                --minOverlap 1 \\
                -s 1 \\
                -T {threads} \\
                -o {output.smplMMcountbkPk} \\
                {params.smplbam}
            Rscript {params.script} \\
                --samplename {params.gid_2} \\
                --background {params.gid_1} \\
                --peak_anno_g1 {params.anno_dir}/{params.gid_2}_annotation_{params.peak_id}readPeaks_final_table.txt \\
                --peak_anno_g2 {params.anno_dir}/{params.gid_1}_annotation_{params.peak_id}readPeaks_final_table.txt \\
                --Smplpeak_bkgroundCount_MM {output.smplMMcountbkPk} \\
                --Smplpeak_bkgroundCount_unique {output.smplUniqcountbkPk} \\
                --pos_manorm {params.de_dir}/{wildcards.group_id}/{wildcards.group_id}_P/{params.gid_2}_{params.peak_id}readPeaks_MAvalues.xls \\
                --neg_manorm {params.de_dir}/{wildcards.group_id}/{wildcards.group_id}_N/{params.gid_2}_{params.peak_id}readPeaks_MAvalues.xls \\
                --output_file {output.post_procRev}
            """

    rule MANORM_RMD:
        input:
            post_proc = rules.MANORM_post_processing.output.post_proc,
            post_procRev = rules.MANORM_post_processing.output.post_procRev
        params:
            rname = "20d_MANORM_RMD",
            R = join(source_dir,'workflow','scripts','08_MAnormAnnotation.Rmd'),
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            p_id = peak_id,
            pval = pval,
            fc = fc,
            rrna = include_rRNA
        threads: getthreads("MANORM_RMD")
        envmodules:
            config['R']
        output:
            report = join(out_dir,'05_demethod','02_analysis','{group_id}','{gid_1}_vs_{gid_2}_' + peak_id + 'readPeaks_manorm_report.html'),
            reportRev = join(out_dir,'05_demethod','02_analysis','{group_id}','{gid_2}_vs_{gid_1}_' + peak_id + 'readPeaks_manorm_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}", \
                output_file = "{output.report}", \
                params= list(peak_in="{input.post_proc}", \
                    PeakIdnt="{params.p_id}",\
                    samplename="{params.gid_1}", \
                    background="{params.gid_2}", \
                    pval="{params.pval}", \
                    FC="{params.fc}", \
                    incd_rRNA="{params.rrna}"\
                    ))'
                    
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}", \
                output_file = "{output.reportRev}", \
                params= list(peak_in="{input.post_procRev}", \
                    PeakIdnt="{params.p_id}",\
                    samplename="{params.gid_2}", \
                    background="{params.gid_1}", \
                    pval="{params.pval}", \
                    FC="{params.fc}", \
                    incd_rRNA="{params.rrna}"\
                    ))'
            """
elif (DE_method == "DIFFBIND"):
    rule DIFFBIND_beds:
        input:
            f1 = rules.dedup.output.bam
        params:
            rname = "20a_DIFFBIND_beds",
            base = "{sp}.dedup"
        threads: getthreads("DIFFBIND_beds")
        envmodules:
            config['samtools']
        output:
            p_bam = (join(out_dir,'05_demethod', '01_input','{sp}_ReadsforDIFFBIND_P.bam')),
            n_bam = (join(out_dir,'05_demethod', '01_input','{sp}_ReadsforDIFFBIND_N.bam')),
        shell:
            """
            set -exo pipefail
            ################################################
            # Setup tmp directory
            ################################################
            if [ -d "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmp_dir="/lscratch/${{SLURM_JOB_ID}}"
            else
                tmp_dir="{out_dir}01_preprocess/05_tmp_bam"
                if [ ! -d $tmp_dir ]; then mkdir $tmp_dir; fi
            fi
            
            # Create header
            samtools view -@ {threads} -H {input.f1} > ${{tmp_dir}}/{params.base}.header.txt
            
            # Run
            samtools view -@ {threads} -F 16 {input.f1} \\
                | cat ${{tmp_dir}}/{params.base}.header.txt - \\
                | samtools view -Sb > {output.p_bam}
            samtools view -@ {threads} -f 16 {input.f1} \\
                | cat ${{tmp_dir}}/{params.base}.header.txt - \\
                | samtools view -Sb > {output.n_bam}
            """

    rule DIFFBIND_preprocess:
        """
        input requirements:
        - '05_demethod', '01_input', sample1 + '_ReadsforDIFFBIND_' + wildcards.strand + '.bam'
        - '05_demethod', '01_input', sample2 + '_ReadsforDIFFBIND_' + wildcards.strand + '.bam'
        - 'manifest', 'sample_manifest.tsv'
        output:
        - '05_demethod', '02_analysis', group_id, groupid + '_DIFFBIND_' + strand + '.txt'
        example input
            Rscript /home/sevillas2/git/iCLIP/workflow/scripts/07_DIFFBIND_PreProcess.R
            --samplename WT
            --background KO
            --sample_overlap 1
            --strand N
            --samplemanifest /data/sevillas2/diffbind/sample_manifest.tsv
            --input_dir /data/sevillas2/diffbind/05_demethod/01_input/
            --output_table /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBINDTable_N.txt
            --output_summary /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBINDSummary_N.txt
            --output_figures /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/figures/WT_vs_KO_DIFFBIND
        """
        input:
            get_DIFFBIND_analysis_input,
            s_manifest = sample_manifest,
        params:
            rname = "20b_DIFFBIND_preprocess",
            script = join(source_dir,'workflow','scripts','07_DIFFBIND_PreProcess.R'),
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            st = '{strand}',
            so = sample_overlap,
            base = join(out_dir,'05_demethod', '01_input'),
            figs = join(out_dir,'05_demethod', '02_analysis','{group_id}','figures','{group_id}_' + peak_id + 'readPeaks_' + DE_method),
        envmodules:
            config['R']
        output:
            table = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_table_{strand}.txt'),
            summary = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_summary_{strand}.txt'),
        shell:
            """
            Rscript {params.script} \\
                --samplename {params.gid_1} \\
                --background {params.gid_2} \\
                --strand {params.st} \\
                --sample_overlap {params.so} \\
                --samplemanifest {input.s_manifest} \\
                --input_dir {params.base} \\
                --output_table {output.table} \\
                --output_summary {output.summary} \\
                --output_figures {params.figs}
            """

    rule DIFFBIND_analysis:
        """
        input
            /home/sevillas/git/iCLIP/manifests/sample_manifest.tsv
            /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_table_P.txt
            /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_table_N.txt
        
        output
            post = /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_post_processing.txt
            final = /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_final_table.txt
        example
            Rscript /home/sevillas2/git/iCLIP/workflow/scripts/07_DIFFBIND_Process.R
            --samplename WT
            --background KO
            --peak_type ALL
            --join_junction TRUE
            --samplemanifest /data/sevillas2/diffbind/sample_manifest.tsv
            --pos_DB /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_table_P.txt
            --neg_DB /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/WT_vs_KO_DIFFBIND_table_N.txt
            --anno_dir_sample /data/sevillas2/diffbind/04_annotation/
            --reftable_path /data/sevillas2/diffbind/config/annotation_config.txt
            --anno_dir_project /data/sevillas2/diffbind/04_annotation/01_project/
            --ref_species hg38
            --gencode_path /hg38/Gencode_V32/fromGencode/gencode.v32.annotation.gtf.txt
            --intron_path /hg38/Gencode_V32/fromUCSC/KnownGene/KnownGene_GencodeV32_GRCh38_introns.bed
            --rmsk_path /hg38/repeatmasker/rmsk_GRCh38.txt
            --function_script /home/sevillas/git/iCLIP/workflow/scripts/07_DIFFBIND_Process.R
            --out_dir /data/sevillas2/diffbind/05_demethod/02_analysis/WT_vs_KO/
        """
        input:
            s_manifest = sample_manifest,
            table_p = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_table_P.txt'),
            table_n = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_table_N.txt'),
        params:
            rname = "20c_DIFFBIND_process",
            script = join(source_dir,'workflow','scripts','07_DIFFBIND_Process.R'),
            script_func = join(source_dir,'workflow','scripts','05_peak_annotation_functions.R'),
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            p_type = peak_id,
            junc = sp_junc,
            ref_sp = species_ref,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
            anno_dir_s = join(out_dir,'04_annotation/'),
            anno_dir_p = join(out_dir,'04_annotation','01_project/'),
            ref_tab_config = annotation_config,
            base = join(out_dir,'05_demethod','02_analysis', '{group_id}'),
        envmodules:
            config['R']
        output:
            post = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_post_processing.txt'),
            final = join(out_dir,'05_demethod','02_analysis', '{group_id}','{group_id}_' + peak_id + 'readPeaks_' + DE_method + '_final_table.txt'),
        shell:
            """
            Rscript {params.script} \\
                --samplename {params.gid_1} \\
                --background {params.gid_2} \\
                --peak_type {params.p_type} \\
                --join_junction {params.junc} \\
                --samplemanifest {input.s_manifest} \\
                --pos_DB {input.table_p} \\
                --neg_DB {input.table_n} \\
                --anno_dir_sample {params.anno_dir_s} \\
                --reftable_path {params.ref_tab_config} \\
                --anno_dir_project {params.anno_dir_p} \\
                --ref_species {params.ref_sp} \\
                --gencode_path {params.g_path} \\
                --intron_path {params.i_path} \\
                --rmsk_path {params.r_path} \\
                --function_script {params.script_func} \\
                --out_dir {params.base}
            """
    
    rule DIFFBIND_report:
        input:
            final = rules.DIFFBIND_analysis.output.final,
            s_manifest = sample_manifest,
        params:
            rname = "20d_DIFFBIND_RMD",
            R = join(source_dir,'workflow','scripts','08_DIFFBINDAnnotation.Rmd'),
            gid_1 = get_DEMETHOD_gid1,
            gid_2 = get_DEMETHOD_gid2,
            p_id = peak_id,
            pval = p_val,
            fc = f_c,
            rrna = include_rRNA,
            base = join(out_dir,'05_demethod','02_analysis','{group_id}')
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'05_demethod','02_analysis','{group_id}','{group_id}_' + peak_id + 'readPeaks_diffbind_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(peak_in="{input.final}", \
                    DEGfolder="{params.base}",\
                    PeakIdnt="{params.p_id}",\
                    samplename="{params.gid_1}", \
                    background="{params.gid_2}", \
                    pval="{params.pval}", \
                    FC="{params.FC}", \
                    incd_rRNA="{params.rrna}"\
                    ))'
            """