'''
Overview

- Multiplexed samples are split based on provided barcodes and named using provide manifests
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged

'''

report: "report/workflow.rst"

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml

'''
* Requirements *
- Four input files are required:
    1) cluster_config.yml
    2) snakemake_config.yaml
    3) multiplex.tsv
    4) samples.tsv
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

#snakemake config
source_dir = config['source_dir']
cont_dir = config['container_dir']
out_dir = config['output_dir'].rstrip('/') + '/'
fastq_dir = config['fastq_dir'].rstrip('/') + '/'

sample_manifest = config['sample_manifest']
multiplex_manifest = config['multiplex_manifest']

nova_ref = config['novoalign_reference']
splice_aware = config['splice_aware']
splice_bp = config['splice_bp_length']

split_value =  config['split_value']
min_count = config['minimum_count']

#split_params
def check_split(split_in):
    if (type(split_in) == str):
        split_replace = split_in.replace(',', '')
        if (type(split_in) == str):
            split_replace = 1000000
    elif (split_in < 3000):
        split_replace = 1000000
    elif (type(split_in) == float):
        split_replace = int(split_in)
    else:
        split_replace = split_in
    return split_replace
split_value = check_split(split_value)

#index selection
index_manifest = join(source_dir, 'config', 'index_config.yaml')
with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)
if (splice_aware=='N' or splice_aware=="n"):
    nova_index = index_list[nova_ref]['std']
else:
    nova_index = index_list[nova_ref]['spliceaware'][str(splice_bp)+'bp']

#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = fastq_dir + multiplex_dict[wildcards.mp]
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + wildcards.mp + '/'

    return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + wildcards.mp + '/demux_' + bc + '.fastq.gz' + ' ' + out_dir + wildcards.mp + '/01_renamed/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)

#command needed to cut adaptors from demux samples
def adapt_cmd(wildcards):
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp) & (df_samples['sample']==wildcards.sp)]

    base = out_dir + wildcards.mp + '/02_adaptor/' + wildcards.sp
    fq_un =  base + '_untrimmed.fastq.gz '
    fq_t = base + '_trimmed.fastq.gz '

    base = out_dir + '/' + wildcards.mp + '/01_renamed/' + wildcards.sp
    fq_o = base + '.fastq.gz '

    command_line = '--untrimmed_output ' + fq_un + '--reads_trimmed ' + fq_t + fq_o + sub_df.iloc[0]['adaptor']

    return(command_line)

#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=out_dir + wildcards.mp + '/03_unzip/' + wildcards.sp + '.fastq '

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]
    except:
        return(0)
    return(sub_df.iloc[0]['chunksize'])

#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

rule all:
    input:
        join(out_dir + "manifest_clean.txt"),
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list), #multiplexed sample fastq files exist
        expand(join(out_dir,'qc/barcode_plot_{mp}.png'), mp=mp_list), #barcode qc
        expand(join(out_dir,'qc/barcode_summary_{mp}.txt'),mp=mp_list), #barcode qc
        #expand(join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),mp=samp_dict.keys()), #demultiplex
        #expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list), #rename
        #expand(join(out_dir,'{mp}/00_qc/{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list), qc reports
<<<<<<< Updated upstream
        join(out_dir,'qc/multiqc_report.html'), #multiqc report
=======
        
        #FastQ Screen
        expand(join(out_dir,'{mp}/00_qc_screen_p1/{sp}_screen.txt'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/00_qc_screen_p2/{sp}_screen.txt'),zip, mp=mp_list, sp=sp_list),
        
        #MultiQC Report
        join(out_dir,'qc/multiqc_report.html'),
>>>>>>> Stashed changes
        #expand(join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz'), zip, mp=mp_list, sp=sp_list), #remove adaptors
        #expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list), #gunzip
        join(out_dir,'split_params.tsv'), #determine_splits
        #expand(join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.i.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/12_bed/{sp}.bed'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/13_peak/{sp}_merged.txt'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/13_peak_anno/{sp}.SAF'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/14_peak_count/{sp}_FCount_unique.txt'),zip, mp=mp_list, sp=sp_list),
        ##expand(join(out_dir,'{mp}/15_gff/{sp}.gff3'),zip, mp=mp_list, sp=sp_list),

include: "rules/common.smk"
include: "rules/other.smk"

rule check_manifest:
    """
    Read in multiplex manifest and sample manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    If files are correct, outputs a temp file. If not, outputs error file.
    """
    input:
        f1 = multiplex_manifest,
        f2 = sample_manifest
    params:
        rname='ic_manif',
        py = join(source_dir,'workflow/scripts/01_check_manifest.py')
    output:
        o1 = join(out_dir, "manifest_clean.txt"),
    shell:
        """
        module load python; \
        python {params.py} \
            '{out_dir}manifest_' {input.f1} {input.f2}
        """

rule barcode_qc:
    input:
        f1 = out_dir + "manifest_clean.txt"
    params:
        rname="ic_bcQC",
        py = join(source_dir,'workflow', 'scripts', '02_barcode_qc.py'),
    output:
        o1 = expand(join(out_dir,'qc/barcode_plot_{mp}.png'),mp=mp_list),
        o2 = expand(join(out_dir,'qc/barcode_summary_{mp}.txt'),mp=mp_list)
    shell:
        """
        module load python; module load Qt/5.13.2; \
        python {params.py} \
            {sample_manifest} {multiplex_manifest} {fastq_dir} {out_dir}qc/
        """

rule demultiplex:
    """
    Reads in fastq files from multiplex_manifest.
    Finds multiplex match between manifest files, splits files based on list of
    barcodeIDs found in sample_manifest

    For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN

    file_name                   multiplex
    SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1

    multiplex       sample          group       barcode     adaptor
    SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
    SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
    """
    input:
        f1 = get_fq_names,
        qc = expand(join(out_dir,'qc/barcode_summary_{mp}.txt'),mp=mp_list)
    params:
        rname='ic_demux',
        ml = 15,
        cmd = demux_cmd,
        doc = join(cont_dir,'icount.sif')
    output:
        o1 = join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),
    shell:
        """
        module load singularity
        singularity exec -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
        {params.doc} iCount demultiplex -ml {params.ml} {params.cmd}
        """

rule rename_files:
    """
    renames demultiplexed files based on
    from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
    to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
    """
    input:
        join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),
    params:
        rname = 'ic_rename',
        p1 = rename_cmd
    output:
        o1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    shell:
        'cp {params.p1}'

rule remove_adaptors:
    """
    removes adaptors from files
    outputs untrimmed and trimmed file
    """
    input:
        f1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    params:
        rname='ic_adapt',
        ml = 1,
        adapt = adapt_cmd,
        doc = join(cont_dir,'icount.sif')
    output:
        o1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    shell:
        """
        module load singularity
        singularity exec -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
        {params.doc} iCount cutadapt -ml {params.ml} {params.adapt}
        """

rule fastqc_pre:
    """
    Runs FastQC report on each sample before adaptors have been removed
    """
    input:
        f1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    output:
        o1 = join(out_dir,'{mp}/00_qc_pre/{sp}_fastqc.html')
    params:
        rname='ic_fqc_pre',
        out = join(out_dir,'{mp}/00_qc_pre/')
    shell:
        """
        module load fastqc;
        fastqc {input.f1} -o {params.out}
        """

rule fastqc_post:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        f1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    output:
        o1 = join(out_dir,'{mp}/00_qc_post/{sp}_untrimmed_fastqc.html')
    params:
        rname='ic_fqc_post',
        out = join(out_dir,'{mp}/00_qc_post/'),
    shell:
        """
        module load fastqc;
        fastqc {input.f1} -o {params.out}
        """

rule fastq_screen_p1:
    """
    this will align to human, mouse, bacteria
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    params:
        rname='ic_screen_p1',
        dir_screen = join(out_dir,'{mp}/00_qc_screen/'),
        fq_config = join(source_dir,'config/fqscreen_p1_config.conf')
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir,'{mp}/00_qc_screen_p1/{sp}_screen.txt'),
        o2 = join(out_dir,'{mp}/00_qc_screen_p1/{sp}_screen.png'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.dir_screen} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}
        '''

rule fastq_screen_p2:
    """
    this will align to rRNA and must be run separately from _p1 as we are interested
    in the total amount aligning to rRNA and not a subset after human and mouse alignment
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    params:
        rname='ic_screen_p2',
        dir_screen = join(out_dir,'{mp}/00_qc_screen/'),
        fq_config = join(source_dir,'config/fqscreen_p2_config.conf')
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir,'{mp}/00_qc_screen_p2/{sp}_screen.txt'),
        o2 = join(out_dir,'{mp}/00_qc_screen_p2/{sp}_screen.png'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.dir_screen} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}
        '''

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    """
    input:
        f1 = expand(join(out_dir,'{mp}/00_qc_pre/{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f2 = expand(join(out_dir,'{mp}/00_qc_post/{sp}_untrimmed_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f3 = expand(join(out_dir,'{mp}/00_qc_screen_p1/{sp}_screen.txt'),zip,mp=mp_list,sp=sp_list),
        f4 = expand(join(out_dir,'{mp}/00_qc_screen_p2/{sp}_screen.txt'),zip,mp=mp_list,sp=sp_list),
    output:
        o1 = join(out_dir,'qc/multiqc_report.html')
    params:
        rname = 'ic_multiqc',
        qc_config = join(source_dir,'config/multiqc_config.yaml'),
        dir_pre = expand(join(out_dir,'{mp_s}/00_qc_pre/'),mp_s = samp_dict.keys()),
        dir_post = expand(join(out_dir,'{mp_s}/00_qc_post/'),mp_s = samp_dict.keys()),
        dir_screen1 = expand(join(out_dir,'{mp_s}/00_qc_screen_p1/'),mp_s = samp_dict.keys()),
        dir_screen2 = expand(join(out_dir,'{mp_s}/00_qc_screen_p2/'),mp_s = samp_dict.keys()),
    shell:
        """
        module load multiqc;
        multiqc -c {params.qc_config} \
            {params.dir_pre} {params.dir_post} {params.dir_screen1} {params.dir_screen2} -o {out_dir}qc
        """

rule gunzip_files:
    """
    unzips untrimmed files
    """
    input:
        f1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    params:
        rname='ic_gunzip',
    output:
        o1 = join(out_dir,'{mp}/03_unzip/{sp}.fastq')
    shell:
        "gunzip -c {input.f1} > {output.o1}"

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to split FASTQ file
    split is necessary to improve speed of alignment
    split is done based off split_value input, maintaining every 4 lines = 1 seq
    """
    input:
        f1 = expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list),
    params:
        rname='ic_splitdoc',
<<<<<<< HEAD
        sh = join(source_dir, 'workflow', 'scripts', '02_find_split_parameters.sh'),
>>>>>>> ea05906be4cf348b62b66c909432388a575ef718
    output:
        o1 = join(out_dir,'split_params.tsv')
    shell:
        """
        sh {params.sh} {output.o1} {split_value} {input.f1}
        """

rule split_files:
    """
    performs split of fastq file into smaller files using parameters in split_params.tsv
    """
    input:
        all = expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list),
        f1 = join(out_dir,'{mp}/03_unzip/{sp}.fastq'),
        sp = join(out_dir,'split_params.tsv')
    params:
        rname='ic_split',
        p1 = join(out_dir,'{mp}/04_split/{sp}.split.'),
        c = get_filechunk_size
    output:
        o1 = dynamic(join(out_dir,'{mp}/04_split/{sp}.split.{n}.fastq'))
    shell:
        """
        split --additional-suffix .fastq -l {params.c} --numeric-suffixes=10 {input.f1} {params.p1}
        """

if (splice_aware == "N" or splice_aware == "n"):
    rule novoalign:
        """
        aligns files to index given; output is a sam file
        """
        input:
            f1 = join(out_dir,'{mp}/04_split/{sp}.split.{n}.fastq'),
        params:
            rname='ic_novo',
            n_index = nova_index,
        output:
            o1 = join(out_dir,'{mp}/05_sam/{sp}.split.{n}.sam'),
        shell:
            """
            set +e
            module load novocraft; novoalign -d {params.n_index} \
            -f {input.f1} -F STDFQ -c 32 \
            -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
            -R 0 -r All 999 > {output.o1}
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """
else:
    rule novoalign_splice:
        """
        aligns files to index given; output is a sam file
        functionally equiv to rule novoalign, expect for output file name
        """
        input:
            f1 = join(out_dir,'{mp_st}/04_split/{sp_st}.split.{n}.fastq'),
        params:
            rname='ic_novo',
            n_index = nova_index,
        output:
            o1 = join(out_dir,'{mp_st}/05_sam_splice/{sp_st}.split.{n}.splice.sam'),
        shell:
            """
            set +e
            module load novocraft; novoalign -d {params.n_index} \
            -f {input.f1} -F STDFQ -c 56 \
            -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
            -R 0 -r All 999 > {output.o1}
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """

    rule sam_cleanup:
        """
        performs cleanup for inserts
        """
        input:
            f1 = join(out_dir,'{mp_st}/05_sam_splice/{sp_st}.split.{n}.splice.sam'),
        params:
            rname = 'ic_sam_cleanup'
        output:
            g1 = join(out_dir,'{mp_st}/05_sam_splice/{sp_st}.split.{n}.tmp.sam'),
            final = join(out_dir,'{mp_st}/05_sam_splice/{sp_st}.split.{n}.splice.final.sam'),
        shell:
            """
            module load samtools;
            samtools view {input.f1} | awk '{{ if (($4 == 1 && $6!~/^[0-9]I/ && $1~/:/ )||($4 > 1 && $1~/:/ )) {{ print }} }}' > {output.g1};
            samtools view -H {input.f1} | cat - {output.g1} > {output.final}
            """

    rule convert_transcript:
        """
        converts transcriptome coordinates to genomic coordinates
        uses USeq version 8.9.6
        """
        input:
            f1 = join(out_dir,'{mp_st}/05_sam_splice/{sp_st}.split.{n}.splice.final.sam'),
        params:
            rname = 'ic_convert',
            base = join(out_dir,'{mp_st}/05_sam_genomic/{sp_st}.split.{n}.sam'),
            doc = join(cont_dir,'USeq_8.9.6/Apps/SamTranscriptomeParser')
        output:
            o1 = join(out_dir,'{mp_st}/05_sam_genomic/{sp_st}.split.{n}.sam.gz'),
        shell:
            """
            module load java;
            java -jar -Xmx50G -jar {params.doc} \
            -f {input.f1} \
            -a 50000 -n 25 -u -s {params.base}
            """

    rule gunzip_sam:
        """
        unzips sam file
        """
        input:
            f1 = join(out_dir,'{mp_st}/05_sam_genomic/{sp_st}.split.{n}.sam.gz'),
        params:
            rname = 'ic_gzipsam'
        output:
            o1 = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
        shell:
            """
            gunzip -c {input.f1} > {output.o1}
            """

    rule novosort:
        """
        Q: where does this belong? this step wasn't included in the original pipeline and creates a new BAM file

        samtools view -uS bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.sam 2> bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.convert.err
        | novosort -  > bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.bam
        """
        input:
            f1 = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
        params:
            rname = 'ic_novosort'
        output:
            o1 = join(out_dir,'{mp_st}/05_bam/{sp_st}.split.{n}.bam'),
        shell:
            """
            module load samtools;
            samtools view -uS {input.f1} 2> bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.convert.err
            | novosort -  > {output.o1}
            """

rule split_mm_unique:
    """
    creates two text files from sam file
    1. unique file without NH:i
    2. MM file with NH:i
    """
    input:
        f1 = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
    params:
        rname='ic_split_mu',
    output:
        o1 = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.unique.txt'),
        o2 = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.mm.txt'),
    shell:
        """
        set +e
        module load samtools; samtools view {input.f1} | grep -v 'NH:i' > {output.o1}; \
        samtools view {input.f1} | grep 'NH:i' > {output.o2};
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule sam_to_header:
    """
    read sam file header, print to header txt
    """
    input:
        sam = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
    params:
        rname='ic_sam_head',
    output:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
    shell:
        """
        module load samtools; samtools view -H {input.sam} > {output.h};
        """

rule create_bam_unique:
    """
    add nh:i:1 tag, cat header, create unique bam file
    """
    input:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
        un = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.unique.txt'),
    params:
        rname='ic_bam_u',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.bam'),
    shell:
        """
        module load samtools;
        awk -F '\t' -v OFS='\t' '{{ $(NF+1) = "NH:i:1"; print }}' {input.un} | cat {input.h} - | \
        samtools sort | samtools view -Sb > {output.o1};
        """

rule create_bam_mm:
    """
    cat header, create multimapped bam file
    """
    input:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
        mm = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.mm.txt'),
    params:
        rname='ic_bam_m',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.bam'),
    shell:
        """
        module load samtools; cat {input.h} {input.mm} | samtools sort | samtools view -Sb > {output.o1}
        """

rule sort_mm_and_unique:
    """
    sort both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.bam'),
        mm = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.bam'),
    params:
        rname='ic_sort_mu',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.s.bam'),
        o2 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.un} -o {output.o1}; \
        samtools sort {input.mm} -o {output.o2};
        """

rule index_mm_and_unique:
    """
    index both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.s.bam'),
        mm = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.s.bam'),
    params:
        rname='ic_index_mu',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.i.bam'),
        o2 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.i.bam'),
    shell:
        """
        module load samtools; \
        cp {input.un} {output.o1}; samtools index {output.o1}; \
        cp {input.mm} {output.o2}; samtools index {output.o2};
        """

rule merge_splits_unique:
    """
    merge split unique bam files into one merged.unique bam file
    """
    input:
        f1 = dynamic(join(out_dir,'{mp}/07_bam_unique/{sp}.split.{n}.unique.i.bam'))
    params:
        rname='ic_merge_u',
    output:
        o1 = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.f1}
        """

rule merge_splits_mm:
    """
    merge split multimapped bam files into one merged.mm bam file
    """
    input:
        f1 = dynamic(join(out_dir,'{mp}/07_bam_mm/{sp}.split.{n}.mm.i.bam'))
    params:
        rname='ic_merge_m',
    output:
        o1 = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.f1}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files by sample
    """
    input:
        un = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),
        mm = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),
    params:
        rname='ic_merge_mu',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.un} {input.mm}
        """

rule sort_merged:
    """
    sort merged.bam
    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),
    params:
        rname='ic_sort_m',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.f1} -o {output.o1};
        """

rule index_merged:
    """
    index merged.s.bam files
    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.s.bam'),
    params:
        rname='ic_index_m',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule dedup:
    """
    deduplicate merged.i.bam files
    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),
    params:
        rname='ic_dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),
        o2 = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.log'),
    shell:
        """
        module load umitools; umi_tools dedup \
        -I {input.f1} \
        --method unique --multimapping-detection-method=NH --umi-separator=rbc: \
        -S {output.o1} \
        --log2stderr -L {output.o2};
        """

rule sort_dedup:
    """
    sort dedup.bam file
    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),
    params:
        rname='ic_sort_dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.f1} -o {output.o1};
        """

rule index_dedup:
    """
    index dedup.s.bam file
    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.s.bam'),
    params:
        rname='ic_index_dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule dedup_header:
    """
    cat bam header
    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
    params:
        rname='ic_dedup_head',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.header.txt'),
    shell:
        """
        module load samtools; samtools view -H {input.f1} > {output.o1}
        """

rule split_dedup_mm_and_unique:
    """
    split dedup file into multimapped and unique files
    """
    input:
        bam = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
        h = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.header.txt'),
    params:
        rname='ic_split_dedup',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.bam'),
        o2 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.bam'),
    shell:
        """
        set +e
        module load samtools; \
        samtools view {input.bam} | grep -v -w 'NH:i:1' | cat {input.h} - | samtools sort | samtools view -Sb > {output.o1}; \
        samtools view {input.bam} | grep -w 'NH:i:1' | cat {input.h} - |  samtools sort | samtools view -Sb > {output.o2}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule index_split_dedup_unique:
    """
    index deduped unique file
    """
    input:
        f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.bam'),
    params:
        rname='ic_index_splitu',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule index_split_dedup_mm:
    """
    index deduped multimapped file
    """
    input:
        f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.bam'),
    params:
        rname='ic_index_splitm',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

if [[splice_aware=='n' or splice_aware=='N']]:
    rule create_bed:
        """
        create bed file from the deduped unique file
        """
        input:
            f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),
        params:
            rname='ic_bed',
        output:
            o1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
        shell:
            """
            module load bedtools; bedtools bamtobed -i {input.f1} > {output.o1}
            """
else:
    rule create_bed:
        """
        create bed file from the deduped unique file
        """
        input:
            f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),
        params:
            rname='ic_bed',
        output:
            o1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
        shell:
            """
            module load bedtools; bedtools bamtobed -split -i {input.f1} | bedtools sort -i - > {output.o1}
            """

rule merge_bed_txt:
    """
    merge overlapping peaks into bed file
    """
    input:
        f1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
    params:
        rname='ic_bed_txt',
    output:
        o1 = join(out_dir,'{mp}/13_peak/{sp}_merged.txt'),
    shell:
        """
        module load bedtools; \
        bedtools merge -c 6 -o count,distinct -s -d -1 -i {input.f1} > {output.o1}
        """

rule merge_bed_saf:
    """
    create peak annotation files
    """
    input:
        f1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
    params:
        rname='ic_bed_saf',
    output:
        o1 = join(out_dir,'{mp}/13_peak_anno/{sp}.SAF')
    shell:
        """
        module load bedtools; \
        bedtools merge -c 6 -o count,distinct -s -d -1 -i {input.f1} | \
        awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2+1,$3,$5}}' | awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > \
        {output.o1}
        """

rule count_peaks:
    """
    create peak count files for all, unique, primary, fraction, and primary/fraction
    """
    input:
        b = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
        anno = join(out_dir,'{mp}/13_peak_anno/{sp}.SAF'),
    params:
        rname='ic_peak_frac',
<<<<<<< HEAD
        sh = join(source_dir, 'workflow', 'scripts', '03_count_features.sh'),
>>>>>>> ea05906be4cf348b62b66c909432388a575ef718
        threads = 8,
        base = join(out_dir,'{mp}/14_peak_count/'),
        sample = '{sp}'
    output:
        txt = join(out_dir,'{mp}/14_peak_count/{sp}_FCount_unique.txt'),
        bam = temp(join(out_dir,'{mp}/14_peak_count/{sp}.dedup.i.bam.featureCounts.bam'))
    shell:
        """
        module load subread; \
        sh {params.sh} {params.threads} {input.anno} {input.b} {params.base} {params.sample}
        """

rule ID_peaks:
    """
    create gtf and gff files
    """
    input:
        f1 = join(out_dir,'{mp}/13_peak/{sp}_merged.txt'),
    params:
        rname='ic_idpeaks',
        sh = join(source_dir, 'workflow', 'scripts', '04_select_peaks.R'),
        dir = join(out_dir,'{mp}/15_gff/{sp}'),
        min = min_count
    output:
        o1 = join(out_dir,'{mp}/15_gff/{sp}.gff3')
    shell:
        """
        module load R; \
        Rscript {params.sh} {input.f1} {params.dir} {params.min}
        """

"""
https://github.com/RBL-NCI/iCLIP_Pipeline/tree/master/3_Deduplicate

# updates 10/26
- new multiplex manifest to take in filenames - flexibility in file naming
- script to check manifests - ensure concordance and flags errors before pipeline runs
- add rules after split_files

# updates 11/2
- differentiate for control vs study samples [controls don't get split]
- use created docker for iCOUNT '/data/sevillas2/iCLIP/container/icount.sif'
- control calculation for split; otherwise lines were splitting the 4-line seq chunk
- error handling of novoalign
- completed runs through rule samtools_sort_index_merge; equivalent of 3_dedup - 1,2a,2b
    - sintearctive --mem=16g --cpus-per-task=4
    - split_number = 30
    - one sample 20K reads = 10min; two samples 10K reads = 14min

# updates 11/16
- update req files and execution documentation
- added logic for variable number of lines - dynamic split will vary by size using 02_fine_split_parameters script
-- need to determine max size of file
- re-organize dir structure
- created 03_select_peaks and 05_count_features scripts; copied 04_create_peak_table - have Q
- testing with one sample, two demultiplexed - 13 minutes to complete
- all rules but create_peak_table,count_peaks execute output

# updates 11/23
- submit job to cluster
- add cluster config

# updates 11/30
- added FASTQC and MULTIQC reports

# updates 12/7
- count_peaks functioning
- add index configs
- schema updated to confirm configs
- copy configs/manifests and use copy; allow for multiple processes to run at once
- updated ic name to handle multiple iterations
- add local execution
- create test files for hg38
"""
