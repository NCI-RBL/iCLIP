'''
Overview

- Multiplexed samples are split based on provided barcodes and named using provide manifests
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged

'''

report: "report/workflow.rst"

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict

'''
* Requirements *
- Four input files are required:
    1) cluster_config.yml
    2) snakemake_config.yaml
    3) multiplex.tsv
    4) samples.tsv

    1) cluster_config.yml must include:
        - gres: lscratch:256
        - mem: 48g
        - partition: norm
        - time: 4-00:00:00
        - threads: 4
        - cores: 8

    2) snakemake_config.yaml must include:
        - source_dir: '/path/to/iCLIP/'
        - out_dir: '/path/to/output/'
        - multiplex_manifest: '/path/to/multiplex_manifest.tsv'
        - sample_manifest: '/path/to/sample_manifest.tsv'
        - fastq_dir: '/path/to/raw/fastq/files'
        - novoalign_reference: selection of either hg38, mm10
        - minimum_count: minimum number of peaks for count

    3) multiplex.tsv should have the following headers:
        - file_name: the full file name of the multiplexed sample; this must be unique
        - multiplex: the basename to reference filename; this must be unique and will match with the multiplex column in the samples.tsv file

        An example multplex.tsv file:
            file_name                   multiplex
            SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1

    4) samples.tsv should have the following headers:
        - multiplex: the mutliplexed sample name; this will not be unique
        - sample: the final sample name; this must be unique
        - barcode: the barcode to identify multiplexed sample; this must be unique per each mutliplex sample name but can repeat between samples
        - adaptor: the adaptor used, to be removed from sample; this may or may not be unique
        - group: CNTRL must be used for control samples, but any other group designation (alpha numeric) is accepted

        An example sample.tsv file:
            multiplex       sample          group       barcode     adaptor
            SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
            SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
- In addition, users must provide a copy of the icount.sif file in  '/{source_dir}/container'.

* Execution *
To run:
    - clone https://github.com/RBL-NCI/iCLIP.git; fetch activeDev branch
    - update required files above
    - copy icount.sif into /{source_dir}/container/
    - cd /path/to/iCLIP/
    - module load snakemake
    - sh run_snakemake.sh [OPTION]
        --run for full run
        --unlock for failed run
        --test for dryrun
'''
#config
configfile:'config/snakemake_config.yaml'
source_dir = config['source_dir']
out_dir = config['out_dir']
sample_manifest = config['sample_manifest']
multiplex_manifest = config['multiplex_manifest']
fastq_dir = config['fastq_dir']
nova_ref = config['novoalign_reference']
min_count = config['minimum_count']

#output paths
split_dir=out_dir + '/split'
sam_dir=out_dir + '/sam'
bam=out_dir + '/bam'

#error checking of index selection
if nova_ref == 'mm10':
    nova_index = '/data/sevillas2/iCLIP/index/mm10_gencode_prerRNA/mm10_gencode_prerRNA'
elif nova_ref == 'hg38':
    nova_index =  '/data/sevillas2/iCLIP/index/hg38/hg38'
else:
    sys.exit("""
    You must select either mm10 or hg38 for a reference in config file.
    Please update config and then re-run snakemake workflow.
    """)

#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = fastq_dir + '/' + multiplex_dict[wildcards.mp]
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + '/' + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + '/' + wildcards.mp + '/'

    return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + '/' + wildcards.mp + '/demux_' + bc + '.fastq.gz' + ' ' + out_dir + '/' + wildcards.mp + '/01_renamed/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)

#command needed to cut adaptors from demux samples
def adapt_cmd(wildcards):
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp) & (df_samples['sample']==wildcards.sp)]

    base = out_dir + '/' + wildcards.mp + '/02_adaptor/' + wildcards.sp
    fq_un =  base + '_untrimmed.fastq.gz '
    fq_t = base + '_trimmed.fastq.gz '

    base = out_dir + '/' + wildcards.mp + '/01_renamed/' + wildcards.sp
    fq_o = base + '.fastq.gz '

    command_line = '--untrimmed_output ' + fq_un + '--reads_trimmed ' + fq_t + fq_o + sub_df.iloc[0]['adaptor']

    return(command_line)

#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=out_dir + '/' + wildcards.mp + '/03_unzip/' + wildcards.sp + '.fastq '

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]
    except:
        return(0)
    return(sub_df.iloc[0]['chunksize'])

#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

rule all:
    input:
        out_dir + "/manifest_clean.txt",
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list), #multiplexed sample fastq files exist
        #expand(join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),mp=samp_dict.keys()), #demultiplex
        #expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list), #rename
        #expand(join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz'), zip, mp=mp_list, sp=sp_list), #remove adaptors
        #expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list), #gunzip
        #join(out_dir,'split_params.tsv'), #determine_splits
        #expand(join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/12_bed/{sp}.bed'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/13_peaks/{sp}.txt'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/13_peaks/{sp}.SAF'),zip, mp=mp_list, sp=sp_list),
        expand(join(out_dir,'{mp}/14_gff/{sp}.gff3'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'{mp}/15_peak_counts/{sp}_FCountUnique.txt'),zip, mp=mp_list, sp=sp_list),

include: "rules/common.smk"
include: "rules/other.smk"

rule check_manifest:
    """
    Read in multiplex manifest and sample manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    If files are correct, outputs a temp file. If not, outputs error file.

    TODO: use rules/schemas to handle this
    """
    input:
        f1 = multiplex_manifest,
        f2 = sample_manifest
    params:
        rname='ic:manif',
    output:
        o1 = temp(out_dir + "/manifest_clean.txt")
    shell:
        "module load python; python workflow/scripts/01_check_manifest.py '{out_dir}/manifest_' {input.f1} {input.f2}"

rule demultiplex:
    """
    Reads in fastq files from multiplex_manifest.
    Finds multiplex match between manifest files, splits files based on list of
    barcodeIDs found in sample_manifest

    For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN

    file_name                   multiplex
    SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1

    multiplex       sample          group       barcode     adaptor
    SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
    SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
    """
    input:
        f1 = get_fq_names
    params:
        rname='ic:demux',
        ml = 15,
        cmd = demux_cmd,
        doc = source_dir + 'container/icount.sif'
    output:
        o1 = join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),
    shell:
        """
        module load singularity
        singularity exec -B /data/$USER,/fdb,/scratch \
        {params.doc} iCount demultiplex -ml {params.ml} {params.cmd}
        """

rule rename_files:
    """
    renames demultiplexed files based on
    from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
    to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
    """
    input:
        join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),
    params:
        rname='ic:rename',
        p1 = rename_cmd
    output:
        o1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    shell:
        'mv {params.p1}'

rule remove_adaptors:
    """
    removes adaptors from files, adds "trimmed" to file name
    """
    input:
        f1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    params:
        rname='ic:adapt',
        ml = 1,
        adapt = adapt_cmd,
        doc = source_dir + 'container/icount.sif',
    output:
        o1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    shell:
        """
        module load singularity
        singularity exec -B /data/$USER,/fdb,/scratch \
        {params.doc} iCount cutadapt -ml {params.ml} {params.adapt}
        """

rule gunzip_files:
    """
    unzips files
    """
    input:
        f1 = join(out_dir,'{mp}/02_adaptor/{sp}_untrimmed.fastq.gz')
    params:
        rname='ic:gunzip',
    output:
        o1 = join(out_dir,'{mp}/03_unzip/{sp}.fastq')
    shell:
        "gunzip -c {input.f1} > {output.o1}"

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to maintain integrity of
    4 line chunk in seq file
    """
    input:
        f1 = expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list),
    params:
        rname='ic:splitdoc',
        p1 = source_dir + 'scripts/02_find_split_parameters.sh'
    output:
        o1 = join(out_dir,'split_params.tsv')
    shell:
        """
        sh {params.p1} {output.o1} {input.f1}
        """

rule split_files:
    """

    """
    input:
        all = expand(join(out_dir,'{mp}/03_unzip/{sp}.fastq'), zip, mp=mp_list, sp=sp_list),
        f1 = join(out_dir,'{mp}/03_unzip/{sp}.fastq'),
        sp = join(out_dir,'split_params.tsv')
    params:
        rname='ic:split',
        p1 = join(out_dir,'{mp}/04_split/{sp}.split.'),
        c = get_filechunk_size
    output:
        o1 = dynamic(join(out_dir,'{mp}/04_split/{sp}.split.{n}.fastq'))
    shell:
        """
        split --additional-suffix .fastq -l {params.c} --numeric-suffixes=10 {input.f1} {params.p1}
        """

rule novoalign:
    """
    aligns files to index given; output is a sam file

    updates:
    1) A reporting limit ( -r All limit )should be set with the -r All option. Defaulting to 999.
    2) Error: Unable to determine file format, test.split01.fastq
 Format could be one of : SLXFQ STDFQ ILMFQ --> add -F STDFQ flag
    3) output interpreted as an error - add code to handle output vs real errors

    """
    input:
        f1 = join(out_dir,'{mp}/04_split/{sp}.split.{n}.fastq'),
    params:
        rname='ic:novo',
        n_index = nova_index,
    output:
        o1 = join(out_dir,'{mp}/05_sam/{sp}.split.{n}.sam'),
    shell:
        """
        set +e
        module load novocraft; novoalign -d {params.n_index} \
        -f {input.f1} -F STDFQ -c 32 \
        -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
        -R 0 -r All 999 > {output.o1}
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule split_mm_unique:
    """
    creates two text files from sam file
    1. unique file without NH:i
    2. MM file with NH:i

    #grab lines without NH tag - becomes "unique file"

    #grab lines with NH tag - becomes "MM file"
    """
    input:
        f1 = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
    params:
        rname='ic:split_mu',
    output:
        o1 = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.unique.txt'),
        o2 = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.mm.txt'),
    shell:
        """
        set +e
        module load samtools; samtools view {input.f1} | grep -v 'NH:i' > {output.o1}; \
        samtools view {input.f1} | grep 'NH:i' > {output.o2};
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule sam_to_header:
    """
    read sam file header, print to header txt
    """
    input:
        sam = join(out_dir,'{mp_st}/05_sam/{sp_st}.split.{n}.sam'),
    params:
        rname='ic:sam_head',
    output:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
    shell:
        """
        module load samtools; samtools view -H {input.sam} > {output.h};
        """

rule create_bam_unique:
    """
    add nh:i:1 tag to "unique file", cat header, create unique bam file
    """
    input:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
        un = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.unique.txt'),
    params:
        rname='ic:bam_u',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.bam'),
    shell:
        """
        module load samtools;
        awk -F '\t' -v OFS='\t' '{{ $(NF+1) = "NH:i:1"; print }}' {input.un} | cat {input.h} - | \
        samtools sort | samtools view -Sb > {output.o1};
        """

rule create_bam_mm:
    """
    cat header, create mm bam file
    """
    input:
        h = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.header.txt'),
        mm = join(out_dir,'{mp_st}/06_reads/{sp_st}.split.{n}.mm.txt'),
    params:
        rname='ic:bam_m',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.bam'),
    shell:
        """
        module load samtools; cat {input.h} {input.mm} | samtools sort | samtools view -Sb > {output.o1}
        """

rule sort_mm_and_unique:
    """
    sort both the mm and unique bam files
    """
    input:
        un = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.bam'),
        mm = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.bam'),
    params:
        rname='ic:sort_mu',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.s.bam'),
        o2 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.un} -o {output.o1}; \
        samtools sort {input.mm} -o {output.o2};
        """

rule index_mm_and_unique:
    """
    index both the mm and unique bam files
    """
    input:
        un = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.s.bam'),
        mm = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.s.bam'),
    params:
        rname='ic:index_mu',
    output:
        o1 = join(out_dir,'{mp_st}/07_bam_unique/{sp_st}.split.{n}.unique.i.bam'),
        o2 = join(out_dir,'{mp_st}/07_bam_mm/{sp_st}.split.{n}.mm.i.bam'),
    shell:
        """
        module load samtools; \
        cp {input.un} {output.o1}; samtools index {output.o1}; \
        cp {input.mm} {output.o2}; samtools index {output.o2};
        """

rule merge_splits_mm:
    """
    merge split mm bam files into merged.mm bam file
    """
    input:
        f1 = dynamic(join(out_dir,'{mp}/07_bam_mm/{sp}.split.{n}.mm.i.bam'))
    params:
        rname='ic:merge_m',
    output:
        o1 = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.f1}
        """

rule merge_splits_unique:
    """
    merge split unique bam files into merged.unique bam file
    """
    input:
        f1 = dynamic(join(out_dir,'{mp}/07_bam_unique/{sp}.split.{n}.unique.i.bam'))
    params:
        rname='ic:merge_u',
    output:
        o1 = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.f1}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files
    """
    input:
        un = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.unique.bam'),
        mm = join(out_dir,'{mp}/08_bam_merged_splits/{sp}.merged.mm.bam'),
    params:
        rname='ic:merge_mu',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),
    shell:
        """
        module load samtools; samtools merge -f {output.o1} {input.un} {input.mm}
        """

rule sort_merged:
    """
    sort merged.bam
    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.bam'),
    params:
        rname='ic:sort_m',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.f1} -o {output.o1};
        """

rule index_merged:
    """
    index merged.s.bam files
    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.s.bam'),
    params:
        rname='ic:index_m',
    output:
        o1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule dedup:
    """
    deduplicate merged.i.bam files


    """
    input:
        f1 = join(out_dir,'{mp}/09_bam_merged/{sp}.merged.i.bam'),
    params:
        rname='ic:dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),
        o2 = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.log'),
    shell:
        """
        module load umitools; umi_tools dedup \
        -I {input.f1} \
        --method unique --multimapping-detection-method=NH --umi-separator=rbc: \
        -S {output.o1} \
        --log2stderr -L {output.o2};
        """

rule sort_dedup:
    """
    sort dedup.bam file
    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.bam'),
    params:
        rname='ic:sort_dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.s.bam'),
    shell:
        """
        module load samtools; samtools sort {input.f1} -o {output.o1};
        """

rule index_dedup:
    """
    index dedup.s.bam file
    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.s.bam'),
    params:
        rname='ic:index_dedup',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule dedup_header:
    """
    cat bam header

    """
    input:
        f1 = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
    params:
        rname='ic:dedup_head',
    output:
        o1 = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.header.txt'),
    shell:
        """
        module load samtools; samtools view -H {input.f1} > {output.o1}
        """

rule split_dedup_mm_and_unique:
    """
    split dedup file into mm file


    """
    input:
        bam = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
        h = join(out_dir,'{mp}/10_dedup_log/{sp}.dedup.header.txt'),
    params:
        rname='ic:split_dedup',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.bam'),
        o2 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.bam'),
    shell:
        """
        set +e
        module load samtools; \
        samtools view {input.bam} | grep -v -w 'NH:i:1' | cat {input.h} - | samtools sort | samtools view -Sb > {output.o1}; \
        samtools view {input.bam} | grep -w 'NH:i:1' | cat {input.h} - |  samtools sort | samtools view -Sb > {output.o2}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule index_split_dedup_mm:
    """
    index split deduped mm file
    """
    input:
        f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.bam'),
    params:
        rname='ic:index_splitm',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.mm.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule index_split_dedup_unique:
    """

    """
    input:
        f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.bam'),
    params:
        rname='ic:index_splitu',
    output:
        o1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),
    shell:
        """
        module load samtools; cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule create_bed:
    """

    """
    input:
        f1 = join(out_dir,'{mp}/11_dedup_split/{sp}.dedup.unique.i.bam'),
    params:
        rname='ic:bed',
    output:
        o1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
    shell:
        """
        module load bedtools; bedtools bamtobed -i {input.f1} > {output.o1}
        """

rule merge_bed_txt:
    """

    """
    input:
        f1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
    params:
        rname='ic:bed_txt',
    output:
        o1 = join(out_dir,'{mp}/13_peaks/{sp}.txt'),
    shell:
        """
        module load bedtools; \
        bedtools merge -c 6 -o count,distinct -s -d -1 -i {input.f1} > {output.o1}
        """

rule merge_bed_saf:
    """

    """
    input:
        f1 = join(out_dir,'{mp}/12_bed/{sp}.bed')
    params:
        rname='ic:bed_saf',
    output:
        o1 = join(out_dir,'{mp}/13_peaks/{sp}.SAF')
    shell:
        """
        module load bedtools; \
        bedtools merge -c 6 -o count,distinct -s -d -1 -i {input.f1} | \
        awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2+1,$3,$5}}' | awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > \
        {output.o1}
        """

rule ID_peaks:
    """

    """
    input:
        f1 = join(out_dir,'{mp}/13_peaks/{sp}.txt')
    params:
        rname='ic:idpeaks',
        s = source_dir + 'scripts/03_select_peaks.R',
        dir = join(out_dir,'{mp}/14_gff/{sp}'),
        min = min_count
    output:
        o1 = join(out_dir,'{mp}/14_gff/{sp}.gff3'),
        o2 = join(out_dir,'{mp}/14_gff/{sp}.gtf')
    shell:
        """
        module load R; \
        Rscript {params.s} {input.f1} {params.dir} {params.min}
        """

rule create_peak_table:
    """
    Rscript /scratch/homanpj/iCLIP_pipeline/4_Peak_Identification/CreateTable.R \
    /scratch/homanpj/iCLIP_pipeline/peaks/Ro_Clip_iCountcutadpt_all.unique.NH.mm.ddup.s.unique.bam.peaks.txt \
    /scratch/homanpj/iCLIP_pipeline/peaks/ \
    dedup/unique.NH.mm.ddup.s.bam \
    .PeakcountsUniq 1

    CreateTable.R
    """
    input:
        f1 = join(out_dir,'{mp}/13_peaks/{sp}.txt')
    params:
        rname='ic:peaktab',
        s = source_dir + 'scripts/04_create_peak_table.R',
        dir = join(out_dir,'{mp}/13_peaks/{sp}')
    output:
        o1 = "tbd"
    shell:
        """
        module load R; \
        {params.s} {params.dir}
        """

rule count_peaks:
    """
    Q: seems fairly time consuming... thinking of separating into sep rules rather than
    processing at once in FeatureCounts.sh file?
    """
    input:
        b = join(out_dir,'{mp}/10_dedup_bam/{sp}.dedup.i.bam'),
        anno = join(out_dir,'{mp}/13_peaks/{sp}.SAF'),
    params:
        rname='ic:peakcount',
        s = source_dir + 'scripts/05_count_features.sh',
        threads = 8,
        base = join(out_dir,'{mp}/15_peak_counts/'),
        sample = '{sp}'
    output:
        o1 = join(out_dir,'{mp}/15_peak_counts/{sp}_FCountUnique.txt'),
        o2 = join(out_dir,'{mp}/15_peak_counts/{sp}_FCountAll.txt'),
        o3 = join(out_dir,'{mp}/15_peak_counts/{sp}_FCountAll_frac.txt'),
        o4 = join(out_dir,'{mp}/15_peak_counts/{sp}_FCountAll_primary.txt'),
        o5 = join(out_dir,'{mp}/15_peak_counts/{sp}_FCountAll_FracPrime.txt'),
    shell:
        """
        module load subread; \
        sh {params.s} {params.threads} {input.anno} {input.b} {params.base} {params.sample}
        """

"""
https://github.com/RBL-NCI/iCLIP_Pipeline/tree/master/3_Deduplicate

# updates 10/26
- new multiplex manifest to take in filenames - flexibility in file naming
- script to check manifests - ensure concordance and flags errors before pipeline runs
- add rules after split_files

# updates 11/2
- differentiate for control vs study samples [controls don't get split]
- use created docker for iCOUNT '/data/sevillas2/iCLIP/container/icount.sif'
- control calculation for split; otherwise lines were splitting the 4-line seq chunk
- error handling of novoalign
- completed runs through rule samtools_sort_index_merge; equivalent of 3_dedup - 1,2a,2b
    - sintearctive --mem=16g --cpus-per-task=4
    - split_number = 30
    - one sample 20K reads = 10min; two samples 10K reads = 14min

# updates 11/16
- update req files and execution documentation
- added logic for variable number of lines - dynamic split will vary by size using 02_fine_split_parameters script
-- need to determine max size of file
- re-organize dir structure
- created 03_select_peaks and 05_count_features scripts; copied 04_create_peak_table - have Q
- testing with one sample, two demultiplexed - 13 minutes to complete
- all rules but create_peak_table,count_peaks execute output

# TODO:
- workflow
    - create_peak_table
- QC metrics
    - adaptors left over
    - counts / barcodes, counts / reads not assigned
- submit jobs to cluster
- figure out time parameters
- set temp files

"""
