'''
* Authors * 
S. Sevilla
P. Homan

* Overview * 
- Multiplexed samples are split based on provided barcodes and named using provide manifests, maximum 10 samples
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged

* Requirements *
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml
import csv

###############################################################
# config handling, set parameters
###############################################################
#snakemake config
source_dir = config['sourceDir']
cont_dir = config['containerDir']
out_dir = config['outputDir'].rstrip('/') + '/'
fastq_dir = config['fastqDir'].rstrip('/') + '/'

sample_manifest = config['sampleManifest']
multiplex_manifest = config['multiplexManifest']
manorm_manifest = config['contrastManifest']

nova_ref = config['reference']
splice_aware = config['spliceaware'].capitalize()
include_rRNA = config['includerRNA'].capitalize()
splice_bp = config['spliceBPlength']
multiplex_flag = config['multiplexflag'].capitalize()
mismatch = config['mismatch']
split_value =  config['split']
min_count = config['mincount']
nt_merge = str(config['ntmerge']) + 'nt'
peak_id = config['peakid'].upper()
DE_method = config['DEmethod'].upper()
sp_junc = config['splicejunction'].upper()
condense_exon = config['condenseexon'].upper()


#expand reference selection
if (nova_ref == "mm10"):
    genome_ref = "GENCODE mm10 v23"
elif (nova_ref == "hg38"):
    genome_ref = "GENCODE hg38 v32"

#read in index config file and assign paths
index_manifest = join(source_dir, 'config', 'index_config.yaml')

with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)

gen_path = index_list[nova_ref]['gencodepath']
rseq_path = index_list[nova_ref]['refseqpath']
can_path = index_list[nova_ref]['canonicalpath']
intron_path = index_list[nova_ref]['intronpath']
rmsk_path = index_list[nova_ref]['rmskpath']
alias_path = index_list[nova_ref]['aliaspath']
add_anno_path = index_list[nova_ref]['additionalannopath']

#singularity exec command
singularity_exec = "singularity exec -B /data/$USER,/data/CCBR_Pipeliner," + out_dir + "," + fastq_dir + ",/fdb,/scratch"

#annotation config
annotation_config = join(source_dir,'config','annotation_config.txt')

#convert splice junction selection
if (sp_junc=="Y"):
    sp_junc = "TRUE"
else:
    sp_junc = "FALSE"

#convert condense junction selection
if (condense_exon=="Y"):
    cond_exon = "TRUE"
else:
    cond_exon = "FALSE"

#create list of alignment types based on splice_aware flag
if (splice_aware == 'N'):
    align_list = ["unaware"]
else:
    align_list = ["unmasked"]

#convert rRNA selection
if (include_rRNA=="Y"):
    refseq_rrna = "TRUE"
else:
    refseq_rrna = "FALSE"

#set strand ids
strand_list=['P','N']

###############################################################
# create sample lists
###############################################################
#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

###############################################################
# snakemake functions
###############################################################
#determines the number of files to split the input into
def check_split(split_in):
    if (type(split_in) == str):
        split_replace = split_in.replace(',', '')
        if (type(split_in) == str):
            split_replace = 1000000
    elif (split_in < 3000):
        split_replace = 1000000
    elif (type(split_in) == float):
        split_replace = int(split_in)
    else:
        split_replace = split_in
    return split_replace

#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = join(fastq_dir,multiplex_dict[wildcards.mp])
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + wildcards.mp + '/01_preprocess/',

    return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + wildcards.mp + '/01_preprocess/demux_' + bc + '.fastq.gz' + ' ' + out_dir + wildcards.mp + '/01_preprocess/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)
    
#create nondemux command line
def nondemux_cmd(wildcards):
  cmd_line = ''

  for k,v in multiplex_dict.items():
    for k2,v2 in samp_dict[k].items():
      cmd_line = fastq_dir + v + ' ' + out_dir + k + '/01_preprocess/' + k2 + '.fastq.gz; ' + cmd_line
  
  return(cmd_line)

#command needed to cut adaptors from demux samples
#example: 
        #singularity exec -B /scratch /data/CCBR_Pipeliner/iCLIP/container/icount.sif iCount cutadapt -ml --untrimmed_output /path/_untrimmed.fastq.gz 
        # --reads_trimmed /path/sp_trimmed.fastq.gz /path/fastq.gz ABCDEDFGH
def adapt_cmd(wildcards):
    cmd=''

    for sp in sp_list:
        #sub df
        df_sub = df_samples[(df_samples['sample']==sp)]

        #create cmd
        filtered = join(out_dir, '01_preprocess', sp + '_filtered.fastq.gz')
        removed =  join(out_dir, '01_preprocess', sp + '_removed_seq.fastq.gz ')
        fastq = join(out_dir, df_sub.iloc[0]['multiplex'], '01_preprocess',sp + '.fastq.gz')

        cmd = singularity_exec + ' ' + join(cont_dir,'icount.sif') + ' iCount cutadapt -ml 1 --untrimmed_output ' + filtered + ' --reads_trimmed ' + removed + ' ' + fastq + ' ' + df_sub.iloc[0]['adaptor'] + '; ' + cmd
    
    return(cmd)
    
#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=join(out_dir,'01_preprocess',wildcards.sp + '_filtered.fastq.gz ')

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'qc','split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]

        #split can only create a max of 99 split files; handle large file sets to ensure max reads per 99 files
        if (sub_df.iloc[0]['filenum']>89):
            chunk_size = int(((sub_df.iloc[0]['chunksize'] * sub_df.iloc[0]['filenum']) / 88)+.9)
        else:
            chunk_size = sub_df.iloc[0]['chunksize']

    except:
        return(0)
    return(chunk_size)

#determine which umi separator to use
#iCount addes rbc: to all demux files; external demux uses an _
def umi_parameter(wildcards):
    if(multiplex_flag == 'Y'):
        umi_sep="rbc:"
    else:
        umi_sep="_"
    return(umi_sep)

#get index file depending on alignment type
def get_index(wildcards):

    #unaware index
    if (wildcards.al == "unaware"):
        nova_index = index_list[nova_ref]['std']
    elif (wildcards.al == "masked"):
        nova_index = index_list[nova_ref]['spliceawaremasked'][str(splice_bp)+'bp']
    elif (wildcards.al == "unmasked"):
        nova_index = index_list[nova_ref]['spliceawareunmasked'][str(splice_bp)+'bp']
    
    return(nova_index)

#determine alignment input based on splice_aware flag
def get_align_input(wildcards):
    if (splice_aware == "N"):
        f1 = join(out_dir,'01_preprocess','01_alignment', wildcards.sp + '.' + wildcards.al + '.split.' + wildcards.n + '.sam'),
    else:
        f1 = join(out_dir,'01_preprocess','03_genomic', wildcards.sp + '.' + wildcards.al + '.split.' + wildcards.n + '.sam'),
    return(f1)

#read in contrast list for manorm
def get_manorm_list():
    
    #read file
    with open(manorm_manifest) as f:
        reader = csv.reader(f, delimiter="\t")
        manorm_file = list(reader)

        #for each group in comparison list
        manorm_list=[]
        for group in manorm_file:
            
            #for each individual id
            for id in group:
                id = id.replace(",", "_vs_")
            
            #append final list
            manorm_list.append(id)
        
        #remove header
        manorm_list.pop(0)
    return(manorm_list)

#get the input files for analysis
def get_MANORM_analysis_input(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir,'14_MAnorm', '01_input',gid_1 + '_PeaksforMAnrom.bed'),
                join(out_dir,'14_MAnorm', '01_input',gid_1 + '_ReadsforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'14_MAnorm', '01_input',gid_1 + '_PeaksforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'14_MAnorm', '01_input',gid_2 + '_PeaksforMAnrom.bed'),
                join(out_dir,'14_MAnorm', '01_input',gid_2 + '_ReadsforMAnrom_' + wildcards.strand + '.bed'),
                join(out_dir,'14_MAnorm', '01_input',gid_2 + '_PeaksforMAnrom_' + wildcards.strand + '.bed')]
    return(input_list)

#get sample name for manorm comparison - sample
def get_MANORM_gid1(wildcards):
    gid = wildcards.group_id.split("_vs_")[0]
    return(gid)

#get sample name for manorm comparison - background
def get_MANORM_gid2(wildcards):
    gid = wildcards.group_id.split("_vs_")[1]
    return(gid)

#get input files for post-processing
def get_MANORM_processing_input(wildcards):
    gid_1 = wildcards.group_id.split("_vs_")[0]
    gid_2 = wildcards.group_id.split("_vs_")[1]

    input_list = [join(out_dir,'13_annotation', '02_peaks', gid_1 + '_peakannotation_complete.txt'),
                join(out_dir,'13_annotation', '02_peaks', gid_2 + '_peakannotation_complete.txt'),
                join(out_dir,'14_MAnorm','02_analysis', wildcards.group_id + "_P", wildcards.group_id + '_all_MAvalues.xls'),
                join(out_dir,'14_MAnorm','02_analysis', wildcards.group_id + "_N", wildcards.group_id + '_all_MAvalues.xls')]
    return(input_list)

###############################################################
# main code
###############################################################
#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

#get split files value
split_value = check_split(split_value)

#create manorm list
manorm_list = get_manorm_list()

###############################################################
# rule all
###############################################################
#set rule_all inputs depending on flags:
## if samples have been multiplexed
if multiplex_flag == 'Y':
    input_multiplex = [expand(join(out_dir,'{mp}', '00_qc_post','{mp}_barcode.png'), mp=samp_dict.keys()),
                        expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
                        expand(join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz'), mp=samp_dict.keys())]
else:
    input_multiplex = [expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list)]

## if samples are spliced
if splice_aware == 'Y':
    input_unmapped = expand(join(out_dir,'04_sam','04_unmapped','{sp}.{al}.complete.bam'),  sp=sp_list, al=align_list)

    input_splice = [expand(join(out_dir,'10_mapq_score','{sp}.readids.txt'), sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.unaware.subset.bam'),sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.mapq_recalculated.bam'),sp=sp_list)]
else:
    input_unmapped = expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'), sp=sp_list, al=align_list),

    input_splice = [expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'),sp=sp_list, al=align_list)]

#if samples are running MANORM
if DE_method == "MANORM":
    input_manorm_input = [expand(join(out_dir,'14_MAnorm', '01_input','{sp}_PeaksforMAnrom.bed'),sp=sp_list),
            expand(join(out_dir,'14_MAnorm', '01_input','{sp}_PeaksforMAnrom_{strand}.bed'),sp=sp_list,strand=strand_list),
            expand(join(out_dir,'14_MAnorm', '01_input','{sp}_ReadsforMAnrom_{strand}.bed'),sp=sp_list,strand=strand_list)]

    input_manorm_analysis = [expand(join(out_dir,'14_MAnorm','02_analysis', '{group_id}_{strand}','{group_id}_all_MAvalues.xls'),group_id=manorm_list,strand=strand_list)]
    
    input_manorm_post_proccessing = [expand(join(out_dir,'14_MAnorm','02_analysis', '{group_id}','{group_id}_post_processing.txt'),group_id=manorm_list)]
    
    input_manorm_reports = [expand(join(out_dir,'14_MAnorm','03_report','{group_id}_manorm_report.html'),group_id=manorm_list)]
else:
    input_manorm_input = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),sp=sp_list)]
    input_manorm_analysis = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),sp=sp_list)]
    input_manorm_post_proccessing = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),sp=sp_list)]
    input_manorm_reports = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),sp=sp_list)]

#local rules
localrules: check_manifest, determine_splits

rule all:
    input:
        #check manifest
        join(out_dir,'qc','manifest_clean.txt'),

        #check input fastq files
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list), #multiplexed sample fastq files exist
        
        #different inputs for multiplexed or nonmultiplexed pipeline
        input_multiplex,
        
        #Remove adaptors, rename samples
        expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list),

        #FastQC
        expand(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list,sp=sp_list),
        expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        
        #FastQ Screen
        expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered_screen.txt'),sp=sp_list),
        expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered_screen.txt'),sp=sp_list),
        
        #Determine file split parameters
        join(out_dir,'qc','split_params.tsv'),
        
        #Merge splits
        expand(join(out_dir,'03_bams','{sp}.{al}.merged.unique.bam'), sp=sp_list, al=align_list),
        expand(join(out_dir,'03_bams','{sp}.{al}.merged.mm.bam'),sp=sp_list, al=align_list),

        # #Merge unique and mm
        # expand(join(out_dir,'03_bams','{sp}.{al}.merged.bam'), sp=sp_list, al=align_list),

        # #Sort and index merged
        # expand(join(out_dir,'03_bams','{sp}.{al}.merged.si.bam'), sp=sp_list, al=align_list),
        # expand(join(out_dir,'03_bams','{sp}.{al}.merged.si.bam.bai'), sp=sp_list, al=align_list),

        # #Samstats
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'), sp=sp_list, al=align_list),

        # #MultiQC Report
        # join(out_dir,'qc/multiqc_report.html'),

        # #Alignment stats
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),

        # #QC Troubleshooting
        # join(out_dir,'qc','qc_report.html'),
        
        # #Unmapped read output
        # input_unmapped,

        # #Deduplicate
        # expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'), sp=sp_list, al=align_list),
        
        # #Bam processing
        # expand(join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'), sp=sp_list),

        # #Bed files
        # expand(join(out_dir,'10_bed','{sp}_all.bed'), sp=sp_list),
        # expand(join(out_dir,'10_bed','{sp}_unique.bed'), sp=sp_list),

        # #SAF 
        # expand(join(out_dir,'11_SAF','{sp}_all.SAF'), sp=sp_list),
        # expand(join(out_dir,'11_SAF','{sp}_unique.SAF'), sp=sp_list),

        # #Count features
        # expand(join(out_dir,'12_counts','uniquereadpeaks','{sp}_uniqueCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'12_counts','uniquereadpeaks','{sp}_allFracMMCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'12_counts','allreadpeaks','{sp}_uniqueCounts.txt'), sp=sp_list),
        # expand(join(out_dir,'12_counts','allreadpeaks','{sp}_allFracMMCounts.txt'), sp=sp_list),

        # #annotations
        # join(out_dir,'13_annotation', '01_project','annotations.txt'),
        # expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),sp=sp_list),
        # expand(join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),sp=sp_list),
        # expand(join(out_dir,'13_annotation', '{sp}_annotation_final_report.html'),sp=sp_list),
        # expand(join(out_dir,'13_annotation', '{sp}_annotation_final_table.txt'),sp=sp_list),

        # #MANORM
        # input_manorm_input,
        # input_manorm_analysis,
        #input_manorm_post_proccessing,
        #input_manorm_reports

#common and other SMK 
if source_dir == "":
    include: "workflow/rules/common.smk"
    include: "workflow/rules/other.smk"
else:
    include: join(source_dir,"workflow/rules/common.smk")
    include: join(source_dir,"workflow/rules/other.smk")

###############################################################
# snakemake rules
###############################################################
rule check_manifest:
    """
    Read in multiplex manifest and sample manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    If files are correct, outputs a temp file. If not, outputs error file.
    """
    input:
        f1 = multiplex_manifest,
        f2 = sample_manifest
    params:
        py = join(source_dir,'workflow','scripts','01_check_manifest.py'),
        base = join(out_dir,'qc','manifest_')
    envmodules:
        config['python']  
    output:
        o1 = join(out_dir, 'qc', 'manifest_clean.txt'),
    shell:
        """
        python {params.py} \
            '{params.base}' {input.f1} {input.f2}
        """

#pipeline branches to demultiplex, if necessary
if (multiplex_flag == 'Y'):
    rule qc_barcode:
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname="01a_qc_barcode",
            py = join(source_dir,'workflow', 'scripts', '02_barcode_qc.py'),
            mm = mismatch,
        envmodules:
            config['python'],
        output:
            o1 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys()),
            o2 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys())
        shell:
            """
            python {params.py} \
                {sample_manifest} {multiplex_manifest} {fastq_dir} {out_dir} {params.mm}
            """

    rule demultiplex:
        """
        https://icount.readthedocs.io/en/latest/ref_CLI.html
        Reads in fastq files from multiplex_manifest.
        Finds multiplex match between manifest files, splits files based on list of
        barcodeIDs found in sample_manifest
        For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN
        file_name                   multiplex
        SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1
        multiplex       sample          group       barcode     adaptor
        SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
        SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
        """
        input:
            f1 = get_fq_names,
        params:
            rname='01b_demux',
            s_exec = singularity_exec,
            doc = join(cont_dir,'icount.sif'),
            cmd = demux_cmd,
            ml = 15,
            mm = mismatch
        envmodules:
            config['singularity'],
        output:
            o1 = temp(join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz')),
        shell:
            """
            {params.s_exec} {params.doc} iCount demultiplex --minimum_length {params.ml} --mismatches {params.mm} {params.cmd}
            """

    rule rename_demux:
        """
        renames demultiplexed files based on
        from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
        to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
        """
        input:
            f1 = join(out_dir,'{mp}','01_preprocess','demux_nomatch5.fastq.gz')
        params:
            rname = '01c_rename',
            cmd = rename_cmd
        output:
            o1 = temp(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'))
        shell:
            """
            #move files
            mv {params.cmd}
            """     
else: 
    rule copy_nondemux:
        """
        copies fastq files from source location and moves into renamed file folder
        """
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname = '01_nondemux',
            cmd = nondemux_cmd
        output:
            o1 = temp(expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list))
        shell:
            """
            cp {params.cmd}
            """

rule remove_adaptors:
    """
    removes adaptors from files. software outputs:
    1) untrimmed.fastq.gz - the remaining sequences without the adaptor --> filtered.fastq.gz
    2) trimmed.fastq.gz - the sequences that were trimmed from the input file --> removed_seq.fastq.gz
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list),
    params:
        rname='02_adapt',
        cmd = adapt_cmd,
    envmodules:
        config['singularity'],    
    output:
        o1 = temp(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz')),
        o2 = temp(join(out_dir,'01_preprocess','{sp}_removed_seq.fastq.gz'))
    shell:
        """
        {params.cmd}
        """

rule qc_fastq_pre:
    """
    Runs FastQC report on each sample before adaptors have been removed
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_preprocess','{sp}.fastq.gz'),mp=mp_list,sp=sp_list)
    params:
        rname='04_qc_fastq_pre',
        base = join(out_dir,'{mp}','00_qc_pre/')
    envmodules:
        config['fastqc']
    output:
        o1 = temp(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'))
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_fastq_post:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        f1 = expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list)
    params:
        rname='04_qc_fastq_post',
        base = join(out_dir, 'qc', '00_qc_post'),
    envmodules:
        config['fastqc']
    output:
        o1 = temp(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'))
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_screen:
    """
    this will align first to human, mouse, bacteria then will align to rRNA
    must run fastq_screen as two separate commands - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz')
    params:
        rname='06_qc_screen',
        tmp = join('{sp}_filtered.fastq'),
        base_species = join(out_dir, 'qc', '00_qc_screen_species'),
        conf_species = join(source_dir,'config','fqscreen_species_config.conf'),
        base_rrna = join(out_dir, 'qc', '00_qc_screen_rrna'),
        conf_rrna = join(source_dir,'config','fqscreen_rrna_config.conf'),
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = temp(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered_screen.txt')),
        o2 = temp(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered_screen.txt')),
    shell:
        '''
        #make tmp dir for pipeline if running locally
        if [[ -d /lscratch/${{SLURM_JOB_ID}} ]]; then tmp_dir="/lscratch/${{SLURM_JOB_ID}}/"; else tmp_dir={out_dir}tmp; fi;
        if [ "$tmp_dir" = {out_dir}tmp ] & [ ! -d {out_dir}tmp ]; then mkdir {out_dir}tmp; fi;

        #gunzip input files
        gunzip -c {input.f1} > ${{tmp_dir}}/{params.tmp};

        #run screen
        fastq_screen --conf {params.conf_species} --outdir {params.base_species} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            ${{tmp_dir}}/{params.tmp};
        fastq_screen --conf {params.conf_rrna} --outdir {params.base_rrna} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            ${{tmp_dir}}/{params.tmp};
        
        '''

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to split FASTQ file
    split is necessary to improve speed of alignment
    split is done based off split_value input, maintaining every 4 lines = 1 seq
    """
    input:
        f1 = expand(join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),sp=sp_list)
    params:
        sh = join(source_dir, 'workflow', 'scripts', '03_find_split_parameters.sh'),
    output:
        o1 = join(out_dir,'qc','split_params.tsv')
    shell:
        """
        sh {params.sh} {output.o1} {split_value} {input.f1}
        """

rule split_files:
    """
    performs split of fastq file into smaller files using parameters in split_params.tsv
    """
    input:
        f1 = join(out_dir,'01_preprocess','{sp}_filtered.fastq.gz'),
        sp = join(out_dir,'qc','split_params.tsv')
    params:
        rname='08_split_files',
        base = join(out_dir,'01_preprocess','splits','{sp}.split.'),
        cmd = get_filechunk_size
    output:
        o1 = temp(dynamic(join(out_dir,'01_preprocess','splits','{sp}.split.{n}.fastq.gz')))
    shell:
        """
        gunzip {input.f1} | split --additional-suffix .fastq -l {params.cmd} --numeric-suffixes=1 --filter ='gzip > $FILE.gz' - {params.base};
        """

rule novoalign:
    """
    alignment for non-splice aware pipeline and for splice aware pipeline MAPQ recalculation
    """
    input:
        f1 = join(out_dir,'01_preprocess','splits','{sp}.split.{n}.fastq.gz')
    params:
        rname='09_novoalign',
        n_index = get_index,
    envmodules:
        config['novocraft']
    output:
        o1 = temp(join(out_dir,'01_preprocess','01_alignment','{sp}.{al}.split.{n}.sam')),
    shell:
        """
        gunzip {input.f1} | \
        novoalign -d {params.n_index} -k \
        -f - -F STDFQ -c 32 \
        -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
        -R 0 -r ALL 10 > {output.o1}
        """

#pipeline branches for splice_aware processing
if (splice_aware == "Y"):
    rule sam_cleanup:
        """
        performs cleanup for inserts
        exitcode error fixed with manual entry
        """
        input:
            f1 = join(out_dir,'01_preprocess','01_alignment','{sp}.{al}.split.{n}.sam'),
        params:
            rname = '10a_sam_cleanup',
            base = '{sp}.{al}.split.{n}'
        envmodules:
            config['samtools']  
        output:
            mapped = temp(join(out_dir,'01_preprocess','02_cleanup','{sp}.{al}.split.{n}.tmp.mapped.sam')),
            unmapped = temp(join(out_dir,'01_preprocess','02_cleanup','{sp}.{al}.split.{n}.tmp.unmapped.sam')),
        shell:
            """
            if [ -w "/lscratch/${{SLURM_JOB_ID}}" ]; then
                tmpdir="/lscratch/${{SLURM_JOB_ID}}"
            else
                tmpdir=out_dir + "tmp/"

                if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            fi

            set +e
            samtools view {input.f1} | awk '{{ if (($4 == 1 && $6!~/^[0-9]I/ && $1~/:/ )||($4 > 1 && $1~/:/ )) {{ print }} }}' > ${{tmpdir}}{params.base}.tmp.sam;
            samtools view -H {input.f1} | cat - ${{tmpdir}}{params.base}.tmp.sam > {output.mapped}; 
            samtools view -f 4 {input.f1} > {output.unmapped};
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """

    rule convert_transcript:
        """
        converts transcriptome coordinates to genomic coordinates
        uses USeq version 8.9.6

        unmapped reads are removed during genomic coordinates conversion, need to include them in final file
        convert sam file to bam file
        
        merge unmapped reads from cleanup to into final sam
        convert bam back to sam file
        """
        input:
            f1 = join(out_dir,'01_preprocess','02_cleanup','{sp}.{al}.split.{n}.tmp.mapped.sam'),
            unmapped = join(out_dir,'01_preprocess','02_cleanup','{sp}.{al}.split.{n}.tmp.unmapped.sam'),
        params:
            rname = '10b_convert',
            base = join(out_dir,'01_preprocess','03_genomic','{sp}.{al}.split.{n}.sam'),
            doc = join(cont_dir,'USeq_8.9.6','Apps/SamTranscriptomeParser')
        envmodules:
            config['java']  
        output:
            converted = temp(join(out_dir,'01_preprocess','03_genomic','{sp}.{al}.split.{n}.sam')),
        shell:
            """
            java -Djava.io.tmpdir={params.base} -jar -Xmx50G -jar {params.doc} \
            -f {input.f1} \
            -a 50000 -n 25 -u -s {params.base};
            gunzip -c {output.converted}.gz > {output.converted}
            """

    rule merge_unmapped_reads:
        """
        unmapped reads are removed during genomic coordinates conversion, need to include them in final file
        convert sam file to bam file
        
        merge unmapped reads from cleanup to into final sam
        convert bam back to sam file
        """
        input:
            converted = join(out_dir,'01_preprocess','03_genomic','{sp}.{al}.split.{n}.sam'),
            unmapped = join(out_dir,'01_preprocess','02_cleanup','{sp}.{al}.split.{n}.tmp.unmapped.sam'),
        params:
            rname = '10d_mergeunmapped',
        envmodules:
            config['samtools'],
        output:
            unmasked = temp(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.unmasked.bam')),
            merged = temp(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.merged.bam')),
            final = temp(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.final.bam')),
            sort = temp(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.final.sorted.bam')),
        shell:
            """
            set +e
            samtools view -S -b {input.converted} > {output.unmasked};
            samtools merge -f {output.merged} {input.unmapped} {output.unmasked};
            samtools view -h -o {output.final} {output.merged};

            samtools sort {output.final} > {output.sort};
            samtools index {output.sort}; 
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 0
            else
                exit 0
            fi
            """
    
    rule merge_unmapped_splits:
        """
        merge all sample unmapped reads into one file
        """
        input:
            sorted_list = dynamic(join(out_dir,'01_preprocess','04_unmapped','{sp}.{al}.split.{n}.final.sorted.bam')),
        params:
            rname='10e_merge',
        envmodules:
            config['samtools']  
        output:
            final = join(out_dir,'02_unmapped','{sp}.{al}.complete.bam'),
        shell:
            """
            samtools merge -f {output.final} {input.sorted_list} 
            """

rule split_mm_unique:
    """
    creates two text files from sam file
    1. unique file without NH:i
    2. MM file with NH:i

    read sam file header, print to header txt
    """
    input:
        f1 = get_align_input
    params:
        rname='11_split_mm_unique',
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir,'02_reads','{sp}.{al}.split.{n}.unique.txt'),
        o2 = join(out_dir,'02_reads','{sp}.{al}.split.{n}.mm.txt'),
        h = join(out_dir,'02_reads','{sp}.{al}.split.{n}.header.txt'),
    shell:
        """
        set +e
        samtools view {input.f1} | grep -v 'NH:i' > {output.o1}; \
        samtools view {input.f1} | grep 'NH:i' > {output.o2};
        samtools view -H {input.sam} > {output.h}
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 0
        else
            exit 0
        fi
        """

rule create_bam_mm_unique:
    """
    add nh:i:1 tag, cat header, create unique bam file
    """
    input:
        h = join(out_dir,'02_reads','{sp}.{al}.split.{n}.header.txt'),
        un = join(out_dir,'02_reads','{sp}.{al}.split.{n}.unique.txt'),
        mm = join(out_dir,'02_reads','{sp}.{al}.split.{n}.mm.txt'),
    params:
        rname='13_create_bam',
    envmodules:
        config['samtools']        
    output:
        bam_u = temp(join(out_dir,'01_preprocess','05_unique','{sp}.{al}.split.{n}.unique.bam')),
        bam_m = temp(join(out_dir,'01_preprocess','05_mm','{sp}.{al}.split.{n}.mm.bam')),
        sort_u = temp(join(out_dir,'01_preprocess','05_unique','{sp}.{al}.split.{n}.unique.si.bam')),
        sort_m = temp(join(out_dir,'01_preprocess','05_mm','{sp}.{al}.split.{n}.mm.si.bam')),
    shell:
        """
        if [ -w "/lscratch/${{SLURM_JOB_ID}}" ]; then
            tmpdir="/lscratch/${{SLURM_JOB_ID}}"
            mkdir "${{tmpdir}}/u";
            mkdir "${{tmpdir}}/m";
        else
            tmpdir=out_dir + "tmp/"

            if [ ! -d $tmpdir ]; then mkdir $tmpdir; fi
            if [ ! -d "${{tmpdir}}/u" ]; then mkdir "${{tmpdir}}/u";
            if [ ! -d "${{tmpdir}}/m" ]; then mkdir "${{tmpdir}}/m";
        fi

        #create unique bam
        awk -F '\t' -v OFS='\t' '{{ $(NF+1) = "NH:i:1"; print }}' {input.un} | cat {input.h} - | \
            samtools sort -T ${{tmpdir}} | samtools view -Sb > {output.bam_u};
        
        #create mm bam
        cat {input.h} {input.mm} | samtools sort -T ${{tmpdir}} | samtools view -Sb > {output.bam_m};
        
        #sort unique and mm bam
        samtools sort -T "${{tmpdir}}/u" {output.bam_u} -o {output.sort_u};
        samtools sort -T "${{tmpdir}}/m" {input.bam_m} -o {output.sort_m};
        samtools index {output.sort_u};
        samtools index {output.sort_m};
        """

rule merge_splits_unique_mm:
    """
    merge split unique bam files into one merged.unique bam file
    merge split multimapped bam files into one merged.mm bam file
    """
    input:
        unique = dynamic(join(out_dir,'01_preprocess','05_unique','{sp}.{al}.split.{n}.unique.si.bam')),
        mm = dynamic(join(out_dir,'01_preprocess','05_mm','{sp}.{al}.split.{n}.mm.si.bam'))
    params:
        rname='16_merge_splits_unique_mm',
    envmodules:
        config['samtools']  
    output:
        unique = join(out_dir,'03_bams','{sp}.{al}.merged.unique.bam'),
        mm = join(out_dir,'03_bams','{sp}.{al}.merged.mm.bam'),
    shell:
        """
        samtools merge -f {output.unique} {input.unique};
        samtools merge -f {output.mm} {input.mm}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files by sample
    """
    input:
        un = join(out_dir,'03_bams','{sp}.{al}.merged.unique.bam'),
        mm = join(out_dir,'03_bams','{sp}.{al}.merged.mm.bam'),
    params:
        rname='17_merge_mm_and_unique',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'03_bams','{sp}.{al}.merged.bam'),
    shell:
        """
        samtools merge -f {output.o1} {input.un} {input.mm}
        """

rule sort_index_merged:
    """
    sort merged.bam
    """
    input:
        f1 = join(out_dir,'03_bams','{sp}.{al}.merged.bam'),
    params:
        rname='18_sort_index_merged',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'03_bams','{sp}.{al}.merged.si.bam'),
        o2 = join(out_dir,'03_bams','{sp}.{al}.merged.si.bam.bai'),
    shell:
        """
        if [ -w "/lscratch/${{SLURM_JOB_ID}}" ];then tmpdir="/lscratch/${{SLURM_JOB_ID}}";else tmpdir="/dev/shm";fi
        samtools sort -T ${{tmpdir}} {input.f1} -o {output.o1};
        samtools index {output.o1}
        """

rule qc_samstats:
    """
    generate statistics for sam file before deduplication
    http://www.htslib.org/doc/samtools-stats.html
    > $1
    """
    input:
        f1 = join(out_dir,'03_bams','{sp}.{al}.merged.si.bam'),
    params:
        rname='19_qc_samstats'
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt')
    shell:
        """
        samtools view -h {input.f1} | samtools stats - > {output.o1}
        """

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir,'{mp}/00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f2 = expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        f3 = expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),sp=sp_list),
        f4 = expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),sp=sp_list),
        f5 = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'),sp=sp_list, al=align_list)
    params:
        rname = '20_multiqc',
        out = join(out_dir,'qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_pre = expand(join(out_dir,'{mp}','00_qc_pre'),mp = samp_dict.keys()),
        dir_post = expand(join(out_dir, 'qc', '00_qc_post')),
        dir_screen_species = expand(join(out_dir, 'qc', '00_qc_screen_species')),
        dir_screen_rrna = expand(join(out_dir, 'qc', '00_qc_screen_rrna')),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'qc/multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
            -d -dd 1 {params.dir_pre} {params.dir_post} \
            {params.dir_screen_rrna} {params.dir_screen_species} \
            -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir,'03_bams','{sp}.{al}.merged.si.bam'),
    params:
        rname = "21_qc_alignment",
        R = join(source_dir,'workflow','scripts','04_alignment_stats.R'),
        sampleid = '{sp}.{al}',
        base = join(out_dir, 'qc', '00_qc_post/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_align_len.txt'),
        bam_u = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unalign_len.txt'),
        png_align = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'),
        png_unalign = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'),
        txt_align = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'),
        txt_unalign = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'),
    shell:
        """
        samtools view -F 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_a}; \
        samtools view -f 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_u}; \
        Rscript {params.R} --sampleid {params.sampleid} --bam_aligned {output.bam_a} --bam_unaligned {output.bam_u} --output_dir {params.base}
        """

#pipeline splits to handle multiplexed QC vs non-multiplexed QC
if (multiplex_flag == 'Y'):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
            txt_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
            png_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys())
        params:
            rname = "22_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','05_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}", \
                    b_txt = "{input.txt_bc}"))'
            """
else:
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
        params:
            rname = "22_qc_troubleshoot",
            R = join(source_dir,'workflow','scripts','05_qc_report_nondemux.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}"))'
            """

rule dedup:
    """
    deduplicate 
    """
    input:
        f1 = join(out_dir,'03_bams','{sp}.{al}.merged.si.bam'),
    params:
        rname='23_dedup',
        umi = umi_parameter
    envmodules:
        config['umitools']  
    output:
        #o1 = temp(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam')),
        o1 = join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'),
        o2 = join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.log'),
    shell:
        """
        umi_tools dedup \
        -I {input.f1} \
        --method unique --multimapping-detection-method=NH --umi-separator={params.umi} \
        -S {output.o1} \
        --log2stderr -L {output.o2};
        """

rule sort_index_dedup:
    """
    sort dedup.bam file
    """
    input:
        f1 = expand(join(out_dir,'09_dedup','01_bam','{{sp}}.{al}.dedup.bam'), al=align_list),
    params:
        rname='25_sort_index_dedup',
    envmodules:
        config['samtools']  
    output:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        bai = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam.bai'),
    shell:
        """
        if [ -w "/lscratch/${{SLURM_JOB_ID}}" ];then tmpdir="/lscratch/${{SLURM_JOB_ID}}";else tmpdir="/dev/shm";fi
        samtools sort -T ${{tmpdir}} {input.f1} -o {output.bam};
        samtools index {output.bam}
        """

rule dedup_header:
    """
    cat bam header
    """
    input:
        f1 = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
    params:
        rname='26_dedup_head',
    envmodules:
        config['samtools']   
    output:
        o1 = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.header.txt'),
    shell:
        """
        samtools view -H {input.f1} > {output.o1}
        """

rule dedup_unique:
    """
    split dedup file into multimapped and unique files
    """
    input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        h = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.header.txt'),
    params:
        rname='27_dedup_unique',
    envmodules:
        config['samtools']  
    output:
        #o1 = temp(join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.bam')),
        o1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.bam'),
    shell:
        """
        set +e
        if [ -w "/lscratch/${{SLURM_JOB_ID}}" ];then tmpdir="/lscratch/${{SLURM_JOB_ID}}";else tmpdir="/dev/shm";fi
        samtools view {input.bam} | grep -w 'NH:i:1' | cat {input.h} - |  samtools sort -T ${{tmpdir}} | samtools view -Sb > {output.o1}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule index_dedup_unique:
    """
    index deduped unique file
    """
    input:
        f1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.bam'),
    params:
        rname='28_index_dedup_unique',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'),
    shell:
        """
        cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule create_beds:
    """
    create bed file from the deduped unique file and deduped all file
    """
    input:
        all = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        unique = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'),
    params:
        rname='29_create_beds',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'10_bed','{sp}_all.bed'),
        unique = join(out_dir,'10_bed','{sp}_unique.bed')
    shell:
        """
        bedtools bamtobed \
            -split -i {input.all} | bedtools sort -i - > {output.all}; 
        bedtools bamtobed \
            -split -i {input.unique} | bedtools sort -i - > {output.unique}; 
        """

rule create_safs: 
    """
    create SAF files from the deduped unique bed and all bed files
    """
    input:
        all = join(out_dir,'10_bed','{sp}_all.bed'),
        unique = join(out_dir,'10_bed','{sp}_unique.bed')
    params:
        rname='30_create_safs',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'11_SAF','{sp}_all.SAF'),
        unique = join(out_dir,'11_SAF','{sp}_unique.SAF')
    shell:
        """
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.all} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.all};
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.unique} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.unique}
        """

rule feature_counts_allreads:
    """
    Unique reads (fractional counts correctly count splice reads for each peak. 
    When peaks counts are combined for peaks connected by splicing in Rscript)
    """
    input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        saf = join(out_dir,'11_SAF','{sp}_all.SAF')
    params:
        rname='31a_feature_counts_allreads',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'12_counts','allreadpeaks','{sp}_uniqueCounts.txt'),
        out_all = join(out_dir,'12_counts','allreadpeaks','{sp}_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """   

rule feature_counts_uniquereads:
    """
    Include Multimap reads - MM reads given fractional count based on # of mapping 
    locations. All spliced reads also get fractional count. So Unique reads can get 
    fractional count when spliced peaks combined in R script the summed counts give 
    whole count for the unique alignement in combined peak.
    """
     input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        saf = join(out_dir,'11_SAF','{sp}_unique.SAF'),
    params:
        rname='31b_feature_counts_uniquereads',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'12_counts','uniquereadpeaks','{sp}_uniqueCounts.txt'),
        out_all = join(out_dir,'12_counts','uniquereadpeaks','{sp}_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """  

rule project_annotations:
    """
    generate annotation table once per project
    """
    input:
        join(out_dir, 'qc', 'manifest_clean.txt'),
    params:
        rname='32_project_annotations',
        script = join(source_dir,'workflow','scripts','08_annotation.R'),
        ref_sp = nova_ref,
        rrna_flag = refseq_rrna,
        a_path = alias_path,
        g_path = gen_path,
        rs_path = rseq_path,
        c_path = can_path,
        i_path = intron_path,
        r_path = rmsk_path,
        custom_path = add_anno_path,
        base = join(out_dir,'13_annotation', '01_project/',),
        a_config = annotation_config,
    envmodules:
        config['R']
    output:
        anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        nc_anno = join(out_dir,'13_annotation', '01_project','ncRNA_annotations.txt'),
        ref_gencode = join(out_dir,'13_annotation', '01_project','ref_gencode.txt'),
    shell:
        """
        Rscript {params.script} \
            --ref_species {params.ref_sp} \
            --refseq_rRNA {params.rrna_flag} \
            --alias_path  {params.a_path} \
            --gencode_path {params.g_path} \
            --refseq_path {params.rs_path} \
            --canonical_path {params.c_path} \
            --intron_path {params.i_path} \
            --rmsk_path {params.r_path} \
            --custom_path {params.custom_path} \
            --out_dir {params.base} \
            --reftable_path {params.a_config}
        """

#pipeline splits to include manorm outputs with annotation outputs, if required
if(DE_method=="MANORM"):
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_uniqueCounts.txt'),
            all = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_allFracMMCounts.txt'),
            anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        params:
            rname = '33_peak_annotations',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = nova_ref,
            base1 = join(out_dir,'13_annotation','02_peaks/',),
            base2 = join(out_dir,'14_MAnorm','01_input/',),
            anno_dir = join(out_dir,'13_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
            error = join(out_dir,'13_annotation','read_depth_error.txt')
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),
            o2 = join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
            o3 = join(out_dir,'14_MAnorm', '01_input','{sp}_PeaksforMAnrom.bed'),
            o4 = join(out_dir,'14_MAnorm', '01_input','{sp}_PeaksforMAnrom_P.bed'),
            o5 = join(out_dir,'14_MAnorm', '01_input','{sp}_PeaksforMAnrom_N.bed'),
        shell:
            '''
            Rscript {params.script} \
                --peak_type {params.p_type} \
                --peak_unique {input.unique} \
                --peak_all {input.all} \
                --join_junction {params.junc} \
                --condense_exon {params.c_exon} \
                --read_depth {params.r_depth} \
                --demethod {params.d_method} \
                --sample_id {params.sp} \
                --ref_species {params.ref_sp} \
                --anno_dir {params.anno_dir} \
                --reftable_path {params.a_config} \
                --gencode_path {params.g_path} \
                --intron_path {params.i_path} \
                --rmsk_path {params.r_path} \
                --out_dir {params.base1} \
                --out_dir_manorm {params.base2} \
                --output_file_error {params.error}
            '''
else:
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_uniqueCounts.txt'),
            all = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_allFracMMCounts.txt'),
            anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        params:
            rname = '33_peak_annotations',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            ref_sp = nova_ref,
            base = join(out_dir,'13_annotation','02_peaks/',),
            anno_dir = join(out_dir,'13_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_mapq_IN.txt'),
            o2 = join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
        shell:
            '''
            Rscript {params.script} \
                --peak_type {params.p_type} \
                --peak_unique {input.unique} \
                --peak_all {input.all} \
                --join_junction {params.junc} \
                --condense_exon {params.c_exon} \
                --read_depth {params.r_depth} \
                --demethod {params.d_method} \
                --sample_id {params.sp} \
                --ref_species {params.ref_sp} \
                --anno_dir {params.anno_dir} \
                --reftable_path {params.a_config} \
                --gencode_path {params.g_path} \
                --intron_path {params.i_path} \
                --rmsk_path {params.r_path} \
                --out_dir {params.base}
            '''

rule annotation_output:
    """
    generates an HTML report for peak annotations
    """
    input:
        peak_in = join(out_dir,'13_annotation', '02_peaks','{sp}_peakannotation_complete.txt'),
    params:
        rname = "34_annotation_output",
        R = join(source_dir,'workflow','scripts','10_annotation.Rmd'),
        sp = "{sp}",
        ref_sp = nova_ref,
        r_depth= min_count,
        c_exon = cond_exon,
        junc = sp_junc,
        p_type = peak_id,
        rrna = refseq_rrna,
    envmodules:
        config['R']
    output:
        o1 = join(out_dir,'13_annotation', '{sp}_annotation_final_report.html'),
        o2 = join(out_dir,'13_annotation', '{sp}_annotation_final_table.txt'),
    shell:
        """
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(samplename = "{params.sp}", \
                peak_in = "{input.peak_in}", \
                output_table = "{output.o2}", \
                readdepth = "{params.r_depth}", \
                PeakIdnt = "{params.p_type}"))'
        """

#pipeline will continue through manorm processing, if required
if(DE_method=="MANORM"):
    rule MANORM_beds:
        input:
            f1 = join(out_dir,'10_bed','{sp}_all.bed'),
        params:
            rname = "35a_MANORM_beds"
        output:
            o1 = join(out_dir,'14_MAnorm', '01_input','{sp}_ReadsforMAnrom_P.bed'),
            o2 = join(out_dir,'14_MAnorm', '01_input','{sp}_ReadsforMAnrom_N.bed'),
        shell:
            """
            awk '$6=="+"' {input.f1} > {output.o1}
            awk '$6=="-"' {input.f1} > {output.o2}
            """

    rule MANORM_analysis:
        '''
        '''
        input:
            get_MANORM_analysis_input
        params:
            rname = "35b_MANORM_analysis",
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
            base = join(out_dir,'14_MAnorm','02_analysis', '{group_id}_{strand}/'),
        envmodules:
            config['manorm']
        output:
            o1 = join(out_dir,'14_MAnorm','02_analysis', '{group_id}_{strand}','{group_id}_all_MAvalues.xls')
        shell:
            """
            manorm \
            --p1 "{out_dir}14_MAnorm/01_input/{params.gid_1}_PeaksforMAnrom_{wildcards.strand}.bed" \
            --p2 "{out_dir}14_MAnorm/01_input/{params.gid_2}_PeaksforMAnrom_{wildcards.strand}.bed" \
            --r1 "{out_dir}14_MAnorm/01_input/{params.gid_1}_ReadsforMAnrom_{wildcards.strand}.bed" \
            --r2 "{out_dir}14_MAnorm/01_input/{params.gid_2}_ReadsforMAnrom_{wildcards.strand}.bed" \
            --s1 0 \
            --s2 0 \
            -p 1 \
            -d 25 \
            -n 10000 \
            -s \
            -o {params.base} \
            --name1 {params.gid_1} \
            --name2 {params.gid_2}
            """
            
    rule MANORM_processing:
        '''
        '''
        input:
            get_MANORM_processing_input
        params:
            rname = "35c_MANORM_processing",
            script = join(source_dir,'workflow','scripts','11_MAnormProcess.R'),
            anno = join(out_dir,'13_annotation','02_peaks'),
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'14_MAnorm','02_analysis', '{group_id}','{group_id}_post_processing.txt')
        shell:
            """
            Rscript {params.script} \
                --samplename {params.gid_1} \
                --background {params.gid_2} \
                --peak_anno_g1 "{out_dir}13_annotation/02_peaks/{params.gid_1}_peakannotation_complete.txt" \
                --peak_anno_g2 "{out_dir}13_annotation/02_peaks/{params.gid_2}_peakannotation_complete.txt" \
                --pos_manorm "{out_dir}14_MAnorm/02_analysis/{wildcards.group_id}_P/{params.gid_1}_MAvalues.xls" \
                --neg_manorm "{out_dir}14_MAnorm/02_analysis/{wildcards.group_id}_N/{params.gid_1}_MAvalues.xls" \
                --output_file {output.o1}
            """

    rule MANORM_RMD:
        input:
            f1 = join(out_dir,'14_MAnorm','02_analysis', '{group_id}','{group_id}_MAnormPeaks.txt')
        params:
            rname = "35d_MANORM_RMD",
            R = join(source_dir,'workflow','scripts','12_MAnormAnnotation.Rmd'),
            gid_1 = get_MANORM_gid1,
            gid_2 = get_MANORM_gid2,
            p_id = peak_id
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'14_MAnorm','03_report','{group_id}_manorm_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(peak_in="{input.f1}", \
                    samplename="{params.gid_1}", \
                    background="{params.gid_2}", \
                    PeakIdnt="{params.p_id}"))'
            """
