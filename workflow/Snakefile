'''
Overview

- Multiplexed samples are split based on provided barcodes and named using provide manifests
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged

'''

report: "report/workflow.rst"

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml

'''
* Requirements *
- Four input files are required:
    1) cluster_config.yml
    2) snakemake_config.yaml
    3) multiplex.tsv
    4) samples.tsv
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

#snakemake config
source_dir = config['source_dir']
cont_dir = config['container_dir']
out_dir = config['output_dir'].rstrip('/') + '/'
fastq_dir = config['fastq_dir'].rstrip('/') + '/'

sample_manifest = config['sample_manifest']
multiplex_manifest = config['multiplex_manifest']

nova_ref = config['novoalign_reference']
splice_aware = config['splice_aware'].capitalize()
splice_bp = config['splice_bp_length']

multiplex_flag = config['multiplex_flag'].capitalize()
mismatch = config['mismatch_allowance']
split_value =  config['split_value']
min_count = config['minimum_count']
nt_merge = str(config['nt_merge']) + 'nt'
peak_id = config['peak_id'].upper()
DE_method = config['DE_method'].upper()
sp_junc = config['splice_junction'].upper()
sy_flag = config['SY_flag'].upper()

#split_params
def check_split(split_in):
    if (type(split_in) == str):
        split_replace = split_in.replace(',', '')
        if (type(split_in) == str):
            split_replace = 1000000
    elif (split_in < 3000):
        split_replace = 1000000
    elif (type(split_in) == float):
        split_replace = int(split_in)
    else:
        split_replace = split_in
    return split_replace
split_value = check_split(split_value)

#index selection
index_manifest = join(source_dir, 'config', 'index_config.yaml')
with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)
if (splice_aware=='N'):
    nova_index = index_list[nova_ref]['std']
else:
    nova_index = index_list[nova_ref]['spliceaware'][str(splice_bp)+'bp']

#reference selection
sy_path = "EMPTY"
if (nova_ref == "mm10"):
    genome_ref = "GENCODE mm10 v23"
    if(sy_flag == "Y"):
        sy_path = index_list[nova_ref]['sy_path']
elif (nova_ref=="hg38"):
    genome_ref = "GENCODE hg38 v32"
    sy_flag = "N"
gen_path = index_list[nova_ref]['gencode_path']
rseq_path = index_list[nova_ref]['refseq_path']
can_path = index_list[nova_ref]['canonical_path']
intron_path = index_list[nova_ref]['intron_path']
rmsk_path = index_list[nova_ref]['rmsk_path']
alias_path = index_list[nova_ref]['alias_path']

#annotation config
if (nova_ref == "mm10"):
    annotation_config = join(source_dir,'config','annotation_config_mm10.txt')
elif (nova_ref == "hg38"):
    annotation_config = join(source_dir,'config','annotation_config_hg38.txt')

#junctions
if (sp_junc=="Y"):
    sp_junc = "TRUE"
else:
    sp_junc = "FALSE"

#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = fastq_dir + multiplex_dict[wildcards.mp]
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + wildcards.mp + '/'

    return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + wildcards.mp + '/demux_' + bc + '.fastq.gz' + ' ' + out_dir + wildcards.mp + '/01_renamed/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)

#create nondemux command line
def nondemux_cmd(wildcards):
  cmd_line = ''

  for k,v in multiplex_dict.items():
    for k2,v2 in samp_dict[k].items():
      cmd_line = 'cp ' + fastq_dir + v + ' ' + out_dir + k + '/01_renamed/' + k2 + '.fastq.gz; ' + cmd_line
  
  return(cmd_line)

#command needed to cut adaptors from demux samples
def adapt_cmd(wildcards):
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp) & (df_samples['sample']==wildcards.sp)]

    base = out_dir + wildcards.mp + '/01_remove_adaptor/' + wildcards.sp
    fq_un =  base + '_untrimmed.fastq.gz '
    fq_t = base + '_trimmed.fastq.gz '

    base = out_dir + '/' + wildcards.mp + '/01_renamed/' + wildcards.sp
    fq_o = base + '.fastq.gz '

    command_line = '--untrimmed_output ' + fq_un + '--reads_trimmed ' + fq_t + fq_o + sub_df.iloc[0]['adaptor']

    return(command_line)

def rename_adapt_cmd(wildcards):
    cmd = ''

    for sp in sp_list:
        df_sub = df_samples[(df_samples['sample']==sp)] 
        
        #input
        f1 = join(out_dir,df_sub.iloc[0]["multiplex"],'01_remove_adaptor',sp + '_trimmed.fastq.gz')
        f2 = join(out_dir,df_sub.iloc[0]["multiplex"],'01_remove_adaptor',sp + '_untrimmed.fastq.gz')

        #output
        o1 = join(out_dir,'01_remove_adaptor',sp + '_removed_seq.fastq.gz')
        o2 = join(out_dir,'01_remove_adaptor',sp + '_filtered.fastq.gz')

        cmd = 'mv ' + f1 + ' ' + o1 + '; mv ' + f2 + ' ' + o2 + '; ' + cmd
    return(cmd)
    
#create cmd to rename fastq screen outputs
def rename_fqscreen1(wildcards):
    n_in = join(out_dir, 'qc', '00_qc_screen_species',wildcards.sp + '_screen')
    n_out = join(out_dir, 'qc', '00_qc_screen_species',wildcards.sp + '_filtered')
    cmd = 'mv ' + n_in + '.txt ' + n_out + '.txt; mv ' + n_in + '.png ' + n_out + '.png; mv '+ n_in + '.html ' + n_out + '.html'
    return(cmd)

def rename_fqscreen2(wildcards):
    n_in = join(out_dir, 'qc', '00_qc_screen_rrna',wildcards.sp + '_screen')
    n_out = join(out_dir, 'qc', '00_qc_screen_rrna',wildcards.sp + '_filtered')
    cmd = 'mv ' + n_in + '.txt ' + n_out + '.txt; mv ' + n_in + '.png ' + n_out + '.png; mv '+ n_in + '.html ' + n_out + '.html'
    return(cmd)

#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=join(out_dir,'02_unzip',wildcards.sp + '.fastq ')

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'qc','split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]

        #split can only create a max of 99 split files; handle large file sets to ensure max reads per 99 files
        if (sub_df.iloc[0]['filenum']>98):
            chunk_size = int(((sub_df.iloc[0]['chunksize'] * sub_df.iloc[0]['filenum']) / 99)+.9)
        else:
            chunk_size = sub_df.iloc[0]['chunksize']

    except:
        return(0)
    return(chunk_size)

#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

#set rule_all inputs depending on if samples have been multiplexed
if multiplex_flag == 'Y':
    input_all = [expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.png'), mp=samp_dict.keys()),expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.txt'),mp=samp_dict.keys())]
else:
    input_all = [expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list)]

rule all:
    input:
        #check manifest
        join(out_dir,'qc','manifest_clean.txt'),

        # #check input fastq files
        # expand(join(fastq_dir,'{fq_file}'), fq_file=file_list), #multiplexed sample fastq files exist
        
        # #different inputs for multiplexed or nonmultiplexed pipeline
        # input_all,
        
        #check barcodes
        #expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.png'), mp=samp_dict.keys()),
        #expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.txt'),mp=samp_dict.keys()), 
        
        #demultiplex samples, rename samples
        #expand(join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),mp=samp_dict.keys()), #demultiplex
        #expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list), #rename
        
        #Remove adaptors, rename samples
        #expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        #expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        # expand(join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz'),sp=sp_list),
        # expand(join(out_dir,'01_remove_adaptor','{sp}_removed_seq.fastq.gz'),sp=sp_list),

        # #FastQC
        # expand(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list,sp=sp_list),
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        
        # #Unzip samples
        # expand(join(out_dir,'02_unzip','{sp}.fastq'), sp=sp_list),
         
        # #FastQ Screen
        # expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),sp=sp_list),
        # expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),sp=sp_list),
        
        # #Determine file split parameters
        # join(out_dir,'qc','split_params.tsv'),
        
        # #Merge splits
        # expand(join(out_dir,'07_bam_merged_splits/{sp}.merged.unique.bam'), sp=sp_list),
        # expand(join(out_dir,'07_bam_merged_splits/{sp}.merged.mm.bam'),sp=sp_list),

        # #Merge unique and mm
        # expand(join(out_dir,'08_bam_merged','{sp}.merged.bam'), sp=sp_list),

        # #Sort and index merged
        # expand(join(out_dir,'08_bam_merged','{sp}.merged.si.bam'),sp=sp_list),
        # expand(join(out_dir,'08_bam_merged','{sp}.merged.si.bam.bai'),sp=sp_list),

        # #Samstats
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}_samstats.txt'), sp=sp_list),

        # #MultiQC Report
        # join(out_dir,'qc/multiqc_report.html'),

        # #Alignment stats
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}_aligned.png'), sp=sp_list),
        # expand(join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.png'), sp=sp_list),

        # #QC Troubleshooting
        # join(out_dir,'qc','qc_report.html'),

        # #Deduplicate
        # expand(join(out_dir,'09_dedup_bam/{sp}.dedup.si.bam'), sp=sp_list),
        # expand(join(out_dir,'10_dedup_split/{sp}.dedup.unique.i.bam'), sp=sp_list),

        # #Bed files
        # expand(join(out_dir,'11_bed','{sp}_all.bed'), sp=sp_list),
        # expand(join(out_dir,'11_bed','{sp}_unique.bed'), sp=sp_list),

        # #SAF 
        # expand(join(out_dir,'12_SAF/{sp}_'+ str(nt_merge) +'_all.SAF'), sp=sp_list),
        # expand(join(out_dir,'12_SAF/{sp}_'+ str(nt_merge) +'_unique.SAF'), sp=sp_list),

        #Count features
        expand(join(out_dir,'13_counts','uniquereadpeaks','{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'), sp=sp_list),
        expand(join(out_dir,'13_counts','uniquereadpeaks','{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'), sp=sp_list),
        expand(join(out_dir,'13_counts','allreadpeaks','{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'), sp=sp_list),
        expand(join(out_dir,'13_counts','allreadpeaks','{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'), sp=sp_list),


        #In progress
        expand(join(out_dir,'14_peaks/{sp}_' + str(nt_merge) + '_peakjunction.txt'),sp=sp_list),
        join(out_dir,'15_annotation','annotations.txt'),

        # #Annotation
        # #expand(join(out_dir,'14_annotation','{sp}_'+ str(nt_merge) +'_MD15.txt'),zip, mp=mp_list, sp=sp_list),
        #expand(join(out_dir,'14_annotation','{sp}_'+ str(nt_merge) +'_MD15.html'),zip, mp=mp_list, sp=sp_list),

include: "rules/common.smk"
include: "rules/other.smk"

rule check_manifest:
    """
    Read in multiplex manifest and sample manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    If files are correct, outputs a temp file. If not, outputs error file.
    """
    input:
        f1 = multiplex_manifest,
        f2 = sample_manifest
    params:
        rname='ic_manif',
        py = join(source_dir,'workflow','scripts','01_check_manifest.py'),
        base = join(out_dir,'qc','manifest_')
    envmodules:
        config['python']  
    output:
        o1 = join(out_dir, 'qc', 'manifest_clean.txt'),
    shell:
        """
        python {params.py} \
            '{params.base}' {input.f1} {input.f2}
        """

if (multiplex_flag == 'Y'):
    rule qc_barcode:
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname="ic_bcQC",
            py = join(source_dir,'workflow', 'scripts', '02_barcode_qc.py'),
            mm = mismatch,
        envmodules:
            config['python'],
            #config['Qt']
        output:
            o1 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys()),
            o2 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys())
        shell:
            """
            python {params.py} \
                {sample_manifest} {multiplex_manifest} {fastq_dir} {out_dir} {params.mm}
            """

    rule demultiplex:
        """
        https://icount.readthedocs.io/en/latest/ref_CLI.html
        Reads in fastq files from multiplex_manifest.
        Finds multiplex match between manifest files, splits files based on list of
        barcodeIDs found in sample_manifest

        For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN

        file_name                   multiplex
        SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1

        multiplex       sample          group       barcode     adaptor
        SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
        SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
        """
        input:
            f1 = get_fq_names,
            qc = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys())
        params:
            rname='ic_demux',
            doc = join(cont_dir,'icount.sif'),
            cmd = demux_cmd,
            ml = 15,
            mm = mismatch
        envmodules:
            config['singularity'],
        output:
            o1 = join(out_dir,'{mp}','demux_nomatch5.fastq.gz'),
        shell:
            """
            singularity exec -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
            {params.doc} iCount demultiplex --minimum_length {params.ml} --mismatches {params.mm} {params.cmd}
            """

    rule rename_demux:
        """
        renames demultiplexed files based on
        from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
        to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
        """
        input:
            join(out_dir,'{mp}','demux_nomatch5.fastq.gz'),
        params:
            rname = 'ic_rename',
            cmd = rename_cmd
        output:
            o1 = join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz')
        shell:
            'cp {params.cmd}'
else: 
    rule copy_nondemux:
        """
        copies fastq files from source location and moves into renamed file folder
        """
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname = 'ic_nondemux',
            cmd = nondemux_cmd
        output:
            o1 = expand(join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list)
        shell:
            '{params.cmd}'

rule remove_adaptors:
    """
    removes adaptors from files. software outputs:
    1) untrimmed.fastq.gz - the remaining sequences without the adaptor and 
    2) trimmed.fastq.gz - the sequences that were trimmed from the input file
    """
    input:
        f1 = join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz')
    params:
        rname='ic_adapt',
        ml = 1,
        cmd = adapt_cmd,
        doc = join(cont_dir,'icount.sif'),
    envmodules:
        config['singularity'],    
    output:
        o1 = join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),
        o2 = join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz')
    shell:
        """
        singularity exec -B /data/$USER,/data/RBL_NCI,{out_dir},/fdb,/scratch \
        {params.doc} iCount cutadapt -ml {params.ml} {params.cmd}
        """

rule rename_adaptors:
    """
    naming schema of adaptors is confusing, and makes interpretation of QC report difficult
    
    files are renamed for clarity, renamed files moved into common dir
    1) untrimmed.fastq.gz --> filtered.fastq.gz
    2) trimmed.fastq.gz --> removed_seq.fastq.gz
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        f2 = expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list)
    params:
        rname = "ic_rename_ad",
        cmd = rename_adapt_cmd
    output:
        o1 = expand(join(out_dir,'01_remove_adaptor','{sp}_removed_seq.fastq.gz'),sp=sp_list),
        o2 = expand(join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz'),sp=sp_list)
    shell:
        '''
        {params.cmd}
        '''

rule qc_fastq_pre:
    """
    Runs FastQC report on each sample before adaptors have been removed
    """
    input:
        f1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    params:
        rname='ic_fqc_pre',
        base = join(out_dir,'{mp}/00_qc_pre/')
    envmodules:
        config['fastqc']
    output:
        o1 = join(out_dir,'{mp}/00_qc_pre/{sp}_fastqc.html')
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_fastq_post:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        f1 = join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz')
    params:
        rname='ic_fqc_post',
        base = join(out_dir, 'qc', '00_qc_post'),
    envmodules:
        config['fastqc']
    output:
        o1 = join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html')
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule gunzip_files:
    """
    unzips untrimmed files - these are what is leftover after adaptor trimming
    """
    input:
        f1 = join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz')
    params:
        rname='ic_gunzip',
    output:
        o1 = join(out_dir,'02_unzip','{sp}.fastq')
    shell:
        "gunzip -c {input.f1} > {output.o1}"
        
rule qc_screen_sp:
    """
    this will align to human, mouse, bacteria
    must run fastq_screen as two separate rules - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'02_unzip','{sp}.fastq')
    params:
        rname='ic_screen1',
        base = join(out_dir, 'qc', '00_qc_screen_species'),
        fq_config = join(source_dir,'config','fqscreen_species_config.conf'),
        cmd = rename_fqscreen1
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.base} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}; {params.cmd}
        '''

rule qc_screen_rrna:
    """
    this will align to rRNA
    must run fastq_screen as two separate rules - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'02_unzip','{sp}.fastq')
    params:
        rname='ic_screen2',
        base = join(out_dir, 'qc', '00_qc_screen_rrna'),
        fq_config = join(source_dir,'config','fqscreen_rrna_config.conf'),
        cmd = rename_fqscreen2
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.base} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}; {params.cmd}
        '''

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to split FASTQ file
    split is necessary to improve speed of alignment
    split is done based off split_value input, maintaining every 4 lines = 1 seq
    """
    input:
        f1 = expand(join(out_dir,'02_unzip/{sp}.fastq'), sp=sp_list),
    params:
        rname='ic_splitdoc',
        sh = join(source_dir, 'workflow', 'scripts', '03_find_split_parameters.sh'),
    output:
        o1 = join(out_dir,'qc','split_params.tsv')
    shell:
        """
        sh {params.sh} {output.o1} {split_value} {input.f1}
        """

rule split_files:
    """
    performs split of fastq file into smaller files using parameters in split_params.tsv
    """
    input:
        all = expand(join(out_dir,'02_unzip/{sp}.fastq'), sp=sp_list),
        f1 = join(out_dir,'02_unzip/{sp}.fastq'),
        sp = join(out_dir,'qc','split_params.tsv')
    params:
        rname='ic_split',
        base = join(out_dir,'03_split/{sp}.split.'),
        cmd = get_filechunk_size
    output:
        o1 = dynamic(join(out_dir,'03_split/{sp}.split.{n}.fastq'))
    shell:
        """
        split --additional-suffix .fastq -l {params.cmd} --numeric-suffixes=10 {input.f1} {params.base}
        """

if (splice_aware == "N"):
    rule novoalign:
        """
        aligns files to index given; output is a sam file
        """
        input:
            f1 = join(out_dir,'03_split','{sp}.split.{n}.fastq'),
        params:
            rname='ic_novo',
            n_index = nova_index,
        envmodules:
            config['novocraft']    
        output:
            o1 = join(out_dir,'04_sam','{sp}.split.{n}.sam'),
        shell:
            """
            set +e
            novoalign -d {params.n_index} -k \
            -f {input.f1} -F STDFQ -c 32 \
            -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
            -R 0 -r All 999 > {output.o1}
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """
else:
    rule novoalign_splice:
        """
        aligns files to index given; output is a sam file
        functionally equiv to rule novoalign, expect for output file name
        """
        input:
            f1 = join(out_dir,'03_split','{sp_st}.split.{n}.fastq'),
        params:
            rname='ic_novo',
            n_index = nova_index,
        envmodules:
            config['novocraft']   
        output:
            o1 = join(out_dir,'04_sam_splice','{sp_st}.split.{n}.splice.sam'),
        shell:
            """
            set +e
            novoalign -d {params.n_index} \
            -f {input.f1} -F STDFQ -c 56 \
            -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
            -R 0 -r All 999 > {output.o1}
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """

    rule sam_cleanup:
        """
        performs cleanup for inserts
        """
        input:
            f1 = join(out_dir,'04_sam_splice','{sp_st}.split.{n}.splice.sam'),
        params:
            rname = 'ic_sam_cleanup'
        envmodules:
            config['samtools']  
        output:
            g1 = join(out_dir,'04_sam_splice','{sp_st}.split.{n}.tmp.sam'),
            final = join(out_dir,'04_sam_splice','{sp_st}.split.{n}.splice.final.sam'),
        shell:
            """
            samtools view {input.f1} | awk '{{ if (($4 == 1 && $6!~/^[0-9]I/ && $1~/:/ )||($4 > 1 && $1~/:/ )) {{ print }} }}' > {output.g1};
            samtools view -H {input.f1} | cat - {output.g1} > {output.final}
            """

    rule convert_transcript:
        """
        converts transcriptome coordinates to genomic coordinates
        uses USeq version 8.9.6
        """
        input:
            f1 = join(out_dir,'04_sam_splice','{sp_st}.split.{n}.splice.final.sam'),
        params:
            rname = 'ic_convert',
            base = join(out_dir,'04_sam_genomic','{sp_st}.split.{n}.sam'),
            doc = join(cont_dir,'USeq_8.9.6','Apps/SamTranscriptomeParser')
        envmodules:
            config['java']  
        output:
            o1 = join(out_dir,'04_sam_genomic','{sp_st}.split.{n}.sam.gz'),
        shell:
            """
            java -Djava.io.tmpdir={params.base} -jar -Xmx50G -jar {params.doc} \
            -f {input.f1} \
            -a 50000 -n 25 -u -s {params.base}
            """

    rule gunzip_sam:
        """
        unzips sam file
        """
        input:
            f1 = join(out_dir,'04_sam_genomic','{sp_st}.split.{n}.sam.gz'),
        params:
            rname = 'ic_gzipsam'
        output:
            o1 = join(out_dir,'04_sam','{sp_st}.split.{n}.sam'),
        shell:
            """
            gunzip -c {input.f1} > {output.o1}
            """

    rule novosort:
        """
        Q: where does this belong? this step wasn't included in the original pipeline and creates a new BAM file

        samtools view -uS bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.sam 2> bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.convert.err
        | novosort -  > bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.bam
        """
        input:
            f1 = join(out_dir,'04_sam','{sp_st}.split.{n}.sam'),
        params:
            rname = 'ic_novosort'
        envmodules:
            config['samtools']      
        output:
            o1 = join(out_dir,'05_bam','{sp_st}.split.{n}.bam'),
        shell:
            """
            samtools view -uS {input.f1} 2> bam/splitNovoAlign/WT/WT_Clip_iCountcutadpt.split.dg.all.convert.err
            | novosort -  > {output.o1}
            """

rule split_mm_unique:
    """
    creates two text files from sam file
    1. unique file without NH:i
    2. MM file with NH:i
    """
    input:
        f1 = join(out_dir,'04_sam','{sp_st}.split.{n}.sam'),
    params:
        rname='ic_split_mu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'05_reads','{sp_st}.split.{n}.unique.txt'),
        o2 = join(out_dir,'05_reads','{sp_st}.split.{n}.mm.txt'),
    shell:
        """
        set +e
        samtools view {input.f1} | grep -v 'NH:i' > {output.o1}; \
        samtools view {input.f1} | grep 'NH:i' > {output.o2};
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule sam_to_header:
    """
    read sam file header, print to header txt
    """
    input:
        sam = join(out_dir,'04_sam','{sp_st}.split.{n}.sam'),
    params:
        rname='ic_sam_head',
    envmodules:
        config['samtools'] 
    output:
        h = join(out_dir,'05_reads','{sp_st}.split.{n}.header.txt'),
    shell:
        """
        samtools view -H {input.sam} > {output.h};
        """

rule create_bam_unique:
    """
    add nh:i:1 tag, cat header, create unique bam file
    """
    input:
        h = join(out_dir,'05_reads','{sp_st}.split.{n}.header.txt'),
        un = join(out_dir,'05_reads','{sp_st}.split.{n}.unique.txt'),
    params:
        rname='ic_bam_u',
    envmodules:
        config['samtools']        
    output:
        o1 = join(out_dir,'06_bam_unique','{sp_st}.split.{n}.unique.bam'),
    shell:
        """
        awk -F '\t' -v OFS='\t' '{{ $(NF+1) = "NH:i:1"; print }}' {input.un} | cat {input.h} - | \
        samtools sort | samtools view -Sb > {output.o1};
        """

rule create_bam_mm:
    """
    cat header, create multimapped bam file
    """
    input:
        h = join(out_dir,'05_reads','{sp_st}.split.{n}.header.txt'),
        mm = join(out_dir,'05_reads','{sp_st}.split.{n}.mm.txt'),
    params:
        rname='ic_bam_m',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'06_bam_mm','{sp_st}.split.{n}.mm.bam'),
    shell:
        """
        cat {input.h} {input.mm} | samtools sort | samtools view -Sb > {output.o1}
        """

rule sort_mm_and_unique:
    """
    sort both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'06_bam_unique','{sp_st}.split.{n}.unique.bam'),
        mm = join(out_dir,'06_bam_mm','{sp_st}.split.{n}.mm.bam'),
    params:
        rname='ic_sort_mu',
    envmodules:
        config['samtools']      
    output:
        o1 = join(out_dir,'06_bam_unique','{sp_st}.split.{n}.unique.s.bam'),
        o2 = join(out_dir,'06_bam_mm','{sp_st}.split.{n}.mm.s.bam'),
    shell:
        """
        samtools sort {input.un} -o {output.o1}; \
        samtools sort {input.mm} -o {output.o2};
        """

rule index_mm_and_unique:
    """
    index both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'06_bam_unique','{sp_st}.split.{n}.unique.s.bam'),
        mm = join(out_dir,'06_bam_mm','{sp_st}.split.{n}.mm.s.bam'),
    params:
        rname='ic_index_mu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'06_bam_unique','{sp_st}.split.{n}.unique.i.bam'),
        o2 = join(out_dir,'06_bam_mm','{sp_st}.split.{n}.mm.i.bam'),
    shell:
        """
        cp {input.un} {output.o1}; samtools index {output.o1}; \
        cp {input.mm} {output.o2}; samtools index {output.o2};
        """

rule merge_splits_unique:
    """
    merge split unique bam files into one merged.unique bam file
    """
    input:
        f1 = dynamic(join(out_dir,'06_bam_unique','{sp}.split.{n}.unique.i.bam'))
    params:
        rname='ic_merge_u',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'07_bam_merged_splits','{sp}.merged.unique.bam'),
    shell:
        """
        samtools merge -f {output.o1} {input.f1}
        """

rule merge_splits_mm:
    """
    merge split multimapped bam files into one merged.mm bam file
    """
    input:
        f1 = dynamic(join(out_dir,'06_bam_mm','{sp}.split.{n}.mm.i.bam'))
    params:
        rname='ic_merge_m',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'07_bam_merged_splits','{sp}.merged.mm.bam'),
    shell:
        """
        samtools merge -f {output.o1} {input.f1}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files by sample
    """
    input:
        un = join(out_dir,'07_bam_merged_splits','{sp}.merged.unique.bam'),
        mm = join(out_dir,'07_bam_merged_splits','{sp}.merged.mm.bam'),
    params:
        rname='ic_merge_mu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'08_bam_merged/{sp}.merged.bam'),
    shell:
        """
        samtools merge -f {output.o1} {input.un} {input.mm}
        """

rule sort_index_merged:
    """
    sort merged.bam
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.merged.bam'),
    params:
        rname='ic_si_m',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'08_bam_merged','{sp}.merged.si.bam'),
        o2 = join(out_dir,'08_bam_merged','{sp}.merged.si.bam.bai'),
    shell:
        """
        samtools sort {input.f1} -o {output.o1};
        samtools index {output.o1}
        """

rule qc_samstats:
    """
    generate statistics for sam file before deduplication
    http://www.htslib.org/doc/samtools-stats.html
    > $1
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.merged.si.bam'),
    params:
        rname='ic_samstats'
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir, 'qc', '00_qc_post','{sp}_samstats.txt')
    shell:
        """
        samtools view -h {input.f1} | samtools stats - > {output.o1}
        """

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir,'{mp}/00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f2 = expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        f3 = expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),sp=sp_list),
        f4 = expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),sp=sp_list),
        f5 = expand(join(out_dir, 'qc', '00_qc_post','{sp}_samstats.txt'),sp=sp_list)
    params:
        rname = 'ic_multiqc',
        out = join(out_dir,'qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_pre = expand(join(out_dir,'{mp}','00_qc_pre'),mp = samp_dict.keys()),
        dir_post = expand(join(out_dir, 'qc', '00_qc_post')),
        dir_screen_species = expand(join(out_dir, 'qc', '00_qc_screen_species')),
        dir_screen_rrna = expand(join(out_dir, 'qc', '00_qc_screen_rrna')),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'qc/multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
            -d -dd 1 {params.dir_pre} {params.dir_post} \
            {params.dir_screen_rrna} {params.dir_screen_species} \
            -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.merged.si.bam'),
    params:
        rname = "ic_qc_align",
        R = join(source_dir,'workflow','scripts','04_alignment_stats.R'),
        base = join(out_dir, 'qc', '00_qc_post/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = join(out_dir, 'qc', '00_qc_post','{sp}_align_len.txt'),
        bam_u = join(out_dir, 'qc', '00_qc_post','{sp}_unalign_len.txt'),
        png_align = join(out_dir, 'qc', '00_qc_post','{sp}_aligned.png'),
        png_unalign = join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.png'),
        txt_align = join(out_dir, 'qc', '00_qc_post','{sp}_aligned.txt'),
        txt_unalign = join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.txt'),
    shell:
        """
        samtools view -F 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_a}; \
        samtools view -f 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_u}; \
        Rscript {params.R} {wildcards.sp} {output.bam_a} {output.bam_u} {params.base}
        """

if (multiplex_flag == 'Y'):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}_aligned.png'), sp=sp_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.png'), sp=sp_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}_aligned.txt'), sp=sp_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.txt'), sp=sp_list),
            txt_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
            png_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys())
        params:
            rname = "ic_qc_ts",
            R = join(source_dir,'workflow','scripts','05_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}", \
                    b_txt = "{input.txt_bc}"))'
            """
else:
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}_aligned.png'), sp=sp_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.png'), sp=sp_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}_aligned.txt'), sp=sp_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}_unaligned.txt'), sp=sp_list),
        params:
            rname = "ic_qc_ts",
            R = join(source_dir,'workflow','scripts','05_qc_report_nondemux.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}"))'
            """

rule dedup:
    """
    deduplicate merged.i.bam files
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.merged.si.bam'),
    params:
        rname='ic_dedup',
    envmodules:
        config['umitools']  
    output:
        o1 = join(out_dir,'09_dedup_bam','{sp}.dedup.bam'),
        o2 = join(out_dir,'09_dedup_bam','{sp}.dedup.log'),
    shell:
        """
        umi_tools dedup \
        -I {input.f1} \
        --method unique --multimapping-detection-method=NH --umi-separator=rbc: \
        -S {output.o1} \
        --log2stderr -L {output.o2};
        """

rule sort_index_dedup:
    """
    sort dedup.bam file
    """
    input:
        f1 = join(out_dir,'09_dedup_bam','{sp}.dedup.bam'),
    params:
        rname='ic_si_dedup',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'09_dedup_bam','{sp}.dedup.si.bam'),
        o12 = join(out_dir,'09_dedup_bam','{sp}.dedup.si.bam.bai'),
    shell:
        """
        samtools sort {input.f1} -o {output.o1};
        samtools index {output.o1}
        """

rule dedup_header:
    """
    cat bam header
    """
    input:
        f1 = join(out_dir,'09_dedup_bam','{sp}.dedup.si.bam'),
    params:
        rname='ic_dedup_head',
    envmodules:
        config['samtools']   
    output:
        o1 = join(out_dir,'09_dedup_bam','{sp}.dedup.header.txt'),
    shell:
        """
        samtools view -H {input.f1} > {output.o1}
        """

rule dedup_unique:
    """
    split dedup file into multimapped and unique files
    """
    input:
        bam = join(out_dir,'09_dedup_bam/{sp}.dedup.si.bam'),
        h = join(out_dir,'09_dedup_bam/{sp}.dedup.header.txt'),
    params:
        rname='ic_split_dedup',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'10_dedup_split/{sp}.dedup.unique.bam'),
    shell:
        """
        set +e
        samtools view {input.bam} | grep -w 'NH:i:1' | cat {input.h} - |  samtools sort | samtools view -Sb > {output.o1}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule index_dedup_unique:
    """
    index deduped unique file
    """
    input:
        f1 = join(out_dir,'10_dedup_split','{sp}.dedup.unique.bam'),
    params:
        rname='ic_index_splitu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'10_dedup_split','{sp}.dedup.unique.i.bam'),
    shell:
        """
        cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule create_beds:
    """
    create bed file from the deduped unique file and deduped all file
    """
    input:
        all = join(out_dir,'09_dedup_bam','{sp}.dedup.si.bam'),
        unique = join(out_dir,'10_dedup_split','{sp}.dedup.unique.i.bam'),
    params:
        rname='ic_bed',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'11_bed','{sp}_all.bed'),
        unique = join(out_dir,'11_bed','{sp}_unique.bed')
    shell:
        """
        bedtools bamtobed \
            -split -i {input.all} | bedtools sort -i - > {output.all}; 
        bedtools bamtobed \
            -split -i {input.unique} | bedtools sort -i - > {output.unique}; 
        """

rule create_safs: 
    """
    create SAF files from the deduped unique bed and all bed files
    """
    input:
        all = join(out_dir,'11_bed','{sp}_all.bed'),
        unique = join(out_dir,'11_bed','{sp}_unique.bed')
    params:
        rname='ic_saf',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'12_SAF/{sp}_' + str(nt_merge) + '_all.SAF'),
        unique = join(out_dir,'12_SAF/{sp}_' + str(nt_merge) + '_unique.SAF')
    shell:
        """
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.all} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.all};
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.unique} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.unique}
        """

rule feature_counts_allreads:
    """
    Unique reads (fractional counts correctly count splice reads for each peak. 
    When peaks counts are combined for peaks connected by splicing in Rscript)
    """
    input:
        bam = join(out_dir,'09_dedup_bam','{sp}.dedup.si.bam'),
        saf = join(out_dir,'12_SAF','{sp}_' + str(nt_merge) + '_all.SAF')
    params:
        rname='ic_allreads',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'13_counts','allreadpeaks','{sp}_' + str(nt_merge) + '_uniqueCounts.txt'),
        out_all = join(out_dir,'13_counts','allreadpeaks','{sp}_' + str(nt_merge) + '_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """   

rule feature_counts_uniquereads:
    """
    Include Multimap reads - MM reads given fractional count based on # of mapping 
    locations. All spliced reads also get fractional count. So Unique reads can get 
    fractional count when spliced peaks combined in R script the summed counts give 
    whole count for the unique alignement in combined peak.
    """
     input:
        bam = join(out_dir,'09_dedup_bam/{sp}.dedup.si.bam'),
        saf = join(out_dir,'12_SAF','{sp}_' + str(nt_merge) + '_unique.SAF'),
    params:
        rname='ic_saf',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'13_counts','uniquereadpeaks','{sp}_' + str(nt_merge) + '_uniqueCounts.txt'),
        out_all = join(out_dir,'13_counts','uniquereadpeaks','{sp}_' + str(nt_merge) + '_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """  

rule peak_junctions:
    input:
        unique = join(out_dir,'13_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'),
        all = join(out_dir,'13_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt')
    params:
        rname = 'ic_junc',
        script = join(source_dir,'workflow','scripts','06_peak_junction.R'),
        p_type = peak_id,
        junc = sp_junc,
        r_depth= min_count,
        d_method=DE_method,
        sp = "{sp}",
        n_merge = nt_merge,
        base = join(out_dir,'14_peaks/'),
    envmodules:
        config['R'],
    output:
        o1 = join(out_dir,'14_peaks','{sp}_' + str(nt_merge) + '_peakjunction.txt'),
        bed = join(out_dir,'14_peaks','{sp}_' + str(nt_merge) + '_peakDepth.bed'),
        pbed = join(out_dir,'14_peaks','{sp}_' + str(nt_merge) + '_peakDepth_P.bed'),
        nbed = join(out_dir,'14_peaks','{sp}_' + str(nt_merge) + '_peakDepth_N.bed'),
    shell:
        '''
        Rscript {params.script} \
            {params.p_type} \
            {input.unique} \
            {input.all} \
            {params.junc} \
            {params.r_depth} \
            {params.d_method} \
            {params.sp} \
            {params.n_merge} \
            {params.base}
        '''

rule annotations:
    """
    generate annotation table for the project
    """
    input:
        expand(join(out_dir,'13_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'), sp=sp_list),
    params:
        rname='ic_anno',
        script = join(source_dir,'workflow','scripts','06_annotation.R'),
        ref_sp = nova_ref,
        s_flag = sy_flag,
        s_path = sy_path,
        a_path = alias_path,
        g_path = gen_path,
        rs_path = rseq_path,
        c_path = can_path,
        i_path = intron_path,
        r_path = rmsk_path,
        base = join(out_dir,'15_annotation/'),
        a_config = annotation_config,       
    envmodules:
        config['R']
    output:
        anno = join(out_dir,'15_annotation','annotations.txt'),
        ncRNA = join(out_dir,'15_annotation','ncRNA_annotations.txt'),
        alias = join(out_dir,'15_annotation','ref_alias.csv'),
        refseq = join(out_dir,'15_annotation','ref_refseq.csv'),
        lncRNA = join(out_dir,'15_annotation','ref_lncRNA.csv'),
    shell:
        """
        Rscript {params.script} \
            {params.ref_sp} \
            {params.s_flag} \
            {params.s_path} \
            {params.a_path} \
            {params.g_path} \
            {params.rs_path} \
            {params.c_path} \
            {params.i_path} \
            {params.r_path} \
            {params.base} \
            {params.a_config}
        """

rule peak_calling:
        '''
        
        '''
        input:
            f1 = join(out_dir,'14_peaks','{sp}_' + str(nt_merge) + '_peakjunction.txt'),
        params:
            rname = "ic_peaks",
            script = join(source_dir,'workflow','scripts','07_peak_generation.R'),
        envmodules:
            config['R'],
        output:
            annobed = join(out_dir, '15_annotation','{sp}_' + str(nt_merge) + '_annotable.bed'),
            peaksbed = join(out_dir, '15_annotation','{sp}_' + str(nt_merge) + '_peakstable.bed'),
            OL = join(out_dir,'14_peaks', '{sp}_' + str(nt_merge) + '_peaks_OL.txt'),
        shell:
            '''
            Rscript {params.script}
            '''

if (DE_method=="LALA"):
    def set_contrasts():
        #read in contrast list as df
        #multiplex_id   #sampleid_1 #sampleid_2
        contrast_list = ""
        return(contrast_list)


    '''
    Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.html}", \
            params = list(
                samplename = "{params.samp}", \
                PeaksUniq = "{input.unique}", \
                PeaksFracMM = "{input.all}", \
                OutDIR = "{params.base}", \
                readdepth = "{params.depth}", \
                PeakIdnt = "{params.peakid}", \
                DEmethod = "{params.de_meth}", \
                species = "{params.n_ref}", \
                ntmerge = "{params.nt_merge}", \
                JoinJunc = "{params.s_aware}", \
                AnnoFiles = "{params.annof}", \
                AnnoScript = "{params.annos}", \
                OutDIR2 = "{params.base2}"))'
                '''
    def manorm_cmd(contrast_df):
        contrast_df = ""
        cmd = ""
        #for each row in df
            #file_in = join(output_dir,multiplex.col,'12_bed',sample1.col + '.bed')
            #file_out = join(output_dir,multiplex.col,'16_manorm',sample1.col + '.P.bed')
            #cmd = cmd + "awk '$6=="+"' " + file_in "> " + file_out " "
        #return(cmd)

    rule create_pbeds:
        '''
        expand(join(out_dir,'{mp}/16_manorm/{sp}.p.bed'),zip,mp=mp_list,sp=sp_list)
        expand(join(out_dir,'{mp}/16_manorm/{sp}.n.bed'),zip,mp=mp_list,sp=sp_list)
        '''
        input:
            f1 = join(out_dir,'{mp}/12_bed/{sp}.unique.bed'),
        output:
            o1 = join(out_dir,'{mp}/16_manorm/{sp}.p.bed'),
            o2 = join(out_dir,'{mp}/16_manorm/{sp}.n.bed'),
        shell:
            '''
            awk '$6=="+"' {input.f1} > {output.o1};
            awk '$6=="-"' {input.f1} > {output.o2};
            '''
    
    rule manorm:
        '''
        expand(join(out_dir,'{pairOfSamples}/{sampleid}.{strand}.bed", pairOfSamples=keys, sampleid=values, strand = c["p","n"]
        '''
        input:
            s1_r = join(out_dir,'{mp}/15_annotation/{sample1}.RNAME.bed'), #{sample1}WT_allPeaksforMAnrom_50nt_peakDepth5Test.P.bed,
            s2_r = join(out_dir,'{mp}/15_annotation/{sample2}.RNAME.bed'), #{sample2}KO_allPeaksforMAnrom_50nt_peakDepth5Test.P.bed,
            s1_p = join(out_dir,'{mp}/16_manorm/{sample1}.p.bed'), #{sample1}WT_CLIP.dedup.i.bam.P.bed
            s2_p = join(out_dir,'{mp}/16_manorm/{sample2}.p.bed'), #{sample2}WT_CLIP.dedup.i.bam.P.bed
        params:
            name1 = join(out_dir,'{mp}','16_manorm','{sample1}vs{sample2}','{sample1}'),
            name2 = join(out_dir,'{mp}','16_manorm','{sample1}vs{sample2}','{sample2}'),
            base = join(out_dir,'{mp}/16manorm/') #/data/RBL_NCI/Wolin/Phil/HaCat_fCLIP2/peaks_MAnorm/WTvsKO/
        envmodules:
            config['MANorm'],
        output:
            o1 = join(out_dir,'{mp}','16_manorm','{sample1}vs{sample2}','{sample1}.txt'),
            o2 = join(out_dir,'{mp}','16_manorm','{sample1}vs{sample2}','{sample2}.txt'),
        shell:
            '''
            manorm \
                --p1 {input.s1_r} \
                --p2 {input.s2_r} \
                --r1 {input.s1_p} \
                --r2 {input.s2_p} \
                --s1 0 \
                --s2 0 \
                -p 1 \
                -d 25 \
                -n 10000 \
                -s \
                -o {params.base} \
                --name1 {params.name1} \
                --name2 {params.name2}
            '''


    rule manorm:
        """
        """
        input:
            
        params:
        envmodules:
        output:
        shell:
            """
            #step 1
            pbed_cmd
            awk '$6=="+"' {sample1} WT_CLIP.dedup.i.bam.bed > {sample1}WT_CLIP.dedup.i.bam.P.bed
            awk '$6=="+"' {sample2}KO_CLIP.dedup.i.bam.bed > {sample2}KO_CLIP.dedup.i.bam.P.bed

            #step2
            manorm_cmd
            manorm \
            --p1 {sample1}WT_allPeaksforMAnrom_50nt_peakDepth5Test.P.bed \
            --p2 {sample2}KO_allPeaksforMAnrom_50nt_peakDepth5Test.P.bed \
            --r1 {sample1}WT_CLIP.dedup.i.bam.P.bed \
            --r2 {sample2}KO_CLIP.dedup.i.bam.P.bed \
            --s1 0 \
            --s2 0 \
            -p 1 \
            -d 25 \
            -n 10000 \
            -s \
            -o /data/RBL_NCI/Wolin/Phil/HaCat_fCLIP2/peaks_MAnorm/WTvsKO/ \
            --name1 WT_50nt_peakDepth5Test_Pos \
            --name2 KO_50nt_peakDepth5Test_Pos

            #step 3
            pbed_cmd
            awk '$6=="-"' {sample1} WT_CLIP.dedup.i.bam.bed > {sample1}WT_CLIP.dedup.i.bam.N.bed
            awk '$6=="-"' {sample2}KO_CLIP.dedup.i.bam.bed > {sample2}KO_CLIP.dedup.i.bam.N.bed

            #step4
            manorm_cmd
            manorm \
            --p1 {sample1}WT_allPeaksforMAnrom_50nt_peakDepth5Test.N.bed \
            --p2 {sample2}KO_allPeaksforMAnrom_50nt_peakDepth5Test.N.bed \
            --r1 {sample1}WT_CLIP.dedup.i.bam.N.bed \
            --r2 {sample2}KO_CLIP.dedup.i.bam.N.bed \
            --s1 0 \
            --s2 0 \
            -p 1 \
            -d 25 \
            -n 10000 \
            -s \
            -o /data/RBL_NCI/Wolin/Phil/HaCat_fCLIP2/peaks_MAnorm/WTvsKO/ \
            --name1 WT_50nt_peakDepth5Test_Neg \
            --name2 KO_50nt_peakDepth5Test_Neg


            """