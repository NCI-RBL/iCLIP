'''
* Authors * 
S. Sevilla
P. Homan

* Overview * 
- Multiplexed samples are split based on provided barcodes and named using provide manifests, maximum 10 samples
- Adaptors are stripped from samples
- Samples are unzipped and split into smaller fastq files to increase speed
- Samples are aligned using NovaAlign
- SAM and BAM files are created
- Samples are merged

* Requirements *
- Read specific input requirements, and execution information on the Wikipage
located at: https://github.com/RBL-NCI/iCLIP.git
'''

report: "report/workflow.rst"

# Allow users to fix the underlying OS via singularity.
container: "docker://continuumio/miniconda3"

from os.path import join
import pandas as pd
from collections import defaultdict
import yaml

###############################################################
# config handling, set parameters
###############################################################
#snakemake config
source_dir = config['source_dir']
cont_dir = config['container_dir']
out_dir = config['output_dir'].rstrip('/') + '/'
fastq_dir = config['fastq_dir'].rstrip('/') + '/'

sample_manifest = config['sample_manifest']
multiplex_manifest = config['multiplex_manifest']

nova_ref = config['novoalign_reference']
splice_aware = config['splice_aware'].capitalize()
include_rRNA = config['include_rRNA'].capitalize()
splice_bp = config['splice_bp_length']
multiplex_flag = config['multiplex_flag'].capitalize()
mismatch = config['mismatch_allowance']
split_value =  config['split_value']
min_count = config['minimum_count']
nt_merge = str(config['nt_merge']) + 'nt'
peak_id = config['peak_id'].upper()
DE_method = config['DE_method'].upper()
sp_junc = config['splice_junction'].upper()
condense_exon = config['condense_exon'].upper()


#expand reference selection
if (nova_ref == "mm10"):
    genome_ref = "GENCODE mm10 v23"
elif (nova_ref == "hg38"):
    genome_ref = "GENCODE hg38 v32"

#read in index config file and assign paths
index_manifest = join(source_dir, 'config', 'index_config.yaml')

with open(index_manifest) as file:
    index_list = yaml.load(file, Loader=yaml.FullLoader)

gen_path = index_list[nova_ref]['gencode_path']
rseq_path = index_list[nova_ref]['refseq_path']
can_path = index_list[nova_ref]['canonical_path']
intron_path = index_list[nova_ref]['intron_path']
rmsk_path = index_list[nova_ref]['rmsk_path']
alias_path = index_list[nova_ref]['alias_path']
add_anno_path = index_list[nova_ref]['additional_anno_path']

#singularity exec command
singularity_exec = "singularity exec -B /data/$USER,/data/CCBR_Pipeliner," + out_dir + "," + fastq_dir + ",/fdb,/scratch"

#annotation config
annotation_config = join(source_dir,'config','annotation_config.txt')

#convert splice junction selection
if (sp_junc=="Y"):
    sp_junc = "TRUE"
else:
    sp_junc = "FALSE"

#convert condense junction selection
if (condense_exon=="Y"):
    cond_exon = "TRUE"
else:
    cond_exon = "FALSE"

#create list of alignment types based on splice_aware flag
if (splice_aware == 'N'):
    align_list = ["unaware"]
else:
    align_list = ["unmasked"]

#convert rRNA selection
if (include_rRNA=="Y"):
    refseq_rrna = "TRUE"
else:
    refseq_rrna = "FALSE"

###############################################################
# create sample lists
###############################################################
#dict multiplex name (1): sample name (1): barcodes (>1, will vary)
def CreateSampleDicts(df_m,df_s):
    s_dict={}
    m_dict={}

    #create dict that maps multiplex: sampleid : barcode
    for multi_name in df_s['multiplex'].unique():
        sub_df = df_s[df_s['multiplex']==multi_name]
        s_dict[multi_name]={}

        #each row maps to a sample:barcode and sample:group
        for index,row in sub_df.iterrows():
            sample_name = row['sample']
            s_dict[multi_name][sample_name]=row['barcode']

    #creat dict that maps mulitplex: filename
    m_dict = dict(zip(df_m.multiplex, df_m.file_name))

    return(m_dict,s_dict)

#create lists of multiplex ids, sample ids, filenames
def CreateProjLists(dict_m,dict_s):
    mp_list = []
    sp_list = []
    file_list = []

    #multiplex,sample
    for k,v in dict_s.items():
        file_list.append(dict_m[k])

        #barcode,sample
        for v,v2 in dict_s[k].items():
            mp_list.append(k)
            sp_list.append(v)
    return(mp_list,sp_list,file_list)

###############################################################
# snakemake functions
###############################################################
#determines the number of files to split the input into
def check_split(split_in):
    if (type(split_in) == str):
        split_replace = split_in.replace(',', '')
        if (type(split_in) == str):
            split_replace = 1000000
    elif (split_in < 3000):
        split_replace = 1000000
    elif (type(split_in) == float):
        split_replace = int(split_in)
    else:
        split_replace = split_in
    return split_replace

#get list of fq names based on multiplex name
#{fastq_dir}/{filename}.fastq.gz
def get_fq_names(wildcards):
    fq = fastq_dir + multiplex_dict[wildcards.mp]
    return(fq)

#create demux command line
def demux_cmd(wildcards):
    #subset dataframe by multiplex name that matches multiplex and barcode
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp)]
    sub_df.reset_index(inplace=True)

    barcodes = ' '.join(sub_df['barcode'].tolist())

    #create command
    cmd_line = fastq_dir + multiplex_dict[wildcards.mp] + ' ' + sub_df.iloc[0]['adaptor'] + ' ' + barcodes + ' --out_dir ' + out_dir + wildcards.mp + '/'

    return(cmd_line)

#create nondemux command line
def nondemux_cmd(wildcards):
  cmd_line = ''

  for k,v in multiplex_dict.items():
    for k2,v2 in samp_dict[k].items():
      cmd_line = 'cp ' + fastq_dir + v + ' ' + out_dir + k + '/01_renamed/' + k2 + '.fastq.gz; ' + cmd_line
  
  return(cmd_line)

#command needed to move and rename demux files
def rename_cmd(wildcards):
    bc = samp_dict[wildcards.mp][wildcards.sp]

    cmd_line = out_dir + wildcards.mp + '/demux_' + bc + '.fastq.gz' + ' ' + out_dir + wildcards.mp + '/01_renamed/' + wildcards.sp + '.fastq.gz'

    return(cmd_line)

#command needed to cut adaptors from demux samples
def adapt_cmd(wildcards):
    sub_df = df_samples[(df_samples['multiplex']==wildcards.mp) & (df_samples['sample']==wildcards.sp)]

    base = out_dir + wildcards.mp + '/01_remove_adaptor/' + wildcards.sp
    fq_un =  base + '_untrimmed.fastq.gz '
    fq_t = base + '_trimmed.fastq.gz '

    base = out_dir + '/' + wildcards.mp + '/01_renamed/' + wildcards.sp
    fq_o = base + '.fastq.gz '

    command_line = '--untrimmed_output ' + fq_un + '--reads_trimmed ' + fq_t + fq_o + sub_df.iloc[0]['adaptor']

    return(command_line)

#rename the output files after adaptor removal
def rename_adapt_cmd(wildcards):
    cmd = ''

    for sp in sp_list:
        df_sub = df_samples[(df_samples['sample']==sp)] 
        
        #input
        f1 = join(out_dir,df_sub.iloc[0]["multiplex"],'01_remove_adaptor',sp + '_trimmed.fastq.gz')
        f2 = join(out_dir,df_sub.iloc[0]["multiplex"],'01_remove_adaptor',sp + '_untrimmed.fastq.gz')

        #output
        o1 = join(out_dir,'01_remove_adaptor',sp + '_removed_seq.fastq.gz')
        o2 = join(out_dir,'01_remove_adaptor',sp + '_filtered.fastq.gz')

        cmd = 'mv ' + f1 + ' ' + o1 + '; mv ' + f2 + ' ' + o2 + '; ' + cmd
    return(cmd)
    
#create cmd to rename fastq screen outputs
def rename_fqscreen1(wildcards):
    n_in = join(out_dir, 'qc', '00_qc_screen_species',wildcards.sp + '_screen')
    n_out = join(out_dir, 'qc', '00_qc_screen_species',wildcards.sp + '_filtered')
    cmd = 'mv ' + n_in + '.txt ' + n_out + '.txt; mv ' + n_in + '.png ' + n_out + '.png; mv '+ n_in + '.html ' + n_out + '.html'
    return(cmd)

def rename_fqscreen2(wildcards):
    n_in = join(out_dir, 'qc', '00_qc_screen_rrna',wildcards.sp + '_screen')
    n_out = join(out_dir, 'qc', '00_qc_screen_rrna',wildcards.sp + '_filtered')
    cmd = 'mv ' + n_in + '.txt ' + n_out + '.txt; mv ' + n_in + '.png ' + n_out + '.png; mv '+ n_in + '.html ' + n_out + '.html'
    return(cmd)

#determine the number of lines to split each file by reading split_params.tsv file
def get_filechunk_size(wildcards):
    fq_path=join(out_dir,'02_unzip',wildcards.sp + '.fastq ')

    #read in split_params file for chunksize
    try:
        split_df=pd.read_csv(join(out_dir,'qc','split_params.tsv'),names=["path","filenum","chunksize"],sep="\t")
        sub_df=split_df[(split_df['path']==fq_path)]

        #split can only create a max of 99 split files; handle large file sets to ensure max reads per 99 files
        if (sub_df.iloc[0]['filenum']>98):
            chunk_size = int(((sub_df.iloc[0]['chunksize'] * sub_df.iloc[0]['filenum']) / 99)+.9)
        else:
            chunk_size = sub_df.iloc[0]['chunksize']

    except:
        return(0)
    return(chunk_size)

#determine which umi separator to use
#iCount addes rbc: to all demux files; external demux uses an _
def umi_parameter(wildcards):
    if(multiplex_flag == 'Y'):
        umi_sep="rbc:"
    else:
        umi_sep="_"
    return(umi_sep)

#get index file depending on alignment type
def get_index(wildcards):

    #unaware index
    if (wildcards.al == "unaware"):
        nova_index = index_list[nova_ref]['std']
    elif (wildcards.al == "masked"):
        nova_index = index_list[nova_ref]['spliceaware_masked'][str(splice_bp)+'bp']
    elif (wildcards.al == "unmasked"):
        nova_index = index_list[nova_ref]['spliceaware_unmasked'][str(splice_bp)+'bp']
    
    return(nova_index)

#determine alignment input based on splice_aware flag
def get_align_input(wildcards):
    if (splice_aware == "N"):
        f1 = join(out_dir,'04_sam','01_alignment','{sp}.{al}.split.{n}.sam'),
    else:
        f1 = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam'),
    return(f1)

###############################################################
# main code
###############################################################
#read manifests
df_multiplex = pd.read_csv(multiplex_manifest,sep="\t")
df_samples = pd.read_csv(sample_manifest,sep="\t")

#create dicts
(multiplex_dict,samp_dict) = CreateSampleDicts(df_multiplex,df_samples)

#create unique filename_list and paired lists, where length = N samples, all samples
#mp_list    sp_list
#mp1        sample1
#mp1        sample2
(mp_list,sp_list,file_list) = CreateProjLists(multiplex_dict,samp_dict)

#get split files value
split_value = check_split(split_value)

###############################################################
# rule all
###############################################################
#set rule_all inputs depending on flags:
## if samples have been multiplexed
if multiplex_flag == 'Y':
    input_multiplex = [expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.png'), mp=samp_dict.keys()),expand(join(out_dir,'{mp}/00_qc_post/{mp}_barcode.txt'),mp=samp_dict.keys())]
else:
    input_multiplex = [expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list)]

## if samples are spliced
if splice_aware == 'Y':
    input_unmapped = expand(join(out_dir,'04_sam','04_unmapped','{sp}.{al}.complete.bam'),  sp=sp_list, al=align_list)

    input_splice = [expand(join(out_dir,'10_mapq_score','{sp}.readids.txt'), sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.unaware.subset.bam'),sp=sp_list),
                    expand(join(out_dir,'10_mapq_score','{sp}.mapq_recalculated.bam'),sp=sp_list)]
else:
    input_unmapped = expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'), sp=sp_list, al=align_list),

    input_splice = [expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'),sp=sp_list, al=align_list)]

#if samples are running MANORM
if DE_method == "MANORM":
    input_peak_annotation = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_mapq_IN.txt'),sp=sp_list),
            expand(join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_complete.txt'),sp=sp_list),
            expand(join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom.bed'),sp=sp_list),
            expand(join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom_P.bed'),sp=sp_list),
            expand(join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom_N.bed'),sp=sp_list)]
else:
    input_peak_annotation = [expand(join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_mapq_IN.txt'),sp=sp_list),
            expand(join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_complete.txt'),sp=sp_list)]


#local rules
localrules: check_manifest

rule all:
    input:
        #check manifest
        join(out_dir,'qc','manifest_clean.txt'),

        #check input fastq files
        expand(join(fastq_dir,'{fq_file}'), fq_file=file_list), #multiplexed sample fastq files exist
        
        #different inputs for multiplexed or nonmultiplexed pipeline
        input_multiplex,
        
        # #demultiplex samples, rename samples
        # expand(join(out_dir,'{mp}/demux_nomatch5.fastq.gz'),mp=samp_dict.keys()), #demultiplex
        # expand(join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz'), zip, mp=mp_list, sp=sp_list), #rename
        
        # #Remove adaptors, rename samples
        # expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        # expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        # expand(join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz'),sp=sp_list),
        # expand(join(out_dir,'01_remove_adaptor','{sp}_removed_seq.fastq.gz'),sp=sp_list),

        #FastQC
        expand(join(out_dir,'{mp}','00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list,sp=sp_list),
        expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        
        #FastQ Screen
        expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),sp=sp_list),
        expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),sp=sp_list),
        
        #Determine file split parameters
        join(out_dir,'qc','split_params.tsv'),
        
        #Merge splits
        expand(join(out_dir,'07_bam_merged_splits/{sp}.{al}.merged.unique.bam'), sp=sp_list, al=align_list),
        expand(join(out_dir,'07_bam_merged_splits/{sp}.{al}.merged.mm.bam'),sp=sp_list, al=align_list),

        #Merge unique and mm
        expand(join(out_dir,'08_bam_merged','{sp}.{al}.merged.bam'), sp=sp_list, al=align_list),

        #Sort and index merged
        expand(join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam'), sp=sp_list, al=align_list),
        expand(join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam.bai'), sp=sp_list, al=align_list),

        #Samstats
        expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'), sp=sp_list, al=align_list),

        #MultiQC Report
        join(out_dir,'qc/multiqc_report.html'),

        #Alignment stats
        expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
        expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),

        #QC Troubleshooting
        join(out_dir,'qc','qc_report.html'),
        
        #Unmapped read output
        input_unmapped,

        #Deduplicate
        expand(join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'), sp=sp_list, al=align_list),
        
        #Bam processing
        expand(join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'), sp=sp_list),

        #Bed files
        expand(join(out_dir,'10_bed','{sp}_all.bed'), sp=sp_list),
        expand(join(out_dir,'10_bed','{sp}_unique.bed'), sp=sp_list),

        #SAF 
        expand(join(out_dir,'11_SAF','{sp}_'+ str(nt_merge) +'_all.SAF'), sp=sp_list),
        expand(join(out_dir,'11_SAF','{sp}_'+ str(nt_merge) +'_unique.SAF'), sp=sp_list),

        #Count features
        expand(join(out_dir,'12_counts','uniquereadpeaks','{sp}_' + str(nt_merge) + '_uniqueCounts.txt'), sp=sp_list),
        expand(join(out_dir,'12_counts','uniquereadpeaks','{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'), sp=sp_list),
        expand(join(out_dir,'12_counts','allreadpeaks','{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'), sp=sp_list),
        expand(join(out_dir,'12_counts','allreadpeaks','{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'), sp=sp_list),

        #annotations
        join(out_dir,'13_annotation', '01_project','annotations.txt'),
        input_peak_annotation,
        expand(join(out_dir,'13_annotation', '{sp}_' + str(nt_merge) + '_annotation_final_report.html'),sp=sp_list),
        expand(join(out_dir,'13_annotation', '{sp}_' + str(nt_merge) + '_annotation_final_table.txt'),sp=sp_list),

#common and other SMK 
if source_dir == "":
    include: "rules/common.smk"
    include: "rules/other.smk"
else:
    include: join(source_dir,"workflow/rules/common.smk")
    include: join(source_dir,"workflow/rules/other.smk")

###############################################################
# snakemake rules
###############################################################
rule check_manifest:
    """
    Read in multiplex manifest and sample manifest.
    Use python script to check file matching, sample matching, and invalid characters.
    If files are correct, outputs a temp file. If not, outputs error file.
    """
    input:
        f1 = multiplex_manifest,
        f2 = sample_manifest
    params:
        py = join(source_dir,'workflow','scripts','01_check_manifest.py'),
        base = join(out_dir,'qc','manifest_')
    envmodules:
        config['python']  
    output:
        o1 = join(out_dir, 'qc', 'manifest_clean.txt'),
    shell:
        """
        python {params.py} \
            '{params.base}' {input.f1} {input.f2}
        """

#pipeline branches to demultiplex, if necessary
if (multiplex_flag == 'Y'):
    rule qc_barcode:
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname="01a_bcQC",
            py = join(source_dir,'workflow', 'scripts', '02_barcode_qc.py'),
            mm = mismatch,
        envmodules:
            config['python'],
        output:
            o1 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys()),
            o2 = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys())
        shell:
            """
            python {params.py} \
                {sample_manifest} {multiplex_manifest} {fastq_dir} {out_dir} {params.mm}
            """

    rule demultiplex:
        """
        https://icount.readthedocs.io/en/latest/ref_CLI.html
        Reads in fastq files from multiplex_manifest.
        Finds multiplex match between manifest files, splits files based on list of
        barcodeIDs found in sample_manifest

        For example: SIM_iCLIP_S1_R1_001.fastq would be split into barcodes NNNTGGCNN and NNNCGGANN

        file_name                   multiplex
        SIM_iCLIP_S1_R1_001.fastq   SIM_iCLIP_S1

        multiplex       sample          group       barcode     adaptor
        SIM_iCLIP_S1    Ro_Clip         CLIP        NNNTGGCNN   AGATCGGAAGAGCGGTTCAG
        SIM_iCLIP_S1    Control_Clip    CNTRL       NNNCGGANN   AGATCGGAAGAGCGGTTCAG
        """
        input:
            f1 = get_fq_names,
            qc = expand(join(out_dir,'{mp}','00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys())
        params:
            rname='01b_demux',
            s_exec = singularity_exec,
            doc = join(cont_dir,'icount.sif'),
            cmd = demux_cmd,
            ml = 15,
            mm = mismatch
        envmodules:
            config['singularity'],
        output:
            o1 = join(out_dir,'{mp}','demux_nomatch5.fastq.gz'),
        shell:
            """
            {params.s_exec} {params.doc} iCount demultiplex --minimum_length {params.ml} --mismatches {params.mm} {params.cmd}
            """

    rule rename_demux:
        """
        renames demultiplexed files based on
        from: {out_dir}/{multiplex_id}/demux_{barcode}.fastq.gz
        to: {out_dir}/{multiplex_id}/01_renamed/demux_{sample_id}.fast.gz
        """
        input:
            join(out_dir,'{mp}','demux_nomatch5.fastq.gz'),
        params:
            rname = '01c_rename',
            cmd = rename_cmd
        output:
            o1 = join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz')
        shell:
            'cp {params.cmd}'       
else: 
    rule copy_nondemux:
        """
        copies fastq files from source location and moves into renamed file folder
        """
        input:
            f1 = join(out_dir,'qc', "manifest_clean.txt")
        params:
            rname = '01_nondemux',
            cmd = nondemux_cmd
        output:
            o1 = expand(join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz'),zip,mp=mp_list,sp=sp_list)
        shell:
            '{params.cmd}'

rule remove_adaptors:
    """
    removes adaptors from files. software outputs:
    1) untrimmed.fastq.gz - the remaining sequences without the adaptor and 
    2) trimmed.fastq.gz - the sequences that were trimmed from the input file
    """
    input:
        f1 = join(out_dir,'{mp}','01_renamed','{sp}.fastq.gz')
    params:
        rname='02_adapt',
        s_exec = singularity_exec,
        ml = 1,
        cmd = adapt_cmd,
        doc = join(cont_dir,'icount.sif'),
    envmodules:
        config['singularity'],    
    output:
        o1 = join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),
        o2 = join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz')
    shell:
        """
        {params.s_exec} {params.doc} iCount cutadapt -ml {params.ml} {params.cmd}
        """

rule rename_adaptors:
    """
    naming schema of adaptors is confusing, and makes interpretation of QC report difficult
    
    files are renamed for clarity, renamed files moved into common dir
    1) untrimmed.fastq.gz --> filtered.fastq.gz
    2) trimmed.fastq.gz --> removed_seq.fastq.gz
    """
    input:
        f1 = expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_trimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list),
        f2 = expand(join(out_dir,'{mp}','01_remove_adaptor','{sp}_untrimmed.fastq.gz'),zip,mp=mp_list,sp=sp_list)
    params:
        rname = "03_rename_ad",
        cmd = rename_adapt_cmd
    output:
        o1 = expand(join(out_dir,'01_remove_adaptor','{sp}_removed_seq.fastq.gz'),sp=sp_list),
        o2 = expand(join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz'),sp=sp_list)
    shell:
        '''
        {params.cmd}
        '''

rule qc_fastq_pre:
    """
    Runs FastQC report on each sample before adaptors have been removed
    """
    input:
        f1 = join(out_dir,'{mp}/01_renamed/{sp}.fastq.gz')
    params:
        rname='04_fqc_pre',
        base = join(out_dir,'{mp}/00_qc_pre/')
    envmodules:
        config['fastqc']
    output:
        o1 = join(out_dir,'{mp}/00_qc_pre/{sp}_fastqc.html')
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule qc_fastq_post:
    """
    Runs FastQC report on each sample after adaptors have been removed
    """
    input:
        f1 = join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz')
    params:
        rname='04_fqc_post',
        base = join(out_dir, 'qc', '00_qc_post'),
    envmodules:
        config['fastqc']
    output:
        o1 = join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html')
    shell:
        """
        fastqc {input.f1} -o {params.base}
        """

rule gunzip_files:
    """
    unzips untrimmed files - these are what is leftover after adaptor trimming
    """
    input:
        f1 = join(out_dir,'01_remove_adaptor','{sp}_filtered.fastq.gz')
    params:
        rname='05_gunzip',
    output:
        o1 = join(out_dir,'02_unzip','{sp}.fastq')
    shell:
        "gunzip -c {input.f1} > {output.o1}"
        
rule qc_screen_sp:
    """
    this will align to human, mouse, bacteria
    must run fastq_screen as two separate rules - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'02_unzip','{sp}.fastq')
    params:
        rname='06_screen1',
        base = join(out_dir, 'qc', '00_qc_screen_species'),
        fq_config = join(source_dir,'config','fqscreen_species_config.conf'),
        cmd = rename_fqscreen1
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.base} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}; {params.cmd}
        '''

rule qc_screen_rrna:
    """
    this will align to rRNA
    must run fastq_screen as two separate rules - multiqc will merge values of rRNA with human/mouse
    http://www.bioinformatics.babraham.ac.uk/projects/fastq_screen/_build/html/index.html
    """
    input:
        f1 = join(out_dir,'02_unzip','{sp}.fastq')
    params:
        rname='06_screen2',
        base = join(out_dir, 'qc', '00_qc_screen_rrna'),
        fq_config = join(source_dir,'config','fqscreen_rrna_config.conf'),
        cmd = rename_fqscreen2
    threads: 24
    envmodules:
        config['bowtie2'],
        config['perl'],
        config['fastq_screen'],
    output:
        o1 = join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),
    shell:
        '''
        fastq_screen --conf {params.fq_config} --outdir {params.base} \
            --threads {threads} --subset 1000000 --aligner bowtie2 --force \
            {input.f1}; {params.cmd}
        '''

rule determine_splits:
    """
    determine the number of files, and chunk sizes, to split FASTQ file
    split is necessary to improve speed of alignment
    split is done based off split_value input, maintaining every 4 lines = 1 seq
    """
    input:
        f1 = expand(join(out_dir,'02_unzip/{sp}.fastq'), sp=sp_list),
    params:
        rname='07_splitdoc',
        sh = join(source_dir, 'workflow', 'scripts', '03_find_split_parameters.sh'),
    output:
        o1 = join(out_dir,'qc','split_params.tsv')
    shell:
        """
        sh {params.sh} {output.o1} {split_value} {input.f1}
        """

rule split_files:
    """
    performs split of fastq file into smaller files using parameters in split_params.tsv
    """
    input:
        all = expand(join(out_dir,'02_unzip/{sp}.fastq'), sp=sp_list),
        f1 = join(out_dir,'02_unzip/{sp}.fastq'),
        sp = join(out_dir,'qc','split_params.tsv')
    params:
        rname='08_split',
        base = join(out_dir,'03_split/{sp}.split.'),
        cmd = get_filechunk_size
    output:
        o1 = dynamic(join(out_dir,'03_split/{sp}.split.{n}.fastq'))
    shell:
        """
        split --additional-suffix .fastq -l {params.cmd} --numeric-suffixes=10 {input.f1} {params.base}
        """

rule novoalign:
    """
    alignment for non-splice aware pipeline and for splice aware pipeline MAPQ recalculation
    """
    input:
        f1 = join(out_dir,'03_split','{sp}.split.{n}.fastq'),
    params:
        rname='09_novo',
        n_index = get_index,
    envmodules:
        config['novocraft']
    output:
        o1 = join(out_dir,'04_sam','01_alignment','{sp}.{al}.split.{n}.sam'),
    shell:
        """
        set +e
        novoalign -d {params.n_index} -k \
        -f {input.f1} -F STDFQ -c 32 \
        -t 15,3 -l 20 -x 4 -g 20 -s 1 -o SAM \
        -R 0 -r All 999 > {output.o1}
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

#pipeline branches for splice_aware processing
if (splice_aware == "Y"):
    rule sam_cleanup:
        """
        performs cleanup for inserts

        exitcode error fixed with manual entry
        """
        input:
            f1 = join(out_dir,'04_sam','01_alignment','{sp}.{al}.split.{n}.sam'),
        params:
            rname = '10a_sam_cleanup'
        envmodules:
            config['samtools']  
        output:
            tmp = join(out_dir,'04_sam','02_cleanup','{sp}.{al}.split.{n}.tmp.sam'),
            final = join(out_dir,'04_sam','02_cleanup','{sp}.{al}.split.{n}.final.sam'),
            unmapped = join(out_dir,'04_sam','02_cleanup','{sp}.{al}.split.{n}.final.unmapped.sam'),
        shell:
            """
            set +e
            samtools view {input.f1} | awk '{{ if (($4 == 1 && $6!~/^[0-9]I/ && $1~/:/ )||($4 > 1 && $1~/:/ )) {{ print }} }}' > {output.tmp};
            samtools view -H {input.f1} | cat - {output.tmp} > {output.final}; 
            samtools view -f 4 {input.f1} > {output.unmapped};
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 1
            else
                exit 0
            fi
            """

    rule convert_transcript:
        """
        converts transcriptome coordinates to genomic coordinates
        uses USeq version 8.9.6
        """
        input:
            f1 = join(out_dir,'04_sam','02_cleanup','{sp}.{al}.split.{n}.final.sam'),
        params:
            rname = '10b_convert',
            base = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam'),
            doc = join(cont_dir,'USeq_8.9.6','Apps/SamTranscriptomeParser')
        envmodules:
            config['java']  
        output:
            o1 = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam.gz'),
        shell:
            """
            java -Djava.io.tmpdir={params.base} -jar -Xmx50G -jar {params.doc} \
            -f {input.f1} \
            -a 50000 -n 25 -u -s {params.base}
            """

    rule gunzip_sam:
        """
        unzips sam file
        """
        input:
            f1 = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam.gz'),
        params:
            rname = '10c_gzipsam'
        output:
            o1 = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam'),
        shell:
            """
            gunzip -c {input.f1} > {output.o1}
            """

    rule merge_unmapped_reads:
        """
        unmapped reads are removed during genomic coordinates conversion, need to include them in final file
        convert sam file to bam file
        
        merge unmapped reads from cleanup to into final sam
        convert bam back to sam file
        """
        input:
            unmasked = join(out_dir,'04_sam','03_genomic','{sp}.{al}.split.{n}.sam'),
            unmapped = join(out_dir,'04_sam','02_cleanup','{sp}.{al}.split.{n}.final.unmapped.sam'),
        params:
            rname = '10d_mergeunmapped',
        envmodules:
            config['samtools'],
        output:
            unmasked = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.unmasked.bam'),
            merged = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.merged.bam'),
            final = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.final.bam'),
        shell:
            """
            set +e
            samtools view -S -b {input.unmasked} > {output.unmasked};
            samtools merge -f {output.merged} {input.unmapped} {output.unmasked};
            samtools view -h -o {output.final} {output.merged};
            exitcode=$?
            if [ $exitcode -eq 1 ]
            then
                exit 0
            else
                exit 0
            fi
            """

    rule sort_index_unmapped_splits:
        """
        merge mapped and unmapped reads into one file
        """
        input:
            f1 = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.final.bam')
        params:
            rname='10e_merge',
        envmodules:
            config['samtools']  
        output:
            sort = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.final.sorted.bam'),
        shell:
            """
            samtools sort {input.f1} > {output.sort};
            samtools index {output.sort}; 
            """
    
    rule merge_unmapped_splits:
        """
        merge all sample unmapped reads into one file
        """
        input:
            sorted_list = dynamic(join(out_dir,'04_sam','04_unmapped','{sp}.{al}.split.{n}.final.sorted.bam')),
        params:
            rname='10e_merge',
        envmodules:
            config['samtools']  
        output:
            final = join(out_dir,'04_sam','04_unmapped','{sp}.{al}.complete.bam'),
        shell:
            """
            samtools merge -f {output.final} {input.sorted_list} 
            """

rule split_mm_unique:
    """
    creates two text files from sam file
    1. unique file without NH:i
    2. MM file with NH:i
    """
    input:
        f1 = get_align_input
    params:
        rname='11_split_mu',
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir,'05_reads','{sp}.{al}.split.{n}.unique.txt'),
        o2 = join(out_dir,'05_reads','{sp}.{al}.split.{n}.mm.txt'),
    shell:
        """
        set +e
        samtools view {input.f1} | grep -v 'NH:i' > {output.o1}; \
        samtools view {input.f1} | grep 'NH:i' > {output.o2};
        exitcode=$?
        if [ $exitcode -eq 1 ]
        then
            exit 0
        else
            exit 0
        fi
        """

rule sam_to_header:
    """
    read sam file header, print to header txt
    """
    input:
        sam = get_align_input
    params:
        rname='12_sam_head',
    envmodules:
        config['samtools'] 
    output:
        h = join(out_dir,'05_reads','{sp}.{al}.split.{n}.header.txt'),
    shell:
        """
        samtools view -H {input.sam} > {output.h};
        """

rule create_bam_unique:
    """
    add nh:i:1 tag, cat header, create unique bam file
    """
    input:
        h = join(out_dir,'05_reads','{sp}.{al}.split.{n}.header.txt'),
        un = join(out_dir,'05_reads','{sp}.{al}.split.{n}.unique.txt'),
    params:
        rname='13a_bam_u',
    envmodules:
        config['samtools']        
    output:
        o1 = join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.bam'),
    shell:
        """
        awk -F '\t' -v OFS='\t' '{{ $(NF+1) = "NH:i:1"; print }}' {input.un} | cat {input.h} - | \
        samtools sort | samtools view -Sb > {output.o1};
        """

rule create_bam_mm:
    """
    cat header, create multimapped bam file
    """
    input:
        h = join(out_dir,'05_reads','{sp}.{al}.split.{n}.header.txt'),
        mm = join(out_dir,'05_reads','{sp}.{al}.split.{n}.mm.txt'),
    params:
        rname='13b_bam_m',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.bam'),
    shell:
        """
        cat {input.h} {input.mm} | samtools sort | samtools view -Sb > {output.o1}
        """

rule sort_mm_and_unique:
    """
    sort both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.bam'),
        mm = join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.bam'),
    params:
        rname='14_sort_mu',
    envmodules:
        config['samtools']      
    output:
        o1 = join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.s.bam'),
        o2 = join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.s.bam'),
    shell:
        """
        samtools sort {input.un} -o {output.o1}; \
        samtools sort {input.mm} -o {output.o2};
        """

rule index_mm_and_unique:
    """
    index both the multimapped and unique bam files
    """
    input:
        un = join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.s.bam'),
        mm = join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.s.bam'),
    params:
        rname='15_index_mu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.i.bam'),
        o2 = join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.i.bam'),
    shell:
        """
        cp {input.un} {output.o1}; samtools index {output.o1}; \
        cp {input.mm} {output.o2}; samtools index {output.o2};
        """

rule merge_splits_unique_mm:
    """
    merge split unique bam files into one merged.unique bam file
    merge split multimapped bam files into one merged.mm bam file
    """
    input:
        unique = dynamic(join(out_dir,'06_bam','unique','{sp}.{al}.split.{n}.unique.i.bam')),
        mm = dynamic(join(out_dir,'06_bam','mm','{sp}.{al}.split.{n}.mm.i.bam'))
    params:
        rname='16_merge_mu',
    envmodules:
        config['samtools']  
    output:
        unique = join(out_dir,'07_bam_merged_splits','{sp}.{al}.merged.unique.bam'),
        mm = join(out_dir,'07_bam_merged_splits','{sp}.{al}.merged.mm.bam'),
    shell:
        """
        samtools merge -f {output.unique} {input.unique}; \
        samtools merge -f {output.mm} {input.mm}
        """

rule merge_mm_and_unique:
    """
    merge merged.mm and merged.unique bam files by sample
    """
    input:
        un = join(out_dir,'07_bam_merged_splits','{sp}.{al}.merged.unique.bam'),
        mm = join(out_dir,'07_bam_merged_splits','{sp}.{al}.merged.mm.bam'),
    params:
        rname='17_merge_mu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'08_bam_merged/{sp}.{al}.merged.bam'),
    shell:
        """
        samtools merge -f {output.o1} {input.un} {input.mm}
        """

rule sort_index_merged:
    """
    sort merged.bam
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.bam'),
    params:
        rname='18_si_m',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam'),
        o2 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam.bai'),
    shell:
        """
        samtools sort {input.f1} -o {output.o1};
        samtools index {output.o1}
        """

rule qc_samstats:
    """
    generate statistics for sam file before deduplication
    http://www.htslib.org/doc/samtools-stats.html
    > $1
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam'),
    params:
        rname='19_samstats'
    envmodules:
        config['samtools']
    output:
        o1 = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt')
    shell:
        """
        samtools view -h {input.f1} | samtools stats - > {output.o1}
        """

rule multiqc:
    """
    merges FastQC reports for pre/post trimmed fastq files into MultiQC report
    https://multiqc.info/docs/#running-multiqc
    """
    input:
        f1 = expand(join(out_dir,'{mp}/00_qc_pre','{sp}_fastqc.html'),zip, mp=mp_list, sp=sp_list),
        f2 = expand(join(out_dir, 'qc', '00_qc_post','{sp}_filtered_fastqc.html'),sp=sp_list),
        f3 = expand(join(out_dir, 'qc', '00_qc_screen_species','{sp}_filtered.txt'),sp=sp_list),
        f4 = expand(join(out_dir, 'qc', '00_qc_screen_rrna','{sp}_filtered.txt'),sp=sp_list),
        f5 = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_samstats.txt'),sp=sp_list, al=align_list)
    params:
        rname = '20_multiqc',
        out = join(out_dir,'qc'),
        qc_config = join(source_dir,'config','multiqc_config.yaml'),
        dir_pre = expand(join(out_dir,'{mp}','00_qc_pre'),mp = samp_dict.keys()),
        dir_post = expand(join(out_dir, 'qc', '00_qc_post')),
        dir_screen_species = expand(join(out_dir, 'qc', '00_qc_screen_species')),
        dir_screen_rrna = expand(join(out_dir, 'qc', '00_qc_screen_rrna')),
    envmodules:
        config['multiqc']
    output:
        o1 = join(out_dir,'qc/multiqc_report.html')
    shell:
        """
        multiqc -f -v -c {params.qc_config} \
            -d -dd 1 {params.dir_pre} {params.dir_post} \
            {params.dir_screen_rrna} {params.dir_screen_species} \
            -o {params.out}
        """

rule qc_alignment:
    """
    uses samtools to create a bams of unaligned reads and aligned reads
    input; print qlength col to text file
    generates plots and summmary file for aligned vs unaligned statistics
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam'),
    params:
        rname = "21_qc_align",
        R = join(source_dir,'workflow','scripts','04_alignment_stats.R'),
        sampleid = '{sp}.{al}',
        base = join(out_dir, 'qc', '00_qc_post/')
    envmodules:
        config['samtools'],
        config['R']
    output:
        bam_a = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_align_len.txt'),
        bam_u = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unalign_len.txt'),
        png_align = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'),
        png_unalign = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'),
        txt_align = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'),
        txt_unalign = join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'),
    shell:
        """
        samtools view -F 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_a}; \
        samtools view -f 4 {input.f1} | awk '{{print length($10)}}' > {output.bam_u}; \
        Rscript {params.R} {params.sampleid} {output.bam_a} {output.bam_u} {params.base}
        """

#pipeline splits to handle multiplexed QC vs non-multiplexed QC
if (multiplex_flag == 'Y'):
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
            txt_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.txt'),mp=samp_dict.keys()),
            png_bc = expand(join(out_dir,'{mp}/00_qc_post','{mp}_barcode.png'),mp=samp_dict.keys())
        params:
            rname = "22_qc_ts",
            R = join(source_dir,'workflow','scripts','05_qc_report.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}", \
                    b_txt = "{input.txt_bc}"))'
            """
else:
    rule qc_troubleshoot:
        """
        generates a PDF of barcode plots and alignment plots for qc troubleshooting
        """
        input:
            png_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.png'), sp=sp_list, al=align_list),
            png_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.png'), sp=sp_list, al=align_list),
            txt_align = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_aligned.txt'), sp=sp_list, al=align_list),
            txt_unalign = expand(join(out_dir, 'qc', '00_qc_post','{sp}.{al}_unaligned.txt'), sp=sp_list, al=align_list),
        params:
            rname = "22_qc_ts",
            R = join(source_dir,'workflow','scripts','05_qc_report_nondemux.Rmd'),
        envmodules:
            config['R']
        output:
            o1 = join(out_dir,'qc','qc_report.html')
        shell:
            """
            Rscript -e 'library(rmarkdown); \
            rmarkdown::render("{params.R}",
                output_file = "{output.o1}", \
                params= list(a_txt = "{input.txt_align}", \
                    u_txt = "{input.txt_unalign}"))'
            """

rule dedup:
    """
    deduplicate 
    """
    input:
        f1 = join(out_dir,'08_bam_merged','{sp}.{al}.merged.si.bam'),
    params:
        rname='23_dedup',
        umi = umi_parameter
    envmodules:
        config['umitools']  
    output:
        o1 = join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.bam'),
        o2 = join(out_dir,'09_dedup','01_bam','{sp}.{al}.dedup.log'),
    shell:
        """
        umi_tools dedup \
        -I {input.f1} \
        --method unique --multimapping-detection-method=NH --umi-separator={params.umi} \
        -S {output.o1} \
        --log2stderr -L {output.o2};
        """

rule sort_index_dedup:
    """
    sort dedup.bam file
    """
    input:
        f1 = expand(join(out_dir,'09_dedup','01_bam','{{sp}}.{al}.dedup.bam'), al=align_list),
    params:
        rname='25_si_dedup',
    envmodules:
        config['samtools']  
    output:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        bai = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam.bai'),
    shell:
        """
        samtools sort {input.f1} -o {output.bam};
        samtools index {output.bam}
        """

rule dedup_header:
    """
    cat bam header
    """
    input:
        f1 = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
    params:
        rname='26_dedup_head',
    envmodules:
        config['samtools']   
    output:
        o1 = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.header.txt'),
    shell:
        """
        samtools view -H {input.f1} > {output.o1}
        """

rule dedup_unique:
    """
    split dedup file into multimapped and unique files
    """
    input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        h = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.header.txt'),
    params:
        rname='27_split_dedup',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.bam'),
    shell:
        """
        set +e
        samtools view {input.bam} | grep -w 'NH:i:1' | cat {input.h} - |  samtools sort | samtools view -Sb > {output.o1}
        exitcode=0
        if [ $exitcode -eq 1 ]
        then
            exit 1
        else
            exit 0
        fi
        """

rule index_dedup_unique:
    """
    index deduped unique file
    """
    input:
        f1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.bam'),
    params:
        rname='28_index_splitu',
    envmodules:
        config['samtools']  
    output:
        o1 = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'),
    shell:
        """
        cp {input.f1} {output.o1}; samtools index {output.o1};
        """

rule create_beds:
    """
    create bed file from the deduped unique file and deduped all file
    """
    input:
        all = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        unique = join(out_dir,'09_dedup','03_unique','{sp}.dedup.unique.i.bam'),
    params:
        rname='29_bed',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'10_bed','{sp}_all.bed'),
        unique = join(out_dir,'10_bed','{sp}_unique.bed')
    shell:
        """
        bedtools bamtobed \
            -split -i {input.all} | bedtools sort -i - > {output.all}; 
        bedtools bamtobed \
            -split -i {input.unique} | bedtools sort -i - > {output.unique}; 
        """

rule create_safs: 
    """
    create SAF files from the deduped unique bed and all bed files
    """
    input:
        all = join(out_dir,'10_bed','{sp}_all.bed'),
        unique = join(out_dir,'10_bed','{sp}_unique.bed')
    params:
        rname='30_saf',
    envmodules:
        config['bedtools']  
    output:
        all = join(out_dir,'11_SAF','{sp}_' + str(nt_merge) + '_all.SAF'),
        unique = join(out_dir,'11_SAF','{sp}_' + str(nt_merge) + '_unique.SAF')
    shell:
        """
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.all} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.all};
        bedtools merge \
            -c 6 -o count,distinct -bed -s -d 50 \
            -i {input.unique} | \
            awk '{{OFS="\t"; print $1":"$2"-"$3,$1,$2,$3,$5}}'| \
            awk 'BEGIN{{print "ID","Chr","Start","End","Strand"}}1' > {output.unique}
        """

rule feature_counts_allreads:
    """
    Unique reads (fractional counts correctly count splice reads for each peak. 
    When peaks counts are combined for peaks connected by splicing in Rscript)
    """
    input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        saf = join(out_dir,'11_SAF','{sp}_' + str(nt_merge) + '_all.SAF')
    params:
        rname='31a_allreads',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'12_counts','allreadpeaks','{sp}_' + str(nt_merge) + '_uniqueCounts.txt'),
        out_all = join(out_dir,'12_counts','allreadpeaks','{sp}_' + str(nt_merge) + '_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """   

rule feature_counts_uniquereads:
    """
    Include Multimap reads - MM reads given fractional count based on # of mapping 
    locations. All spliced reads also get fractional count. So Unique reads can get 
    fractional count when spliced peaks combined in R script the summed counts give 
    whole count for the unique alignement in combined peak.
    """
     input:
        bam = join(out_dir,'09_dedup','02_sorted','{sp}.dedup.si.bam'),
        saf = join(out_dir,'11_SAF','{sp}_' + str(nt_merge) + '_unique.SAF'),
    params:
        rname='32b_saf',
    envmodules:
        config['subread']  
    output:
        out_unique = join(out_dir,'12_counts','uniquereadpeaks','{sp}_' + str(nt_merge) + '_uniqueCounts.txt'),
        out_all = join(out_dir,'12_counts','uniquereadpeaks','{sp}_' + str(nt_merge) + '_allFracMMCounts.txt')
    shell:
        """
        featureCounts -F SAF \
            -a {input.saf} \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_unique} \
            {input.bam};
        featureCounts -F SAF \
            -a {input.saf} \
            -M \
            -O \
            -J \
            --fraction \
            --minOverlap 1 \
            -s 1 \
            -T 8 \
            -o {output.out_all} \
            {input.bam}
        """  

rule project_annotations:
    """
    generate annotation table once per project
    """
    input:
        expand(join(out_dir,'12_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'), sp=sp_list),
    params:
        rname='36_proj_anno',
        script = join(source_dir,'workflow','scripts','08_annotation.R'),
        ref_sp = nova_ref,
        rrna_flag = refseq_rrna,
        a_path = alias_path,
        g_path = gen_path,
        rs_path = rseq_path,
        c_path = can_path,
        i_path = intron_path,
        r_path = rmsk_path,
        custom_path = add_anno_path,
        base = join(out_dir,'13_annotation', '01_project/',),
        a_config = annotation_config,
    envmodules:
        config['R']
    output:
        anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        nc_anno = join(out_dir,'13_annotation', '01_project','ncRNA_annotations.txt'),
        ref_gencode = join(out_dir,'13_annotation', '01_project','ref_gencode.txt'),
    shell:
        """
        Rscript {params.script} \
            {params.ref_sp} \
            {params.rrna_flag} \
            {params.a_path} \
            {params.g_path} \
            {params.rs_path} \
            {params.c_path} \
            {params.i_path} \
            {params.r_path} \
            {params.custom_path} \
            {params.base} \
            {params.a_config}
        """

if(DE_method=="MANORM"):
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'),
            all = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'),
            anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        params:
            rname = '37_peak_anno',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = nova_ref,
            base1 = join(out_dir,'13_annotation','peaks/',),
            base2 = join(out_dir,'14_MAnorm','input/',),
            anno_dir = join(out_dir,'13_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_mapq_IN.txt'),
            o2 = join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_complete.txt'),
            o3 = join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom.bed'),
            o4 = join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom_P.bed'),
            o5 = join(out_dir,'14_MAnorm', 'input','{sp}_' + str(nt_merge) + '_' + peak_id + '_PeaksforMAnrom_N.bed'),
        shell:
            '''
            Rscript {params.script} \
                {params.p_type} \
                {input.unique} \
                {input.all} \
                {params.junc} \
                {params.c_exon} \
                {params.r_depth} \
                {params.d_method} \
                {params.sp} \
                {params.n_merge} \
                {params.ref_sp} \
                {params.base1} \
                {params.base2} \
                {params.anno_dir} \
                {params.a_config} \
                {params.g_path} \
                {params.i_path} \
                {params.r_path} \
            '''
else:
    rule peak_annotations:
        '''
        find peak junctions, annotations peaks, merges junction and annotation information
        '''
        input:
            unique = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_uniqueCounts.txt'),
            all = join(out_dir,'12_counts', 'allreadpeaks', '{sp}_'+ str(nt_merge) +'_allFracMMCounts.txt'),
            anno = join(out_dir,'13_annotation', '01_project','annotations.txt'),
        params:
            rname = '37_peak_anno',
            script = join(source_dir,'workflow','scripts','09_peak_annotation.R'),
            p_type = peak_id,
            junc = sp_junc,
            c_exon = cond_exon,
            r_depth= min_count,
            d_method=DE_method,
            sp = "{sp}",
            n_merge = nt_merge,
            ref_sp = nova_ref,
            base = join(out_dir,'13_annotation','02_peaks/',),
            anno_dir = join(out_dir,'13_annotation','01_project/'),
            a_config = annotation_config,
            g_path = gen_path,
            i_path = intron_path,
            r_path = rmsk_path,
        envmodules:
            config['R'],
            config['bedtools'],
        output:
            o1 = join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_mapq_IN.txt'),
            o2 = join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_complete.txt'),
        shell:
            '''
            Rscript {params.script} \
                {params.p_type} \
                {input.unique} \
                {input.all} \
                {params.junc} \
                {params.c_exon} \
                {params.r_depth} \
                {params.d_method} \
                {params.sp} \
                {params.n_merge} \
                {params.ref_sp} \
                {params.base} \
                {params.anno_dir} \
                {params.a_config} \
                {params.g_path} \
                {params.i_path} \
                {params.r_path} \
            '''

rule annotation_output:
    """
    generates an HTML report for peak annotations
    """
    input:
        peak_in = join(out_dir,'13_annotation', '02_peaks','{sp}_' + str(nt_merge) + '_peakannotation_complete.txt'),
    params:
        rname = "38_anno_output",
        R = join(source_dir,'workflow','scripts','10_annotation.Rmd'),
        sp = "{sp}",
        ref_sp = nova_ref,
        n_merge = nt_merge,
        r_depth= min_count,
        c_exon = cond_exon,
        junc = sp_junc,
        p_type = peak_id,
        rrna = refseq_rrna,
    envmodules:
        config['R']
    output:
        o1 = join(out_dir,'13_annotation', '{sp}_' + str(nt_merge) + '_annotation_final_report.html'),
        o2 = join(out_dir,'13_annotation', '{sp}_' + str(nt_merge) + '_annotation_final_table.txt'),
    shell:
        """
        Rscript -e 'library(rmarkdown); \
        rmarkdown::render("{params.R}",
            output_file = "{output.o1}", \
            params= list(samplename = "{params.sp}", \
                peak_in = "{input.peak_in}", \
                output_table = "{output.o2}", \
                ntmerge = "{params.n_merge}", \
                readdepth = "{params.r_depth}", \
                species = "{params.ref_sp}", \
                Condense = "{params.c_exon}", \
                JoinJunc = "{params.junc}", \
                include_rRNA = "{params.rrna}", \
                PeakIdnt = "{params.p_type}"))'
        """